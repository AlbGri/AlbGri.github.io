---
title: "Learning Python (016)"
excerpt: "Neural Nets and TensorBoard -- House Sales in King County, Breast cancer Wisconsin and LendingClub dataset"
date: 2020-08-01
tags: [udemy, python, coding]
mathjax: "true"

---
*Scritto ed eseguito sul portatile con Windows 10 -- Effetto South Working*

*Utilizzo l'environment conda py3_tf*  
```console
~$ conda activate py3_tf
~$ conda deactivate
```

*Versione modulo installato*  
```console
~$ pip show tensorflow
Name: tensorflow
Version: 2.2.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/unknown/miniconda3/envs/py3_tf/lib/python3.7/site-packages
Requires: h5py, keras-preprocessing, opt-einsum, wrapt, six, protobuf, tensorboard, wheel, gast, tensorflow-estimator, google-pasta, termcolor, absl-py, astunparse, scipy, numpy, grpcio
Required-by: 
```

Utile per monitorare la GPU
```console
~$ nvidia-smi
```

# Indice
1. Neural Nets and Deep Learning
	* Perceptron Model
	* Neural Networks
	* Activation functions
	* Backpropagation
	* Keras Neural Network Example
	* House Sales in King County (Regression)
	* Breast cancer Wisconsin (Classification)
	* LendingClub dataset (Classification)
1. TensorBoard

# Neural Nets and Deep Learning

## Perceptron Model

<img src="/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/001-NN.png" width="400">  
$$\hat{y}=\sum_{i=1}^{n}x_iw_i+b_i$$  
Il termine Bias, è da interpretare come una soglia (se negativo) da superare prima che una variabile possa avere un'impatto positivo.

## Neural Networks  
*blablabla*  
[Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)

## Activation functions  
Hanno l'utilità di vincolare l'output, ad esempio se si vuole ottenere un output di classificazione, ad esempio:
   * la funzione logistica (sigmoid) come funzione d'attivazione (o il *cosh*, *sinh*, *tanh*)  
   * Rectified Linear Unit (ReLU) con dominio $$\max(0,z)$$ , la ReLU è buona per contenere il *vanishing gradient*
   * [others](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)

nb. Z e X sono spesso maiuscoli per indicare un tensore (multiple values)  

### Multiclass Activation Functions
1. Non-Esclusive Classes
Ci può essere una pluri assegnazione di etichetta per ogni osservazione, es multiple tag. La funzione di attivazione logistica funziona bene, hai una probabilità per ogni classe e scelta una soglia assegni una o più etichette
1. Mutually Esclusive Classes
Solo una classe assegnata. Si usa la Softmax Function  
$$\sigma\left (\textbf{z}\right )_i=\frac{e^{z_i}}{\sum_{j=1}^Ke^{z_j}}$$ for $$i=1,...,K$$ eventi. Si ottiene una distribuzione di probabilità la cui somma è uno, la classe scelta è associata al valore di probabilità massimo.

Si struttura la rete affinché abbia più nodi output

### Cost Functions and Gradient Descent
La funzione di costo serve a monitorare l'andamento dell'aggiornamento dei pesi in fase di training.

#### Quadratic Cost Function  
$$C=\frac{1}{2n}\sum_x\left \|y(x)-a^L(x)  \right \|^2$$  
con  
*a =* valori previsti  
*y =* valori osservati 
Si può pensare alla funzione di costo per le NN  
$$C(W,B,S^r,E^r)$$  
con  
$$W =$$ pesi della neural network
$$B =$$ bias  
$$S^r =$$ input per il singolo campione di training  
$$E^r =$$ output per il singolo campione di training  

<img src="/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/002-NN.png" width="400">

Bisogna trovare il $$W_{\min}$$ che minimizzi la funzione di costo. Se è *n-dimensionale* si usa il Gradient Descent. Con una funzione di costo convessa, si ottiene tramite step (tutti parametrizzabili) fino a quando i pesi portano la derivata prima della funzione di costo a 0 (o quasi). Adam come ottimizzatore. Se si parla di due dimensioni passiamo dalle derivate al gradiente, quindi si calcola  
$$\nabla C\left (w_1,w_2,...,w_n\right )$$

#### Cross-Entropy
Per i problemi di classificazione si usa spesso la *cross entropy* loss function.  
$$C=-\left (y \log\left (p\right )+\left (1-y\right ) \log\left (1-p\right )\right )$$  
per un problema Multiclasse  
$$C=-\sum_{c=1}^{M}y_{o,c}\log\left (p_{o,c}\right )$$

## Backpropagation
Chain-rule derivative per aggiornare iterativamente i vari pesi partendo dall'ultimo minimizzando la funzione di costo.  
Hadamard Product (il prodotto come funziona su numpy)
$$\begin{bmatrix}
1\\ 1
\end{bmatrix} \odot \begin{bmatrix}
3\\ 4
\end{bmatrix} = \begin{bmatrix}
1*3\\ 2*4
\end{bmatrix} = \begin{bmatrix}
3\\ 8
\end{bmatrix}$$  

## Keras Neural Network Example


```python
# lib
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

import tensorflow as tf
import random as rn
import os

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import mean_absolute_error,mean_squared_error
from tensorflow.keras.models import load_model
```


```python
# df (obiettivo: predizione del price)
df = pd.read_csv('Refactored_Py_DS_ML_Bootcamp-master/22-Deep Learning/TensorFlow_FILES/DATA/fake_reg.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>feature1</th>
      <th>feature2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>461.527929</td>
      <td>999.787558</td>
      <td>999.766096</td>
    </tr>
    <tr>
      <th>1</th>
      <td>548.130011</td>
      <td>998.861615</td>
      <td>1001.042403</td>
    </tr>
    <tr>
      <th>2</th>
      <td>410.297162</td>
      <td>1000.070267</td>
      <td>998.844015</td>
    </tr>
    <tr>
      <th>3</th>
      <td>540.382220</td>
      <td>999.952251</td>
      <td>1000.440940</td>
    </tr>
    <tr>
      <th>4</th>
      <td>546.024553</td>
      <td>1000.446011</td>
      <td>1000.338531</td>
    </tr>
  </tbody>
</table>
</div>




```python
# pairplot
sns.set_style('whitegrid')
sns.pairplot(df, palette='red')
```




    <seaborn.axisgrid.PairGrid at 0x2722a98fec8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_19_1.png)



```python
# X e y come np array
X = df[['feature1','feature2']].values
y = df['price'].values
```


```python
# train test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print(X_train.shape)
print(X_test.shape)
```

    (700, 2)
    (300, 2)
    


```python
# min max scaling -- help
help(MinMaxScaler)
```

    Help on class MinMaxScaler in module sklearn.preprocessing._data:
    
    class MinMaxScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
     |  MinMaxScaler(feature_range=(0, 1), *, copy=True)
     |  
     |  Transform features by scaling each feature to a given range.
     |  
     |  This estimator scales and translates each feature individually such
     |  that it is in the given range on the training set, e.g. between
     |  zero and one.
     |  
     |  The transformation is given by::
     |  
     |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
     |      X_scaled = X_std * (max - min) + min
     |  
     |  where min, max = feature_range.
     |  
     |  This transformation is often used as an alternative to zero mean,
     |  unit variance scaling.
     |  ...
    
    


```python
# min max scaling
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
```


```python
print('Train Min-Max: ', X_train.min(), X_train.max())
print('Test Min-Max: ', X_test.min(), X_test.max())
```

    Train Min-Max:  0.0 1.0
    Test Min-Max:  -0.014108392024496652 1.0186515935232023
    

### Choosing an optimizer and loss
Seguono i principali problemi supervisionati con Keras:
1. Multi-class classification problem
```python
# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
```
1. Binary classification problem
```python
# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
```
1. Mean squared error regression problem
```python
# model.compile(optimizer='rmsprop', loss='mse')
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# define model multiple line (si possono passare i Dense anche dentro il Sequential come lista)
model = Sequential()
model.add(Dense(units=4,activation='relu')) # units sono i nodi
model.add(Dense(units=4,activation='relu'))
model.add(Dense(units=4,activation='relu'))
model.add(Dense(units=1)) # final layer (vogliamo solo il price)

model.compile(optimizer='rmsprop',loss='mse')
# nb. questo passo mangia la GPU
```

### Training
Descrizione parametri principali in Keras:
* Sample: one element of a dataset.
    * Example: one image is a sample in a convolutional network
    * Example: one audio file is a sample for a speech recognition model
* Batch: a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model.A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluation/prediction).
* Epoch: an arbitrary cutoff, generally defined as "one pass over the entire dataset", used to separate training into distinct phases, which is useful for logging and periodic evaluation.
* When using validation_data or validation_split with the fit method of Keras models, evaluation will be run at the end of every epoch.



```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# training model
model.fit(X_train,y_train,epochs=250)
```

    Train on 700 samples
    Epoch 1/250
    700/700 [==============================] - 1s 2ms/sample - loss: 256557.3441
    Epoch 2/250
    700/700 [==============================] - 0s 73us/sample - loss: 256365.9997
	...
    Epoch 250/250
    700/700 [==============================] - 0s 58us/sample - loss: 24.1132

    <tensorflow.python.keras.callbacks.History at 0x27236051688>



### Evaluation


```python
# loss trend
loss = model.history.history['loss']
```


```python
# plot loss trend
sns.lineplot(x=range(len(loss)),y=loss)
plt.title("Training Loss per Epoch")
```




    Text(0.5, 1.0, 'Training Loss per Epoch')




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_33_1.png)



```python
# Loss (in questo caso MSE)
training_score = model.evaluate(X_train,y_train,verbose=0)
test_score = model.evaluate(X_test,y_test,verbose=0)
print('training Score:', training_score)
print('test Score:', test_score)
```

    training Score: 23.728277675083707
    test Score: 25.146427205403647
    


```python
# predictions
test_predictions = model.predict(X_test)
```


```python
# previsti
test_predictions = pd.Series(test_predictions.reshape(300,))
```


```python
# osservati
pred_df = pd.DataFrame(y_test,columns=['Test Y'])
pred_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Test Y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>402.296319</td>
    </tr>
    <tr>
      <th>1</th>
      <td>624.156198</td>
    </tr>
    <tr>
      <th>2</th>
      <td>582.455066</td>
    </tr>
    <tr>
      <th>3</th>
      <td>578.588606</td>
    </tr>
    <tr>
      <th>4</th>
      <td>371.224104</td>
    </tr>
  </tbody>
</table>
</div>




```python
# previsti e osservati
pred_df = pd.concat([pred_df,test_predictions],axis=1)
pred_df.columns = ['Test Y','Model Predictions']
pred_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Test Y</th>
      <th>Model Predictions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>402.296319</td>
      <td>405.533844</td>
    </tr>
    <tr>
      <th>1</th>
      <td>624.156198</td>
      <td>623.994934</td>
    </tr>
    <tr>
      <th>2</th>
      <td>582.455066</td>
      <td>592.561340</td>
    </tr>
    <tr>
      <th>3</th>
      <td>578.588606</td>
      <td>572.621155</td>
    </tr>
    <tr>
      <th>4</th>
      <td>371.224104</td>
      <td>366.802795</td>
    </tr>
  </tbody>
</table>
</div>




```python
# scatter predict vs observed
sns.set_style('whitegrid')
sns.scatterplot(x='Test Y',y='Model Predictions',data=pred_df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x27238970d08>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_39_1.png)



```python
# distribution errors
pred_df['Error'] = pred_df['Test Y'] - pred_df['Model Predictions']
sns.distplot(pred_df['Error'],bins=50)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x27235ddbd88>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_40_1.png)



```python
# metrics
print('MAE:',mean_absolute_error(pred_df['Test Y'],pred_df['Model Predictions']))
print('MSE:',mean_squared_error(pred_df['Test Y'],pred_df['Model Predictions']))
print('MSE (from model.evaluate):',test_score)
print('RMSE:',test_score**0.5)
```

    MAE: 4.023428708666904
    MSE: 25.14642937056938
    MSE (from model.evaluate): 25.146427205403647
    RMSE: 5.014621342175663
    


```python
# Il MAE di circa 4, un errore di meno dell'1% della media
df['price'].describe()
```




    count    1000.000000
    mean      498.673029
    std        93.785431
    min       223.346793
    25%       433.025732
    50%       502.382117
    75%       564.921588
    max       774.407854
    Name: price, dtype: float64



### New observation to predict


```python
# [[Feature1, Feature2]]
new_gem = [[998,1000]]
# scaling
new_gem = scaler.transform(new_gem)
# predict
print(model.predict(new_gem))
```

    [[419.92566]]
    

### Saving model


```python
# working directory
os.getcwd()
```




    'F:\\Udemy\\Python for DS and ML Bootcamp'




```python
# save
model.save('Keras_Neural_Network_Example.h5')  # creates a HDF5 file
```


```python
# load
later_model = load_model(r'F:\GitHub\AlbGri.github.io\assets\files\Udemy\Python for DS and ML Bootcamp\Keras_Neural_Network_Example.h5')
```

    WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.
    


```python
# prediction with loaded model
print(later_model.predict(new_gem))
```

    [[420.05133]]
    

## House Sales in King County
[Kaggle: Predict house price using regression](https://www.kaggle.com/harlfoxem/housesalesprediction)


```python
# lib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score
```


```python
# df
df = pd.read_csv('Refactored_Py_DS_ML_Bootcamp-master/22-Deep Learning/TensorFlow_FILES/DATA//kc_house_data.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>date</th>
      <th>price</th>
      <th>bedrooms</th>
      <th>bathrooms</th>
      <th>sqft_living</th>
      <th>sqft_lot</th>
      <th>floors</th>
      <th>waterfront</th>
      <th>view</th>
      <th>...</th>
      <th>grade</th>
      <th>sqft_above</th>
      <th>sqft_basement</th>
      <th>yr_built</th>
      <th>yr_renovated</th>
      <th>zipcode</th>
      <th>lat</th>
      <th>long</th>
      <th>sqft_living15</th>
      <th>sqft_lot15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7129300520</td>
      <td>10/13/2014</td>
      <td>221900.0</td>
      <td>3</td>
      <td>1.00</td>
      <td>1180</td>
      <td>5650</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>7</td>
      <td>1180</td>
      <td>0</td>
      <td>1955</td>
      <td>0</td>
      <td>98178</td>
      <td>47.5112</td>
      <td>-122.257</td>
      <td>1340</td>
      <td>5650</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6414100192</td>
      <td>12/9/2014</td>
      <td>538000.0</td>
      <td>3</td>
      <td>2.25</td>
      <td>2570</td>
      <td>7242</td>
      <td>2.0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>7</td>
      <td>2170</td>
      <td>400</td>
      <td>1951</td>
      <td>1991</td>
      <td>98125</td>
      <td>47.7210</td>
      <td>-122.319</td>
      <td>1690</td>
      <td>7639</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5631500400</td>
      <td>2/25/2015</td>
      <td>180000.0</td>
      <td>2</td>
      <td>1.00</td>
      <td>770</td>
      <td>10000</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>6</td>
      <td>770</td>
      <td>0</td>
      <td>1933</td>
      <td>0</td>
      <td>98028</td>
      <td>47.7379</td>
      <td>-122.233</td>
      <td>2720</td>
      <td>8062</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2487200875</td>
      <td>12/9/2014</td>
      <td>604000.0</td>
      <td>4</td>
      <td>3.00</td>
      <td>1960</td>
      <td>5000</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>7</td>
      <td>1050</td>
      <td>910</td>
      <td>1965</td>
      <td>0</td>
      <td>98136</td>
      <td>47.5208</td>
      <td>-122.393</td>
      <td>1360</td>
      <td>5000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1954400510</td>
      <td>2/18/2015</td>
      <td>510000.0</td>
      <td>3</td>
      <td>2.00</td>
      <td>1680</td>
      <td>8080</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>8</td>
      <td>1680</td>
      <td>0</td>
      <td>1987</td>
      <td>0</td>
      <td>98074</td>
      <td>47.6168</td>
      <td>-122.045</td>
      <td>1800</td>
      <td>7503</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>




```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 21597 entries, 0 to 21596
    Data columns (total 21 columns):
     #   Column         Non-Null Count  Dtype  
    ---  ------         --------------  -----  
     0   id             21597 non-null  int64  
     1   date           21597 non-null  object 
     2   price          21597 non-null  float64
     3   bedrooms       21597 non-null  int64  
     4   bathrooms      21597 non-null  float64
     5   sqft_living    21597 non-null  int64  
     6   sqft_lot       21597 non-null  int64  
     7   floors         21597 non-null  float64
     8   waterfront     21597 non-null  int64  
     9   view           21597 non-null  int64  
     10  condition      21597 non-null  int64  
     11  grade          21597 non-null  int64  
     12  sqft_above     21597 non-null  int64  
     13  sqft_basement  21597 non-null  int64  
     14  yr_built       21597 non-null  int64  
     15  yr_renovated   21597 non-null  int64  
     16  zipcode        21597 non-null  int64  
     17  lat            21597 non-null  float64
     18  long           21597 non-null  float64
     19  sqft_living15  21597 non-null  int64  
     20  sqft_lot15     21597 non-null  int64  
    dtypes: float64(5), int64(15), object(1)
    memory usage: 3.5+ MB
    


```python
# converto id a stringa (così non la escludiamo tra le misure di sintesi)
df['id'] = df['id'].apply(str)
```

### EDA


```python
# check missing
df.isnull().sum().sum()
```




    0




```python
# per mostrare i separatori delle migliaia come punti e decimali come virgola
dot_sep = lambda x: format(round(x,2) if abs(x) < 1 else round(x,1) if abs(x) < 10 else int(x), ',').replace(",", "X").replace(".", ",").replace("X", ".")
```


```python
# describe, potrei usare il .transpose, ma preferico così e miglioro i decimali
df.describe(percentiles=[0.25,0.5,0.75,0.999]).applymap(dot_sep)
# df.describe(percentiles=[0.25,0.5,0.75,0.999]).style.format("{:.1f}")
# sono presenti forti outliers
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>bedrooms</th>
      <th>bathrooms</th>
      <th>sqft_living</th>
      <th>sqft_lot</th>
      <th>floors</th>
      <th>waterfront</th>
      <th>view</th>
      <th>condition</th>
      <th>grade</th>
      <th>sqft_above</th>
      <th>sqft_basement</th>
      <th>yr_built</th>
      <th>yr_renovated</th>
      <th>lat</th>
      <th>long</th>
      <th>sqft_living15</th>
      <th>sqft_lot15</th>
      <th>month</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>540.296</td>
      <td>3,4</td>
      <td>2,1</td>
      <td>2.080</td>
      <td>15.099</td>
      <td>1,5</td>
      <td>0,01</td>
      <td>0,23</td>
      <td>3,4</td>
      <td>7,7</td>
      <td>1.788</td>
      <td>291</td>
      <td>1.970</td>
      <td>84</td>
      <td>47</td>
      <td>-122</td>
      <td>1.986</td>
      <td>12.758</td>
      <td>6,6</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>std</th>
      <td>367.368</td>
      <td>0,93</td>
      <td>0,77</td>
      <td>918</td>
      <td>41.412</td>
      <td>0,54</td>
      <td>0,09</td>
      <td>0,77</td>
      <td>0,65</td>
      <td>1,2</td>
      <td>827</td>
      <td>442</td>
      <td>29</td>
      <td>401</td>
      <td>0,14</td>
      <td>0,14</td>
      <td>685</td>
      <td>27.274</td>
      <td>3,1</td>
      <td>0,47</td>
    </tr>
    <tr>
      <th>min</th>
      <td>78.000</td>
      <td>1,0</td>
      <td>0,5</td>
      <td>370</td>
      <td>520</td>
      <td>1,0</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>1,0</td>
      <td>3,0</td>
      <td>370</td>
      <td>0,0</td>
      <td>1.900</td>
      <td>0,0</td>
      <td>47</td>
      <td>-122</td>
      <td>399</td>
      <td>651</td>
      <td>1,0</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>322.000</td>
      <td>3,0</td>
      <td>1,8</td>
      <td>1.430</td>
      <td>5.040</td>
      <td>1,0</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>3,0</td>
      <td>7,0</td>
      <td>1.190</td>
      <td>0,0</td>
      <td>1.951</td>
      <td>0,0</td>
      <td>47</td>
      <td>-122</td>
      <td>1.490</td>
      <td>5.100</td>
      <td>4,0</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>450.000</td>
      <td>3,0</td>
      <td>2,2</td>
      <td>1.910</td>
      <td>7.618</td>
      <td>1,5</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>3,0</td>
      <td>7,0</td>
      <td>1.560</td>
      <td>0,0</td>
      <td>1.975</td>
      <td>0,0</td>
      <td>47</td>
      <td>-122</td>
      <td>1.840</td>
      <td>7.620</td>
      <td>6,0</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>645.000</td>
      <td>4,0</td>
      <td>2,5</td>
      <td>2.550</td>
      <td>10.685</td>
      <td>2,0</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>4,0</td>
      <td>8,0</td>
      <td>2.210</td>
      <td>560</td>
      <td>1.997</td>
      <td>0,0</td>
      <td>47</td>
      <td>-122</td>
      <td>2.360</td>
      <td>10.083</td>
      <td>9,0</td>
      <td>2.015</td>
    </tr>
    <tr>
      <th>99.9%</th>
      <td>3.480.600</td>
      <td>8,0</td>
      <td>5,5</td>
      <td>7.290</td>
      <td>495.972</td>
      <td>3,0</td>
      <td>1,0</td>
      <td>4,0</td>
      <td>5,0</td>
      <td>12</td>
      <td>6.114</td>
      <td>2.372</td>
      <td>2.015</td>
      <td>2.014</td>
      <td>47</td>
      <td>-121</td>
      <td>5.012</td>
      <td>303.191</td>
      <td>12</td>
      <td>2.015</td>
    </tr>
    <tr>
      <th>max</th>
      <td>7.700.000</td>
      <td>33</td>
      <td>8,0</td>
      <td>13.540</td>
      <td>1.651.359</td>
      <td>3,5</td>
      <td>1,0</td>
      <td>4,0</td>
      <td>5,0</td>
      <td>13</td>
      <td>9.410</td>
      <td>4.820</td>
      <td>2.015</td>
      <td>2.015</td>
      <td>47</td>
      <td>-121</td>
      <td>6.210</td>
      <td>871.200</td>
      <td>12</td>
      <td>2.015</td>
    </tr>
  </tbody>
</table>
</div>




```python
# distribuzione (continua) del price
sns.set_style('whitegrid')
plt.figure(figsize=(12,8))
sns.distplot(df['price'])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba0584148>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_59_1.png)



```python
# distribuzione (discreta) bedrooms
sns.countplot(df['bedrooms'])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba2b8e148>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_60_1.png)



```python
# correlazioni con target
df.corr()['price'].sort_values()[-10:]
```




    lat              0.306692
    bedrooms         0.308787
    sqft_basement    0.323799
    view             0.397370
    bathrooms        0.525906
    sqft_living15    0.585241
    sqft_above       0.605368
    grade            0.667951
    sqft_living      0.701917
    price            1.000000
    Name: price, dtype: float64




```python
# scatterplot price e sqft_living
plt.figure(figsize=(12,8))
sns.scatterplot(x='price',y='sqft_living',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba1a2c0c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_62_1.png)



```python
# boxplot bedrooms e price
sns.boxplot(x='bedrooms',y='price',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba29eb148>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_63_1.png)



```python
# boxplot waterfront e price
sns.boxplot(x='waterfront',y='price',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba4e746c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_64_1.png)


#### Geographical Properties 


```python
# il prezzo varia in funzione della longitudine?
plt.figure(figsize=(12,8))
sns.scatterplot(x='price',y='long',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba2eda608>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_66_1.png)



```python
# il prezzo varia in funzione della latitudine?
plt.figure(figsize=(12,8))
sns.scatterplot(x='price',y='lat',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba2f71108>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_67_1.png)



```python
# plot latitudine e longitudine (King County)
plt.figure(figsize=(12,8))
sns.scatterplot(x='long',y='lat',data=df,hue='price')
# il colore non è distribuito bene perché ci sono forti outliers che spingono la distribuzione verso il basso
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba2ebc508>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_68_1.png)



```python
# top 10 outliers per price
df.select_dtypes(include=np.number).sort_values('price',ascending=False).head(10).applymap(dot_sep)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>bedrooms</th>
      <th>bathrooms</th>
      <th>sqft_living</th>
      <th>sqft_lot</th>
      <th>floors</th>
      <th>waterfront</th>
      <th>view</th>
      <th>condition</th>
      <th>grade</th>
      <th>sqft_above</th>
      <th>sqft_basement</th>
      <th>yr_built</th>
      <th>yr_renovated</th>
      <th>lat</th>
      <th>long</th>
      <th>sqft_living15</th>
      <th>sqft_lot15</th>
      <th>month</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7245</th>
      <td>7.700.000</td>
      <td>6</td>
      <td>8,0</td>
      <td>12.050</td>
      <td>27.600</td>
      <td>2,5</td>
      <td>0</td>
      <td>3</td>
      <td>4</td>
      <td>13</td>
      <td>8.570</td>
      <td>3.480</td>
      <td>1.910</td>
      <td>1.987</td>
      <td>47</td>
      <td>-122</td>
      <td>3.940</td>
      <td>8.800</td>
      <td>10</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>3910</th>
      <td>7.060.000</td>
      <td>5</td>
      <td>4,5</td>
      <td>10.040</td>
      <td>37.325</td>
      <td>2,0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>11</td>
      <td>7.680</td>
      <td>2.360</td>
      <td>1.940</td>
      <td>2.001</td>
      <td>47</td>
      <td>-122</td>
      <td>3.930</td>
      <td>25.449</td>
      <td>6</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>9245</th>
      <td>6.890.000</td>
      <td>6</td>
      <td>7,8</td>
      <td>9.890</td>
      <td>31.374</td>
      <td>2,0</td>
      <td>0</td>
      <td>4</td>
      <td>3</td>
      <td>13</td>
      <td>8.860</td>
      <td>1.030</td>
      <td>2.001</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>4.540</td>
      <td>42.730</td>
      <td>9</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>4407</th>
      <td>5.570.000</td>
      <td>5</td>
      <td>5,8</td>
      <td>9.200</td>
      <td>35.069</td>
      <td>2,0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>13</td>
      <td>6.200</td>
      <td>3.000</td>
      <td>2.001</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>3.560</td>
      <td>24.345</td>
      <td>8</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>1446</th>
      <td>5.350.000</td>
      <td>5</td>
      <td>5,0</td>
      <td>8.000</td>
      <td>23.985</td>
      <td>2,0</td>
      <td>0</td>
      <td>4</td>
      <td>3</td>
      <td>12</td>
      <td>6.720</td>
      <td>1.280</td>
      <td>2.009</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>4.600</td>
      <td>21.750</td>
      <td>4</td>
      <td>2.015</td>
    </tr>
    <tr>
      <th>1313</th>
      <td>5.300.000</td>
      <td>6</td>
      <td>6,0</td>
      <td>7.390</td>
      <td>24.829</td>
      <td>2,0</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
      <td>12</td>
      <td>5.000</td>
      <td>2.390</td>
      <td>1.991</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>4.320</td>
      <td>24.619</td>
      <td>4</td>
      <td>2.015</td>
    </tr>
    <tr>
      <th>1162</th>
      <td>5.110.000</td>
      <td>5</td>
      <td>5,2</td>
      <td>8.010</td>
      <td>45.517</td>
      <td>2,0</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
      <td>12</td>
      <td>5.990</td>
      <td>2.020</td>
      <td>1.999</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>3.430</td>
      <td>26.788</td>
      <td>10</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>8085</th>
      <td>4.670.000</td>
      <td>5</td>
      <td>6,8</td>
      <td>9.640</td>
      <td>13.068</td>
      <td>1,0</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
      <td>12</td>
      <td>4.820</td>
      <td>4.820</td>
      <td>1.983</td>
      <td>2.009</td>
      <td>47</td>
      <td>-122</td>
      <td>3.270</td>
      <td>10.454</td>
      <td>6</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>2624</th>
      <td>4.500.000</td>
      <td>5</td>
      <td>5,5</td>
      <td>6.640</td>
      <td>40.014</td>
      <td>2,0</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
      <td>12</td>
      <td>6.350</td>
      <td>290</td>
      <td>2.004</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>3.030</td>
      <td>23.408</td>
      <td>8</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>8629</th>
      <td>4.490.000</td>
      <td>4</td>
      <td>3,0</td>
      <td>6.430</td>
      <td>27.517</td>
      <td>2,0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>12</td>
      <td>6.430</td>
      <td>0</td>
      <td>2.001</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>3.720</td>
      <td>14.592</td>
      <td>6</td>
      <td>2.014</td>
    </tr>
  </tbody>
</table>
</div>




```python
# escludo l'1% di coda del dataset, cioè 216 osservazioni
non_top_1_perc = df.sort_values('price',ascending=False).iloc[216:]
```


```python
# plot latitudine e longitudine senza l'1% di coda
plt.figure(figsize=(12,8))
sns.scatterplot(x='long',y='lat',
                data=non_top_1_perc,hue='price',
                palette='RdYlGn',edgecolor=None,alpha=0.2)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba4fdd148>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_71_1.png)


### Feature Engineering


```python
# drop id
df = df.drop('id',axis=1)
```


```python
# engineering date
# convertiamo da string a time così è più semplice estrarre le info
df['date_string'] = df['date']
df['date'] = pd.to_datetime(df['date_string'])
df['month'] = df['date'].apply(lambda x: x.month)
df['year'] = df['date'].apply(lambda x: x.year)
df[['date_string','date','month','year']].info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 21597 entries, 0 to 21596
    Data columns (total 4 columns):
     #   Column       Non-Null Count  Dtype         
    ---  ------       --------------  -----         
     0   date_string  21597 non-null  object        
     1   date         21597 non-null  datetime64[ns]
     2   month        21597 non-null  int64         
     3   year         21597 non-null  int64         
    dtypes: datetime64[ns](1), int64(2), object(1)
    memory usage: 675.0+ KB
    


```python
# boxplot anno price
sns.boxplot(x='year',y='price',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba4e86308>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_75_1.png)



```python
# boxplot mese price
sns.boxplot(x='month',y='price',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba5505408>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_76_1.png)



```python
# andamento prezzo medio per mese
df.groupby('month').mean()['price'].plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba197cac8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_77_1.png)



```python
# andamento prezzo medio per anno
df.groupby('year').mean()['price'].plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba5090648>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_78_1.png)



```python
# escludo le variabili con le date complete
df = df.drop(['date','date_string'],axis=1)
df.columns
```




    Index(['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',
           'waterfront', 'view', 'condition', 'grade', 'sqft_above',
           'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',
           'sqft_living15', 'sqft_lot15', 'month', 'year'],
          dtype='object')




```python
# lo zip code va lavorato o escluso. lo escludiamo.
# se si include nel modello verrebbe considerato come numerico
# ci sono 70 zipcode diversi, quindi è un problema renderli dummy, si potrebbero raggruppare come zipcode più ricchi e meno ricchi, oppure fare raggruppamenti geografici come nord/centro/sud
df = df.drop('zipcode',axis=1)
```


```python
# la maggior parte dei yr_renoveted sono 0, si potrebbe discretizzare come chi ha rinnovato e chi no.
# inoltre c'è una correlazione, più è recente e più sono i casi
df['yr_renovated'].value_counts()
```




    0       20683
    2014       91
    2013       37
    2003       36
    2000       35
            ...  
    1934        1
    1959        1
    1951        1
    1948        1
    1944        1
    Name: yr_renovated, Length: 70, dtype: int64




```python
# la maggior parte dei sqft_basement sono 0, si potrebbe discretizzare come chi ha rinnovato e chi no
df['sqft_basement'].value_counts()
```




    0       13110
    600       221
    700       218
    500       214
    800       206
            ...  
    792         1
    2590        1
    935         1
    2390        1
    248         1
    Name: sqft_basement, Length: 306, dtype: int64



### Models


```python
# X e y
X = df.drop('price',axis=1)
y = df['price']
```


```python
# train test
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)
```


```python
# scaling Min Max
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train) # attenzione il fit solo sul train
X_test = scaler.transform(X_test)
print(X_train.shape)
print(X_test.shape)
```

    (15117, 19)
    (6480, 19)
    


```python
# definisco il modello
model = Sequential()

model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(1))

model.compile(optimizer='adam',loss='mse')
```


```python
# stimo il modello
model.fit(x=X_train,y=y_train.values,
          validation_data=(X_test,y_test.values),
          verbose=2,batch_size=128,epochs=400)
# il validation non viene utilizzato per il tuning solo per monitorare
```

    Train on 15117 samples, validate on 6480 samples
    Epoch 1/400
    15117/15117 - 0s - loss: 96382199451.0191 - val_loss: 92658238633.4025
    Epoch 2/400
	...
    Epoch 400/400
    15117/15117 - 0s - loss: 29373463675.0805 - val_loss: 27157900968.1383
    




    <tensorflow.python.keras.callbacks.History at 0x18bafce7548>




```python
# andamento loss (mse)
losses = pd.DataFrame(model.history.history)
losses.plot()
# val_loss è sul test set, utile per capire se sto facendo overfit. non stiamo facendo overfit, potremmo continuare con le epochs.
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18bafd17f48>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_89_1.png)



```python
# predictions
predictions = model.predict(X_test)
```


```python
# metrics
print('MAE:',dot_sep(mean_absolute_error(y_test,predictions)))
print('MSE:',dot_sep(mean_squared_error(y_test,predictions)))
print('RMSE:',dot_sep(mean_squared_error(y_test,predictions)**0.5))
print('Explained Var Score:',dot_sep(explained_variance_score(y_test,predictions)))
```

    MAE: 99.996
    MSE: 27.157.901.022
    RMSE: 164.796
    Explained Var Score: 0,8
    


```python
print('Media Price:',dot_sep(df['price'].mean()))
print('Mediana Price:',dot_sep(df['price'].median()))
# il MAE è di circa 100k, quindi un 20% della media
```

    Media Price: 540.296
    Mediana Price: 450.000
    


```python
# plot osservate vs previste
plt.scatter(y_test,predictions,edgecolors='black',alpha=0.5)
plt.plot(y_test,y_test,'r')
```




    [<matplotlib.lines.Line2D at 0x18bafcd3948>]




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_93_1.png)



```python
# distribuzione errori (devo formattarli uguali)
print(type(y_test))
print(type(predictions))
print(type(y_test.values.reshape(6480, 1)))
```

    <class 'pandas.core.series.Series'>
    <class 'numpy.ndarray'>
    <class 'numpy.ndarray'>
    


```python
# distribuzione errori
errors = y_test.values.reshape(6480, 1) - predictions
sns.distplot(errors)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18bb3093288>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_95_1.png)


### Prediction on new observation


```python
# esempio da prevedere
single_house = df.drop('price',axis=1).iloc[0]
single_house
```




    bedrooms            3.0000
    bathrooms           1.0000
    sqft_living      1180.0000
    sqft_lot         5650.0000
    floors              1.0000
    waterfront          0.0000
    view                0.0000
    condition           3.0000
    grade               7.0000
    sqft_above       1180.0000
    sqft_basement       0.0000
    yr_built         1955.0000
    yr_renovated        0.0000
    lat                47.5112
    long             -122.2570
    sqft_living15    1340.0000
    sqft_lot15       5650.0000
    month              10.0000
    year             2014.0000
    Name: 0, dtype: float64




```python
# bisogna scalarlo e renderlo in formato vettore
single_house = scaler.transform(single_house.values.reshape(-1, 19))
single_house
```




    array([[0.2       , 0.08      , 0.08376422, 0.00310751, 0.        ,
            0.        , 0.        , 0.5       , 0.4       , 0.10785619,
            0.        , 0.47826087, 0.        , 0.57149751, 0.21760797,
            0.16193426, 0.00582059, 0.81818182, 0.        ]])




```python
# sappiamo qual è il target osservato
df.head(1)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>bedrooms</th>
      <th>bathrooms</th>
      <th>sqft_living</th>
      <th>sqft_lot</th>
      <th>floors</th>
      <th>waterfront</th>
      <th>view</th>
      <th>condition</th>
      <th>grade</th>
      <th>sqft_above</th>
      <th>sqft_basement</th>
      <th>yr_built</th>
      <th>yr_renovated</th>
      <th>lat</th>
      <th>long</th>
      <th>sqft_living15</th>
      <th>sqft_lot15</th>
      <th>month</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>221900.0</td>
      <td>3</td>
      <td>1.0</td>
      <td>1180</td>
      <td>5650</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>7</td>
      <td>1180</td>
      <td>0</td>
      <td>1955</td>
      <td>0</td>
      <td>47.5112</td>
      <td>-122.257</td>
      <td>1340</td>
      <td>5650</td>
      <td>10</td>
      <td>2014</td>
    </tr>
  </tbody>
</table>
</div>




```python
# previsione
model.predict(single_house)
# si potrebbe migliorare il modello escludendo l'1% degli outliers e facendo più feature engineering
```




    array([[294433.44]], dtype=float32)



## Breast cancer Wisconsin
Keras Classification


```python
# lib
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report,confusion_matrix

import random as rn
import os

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation,Dropout
from tensorflow.keras.callbacks import EarlyStopping
```


```python
# df
df = pd.read_csv('Refactored_Py_DS_ML_Bootcamp-master/22-Deep Learning/TensorFlow_FILES/DATA/cancer_classification.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
      <th>benign_0__mal_1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.3001</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>...</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.1622</td>
      <td>0.6656</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.0869</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>...</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.1238</td>
      <td>0.1866</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.1974</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>...</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.1444</td>
      <td>0.4245</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.2414</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>0.09744</td>
      <td>...</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.2098</td>
      <td>0.8663</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.1980</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>0.05883</td>
      <td>...</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.1374</td>
      <td>0.2050</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>
</div>




```python
# controllo missing
df.isnull().sum().sum()
```




    0




```python
# per mostrare i separatori delle migliaia come punti e decimali come virgola
dot_sep = lambda x: format(round(x,2) if abs(x) < 1 else round(x,1) if abs(x) < 10 else int(x), ',').replace(",", "X").replace(".", ",").replace("X", ".")
```


```python
# describe, potrei usare il .transpose, ma preferico così e miglioro i decimali
df.describe(percentiles=[0.25,0.5,0.75,0.999]).applymap(dot_sep)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
      <th>benign_0__mal_1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>...</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>14</td>
      <td>19</td>
      <td>91</td>
      <td>654</td>
      <td>0,1</td>
      <td>0,1</td>
      <td>0,09</td>
      <td>0,05</td>
      <td>0,18</td>
      <td>0,06</td>
      <td>...</td>
      <td>25</td>
      <td>107</td>
      <td>880</td>
      <td>0,13</td>
      <td>0,25</td>
      <td>0,27</td>
      <td>0,11</td>
      <td>0,29</td>
      <td>0,08</td>
      <td>0,63</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3,5</td>
      <td>4,3</td>
      <td>24</td>
      <td>351</td>
      <td>0,01</td>
      <td>0,05</td>
      <td>0,08</td>
      <td>0,04</td>
      <td>0,03</td>
      <td>0,01</td>
      <td>...</td>
      <td>6,1</td>
      <td>33</td>
      <td>569</td>
      <td>0,02</td>
      <td>0,16</td>
      <td>0,21</td>
      <td>0,07</td>
      <td>0,06</td>
      <td>0,02</td>
      <td>0,48</td>
    </tr>
    <tr>
      <th>min</th>
      <td>7,0</td>
      <td>9,7</td>
      <td>43</td>
      <td>143</td>
      <td>0,05</td>
      <td>0,02</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>0,11</td>
      <td>0,05</td>
      <td>...</td>
      <td>12</td>
      <td>50</td>
      <td>185</td>
      <td>0,07</td>
      <td>0,03</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>0,16</td>
      <td>0,06</td>
      <td>0,0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>11</td>
      <td>16</td>
      <td>75</td>
      <td>420</td>
      <td>0,09</td>
      <td>0,06</td>
      <td>0,03</td>
      <td>0,02</td>
      <td>0,16</td>
      <td>0,06</td>
      <td>...</td>
      <td>21</td>
      <td>84</td>
      <td>515</td>
      <td>0,12</td>
      <td>0,15</td>
      <td>0,11</td>
      <td>0,06</td>
      <td>0,25</td>
      <td>0,07</td>
      <td>0,0</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>13</td>
      <td>18</td>
      <td>86</td>
      <td>551</td>
      <td>0,1</td>
      <td>0,09</td>
      <td>0,06</td>
      <td>0,03</td>
      <td>0,18</td>
      <td>0,06</td>
      <td>...</td>
      <td>25</td>
      <td>97</td>
      <td>686</td>
      <td>0,13</td>
      <td>0,21</td>
      <td>0,23</td>
      <td>0,1</td>
      <td>0,28</td>
      <td>0,08</td>
      <td>1,0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>15</td>
      <td>21</td>
      <td>104</td>
      <td>782</td>
      <td>0,11</td>
      <td>0,13</td>
      <td>0,13</td>
      <td>0,07</td>
      <td>0,2</td>
      <td>0,07</td>
      <td>...</td>
      <td>29</td>
      <td>125</td>
      <td>1.084</td>
      <td>0,15</td>
      <td>0,34</td>
      <td>0,38</td>
      <td>0,16</td>
      <td>0,32</td>
      <td>0,09</td>
      <td>1,0</td>
    </tr>
    <tr>
      <th>99.9%</th>
      <td>27</td>
      <td>36</td>
      <td>187</td>
      <td>2.499</td>
      <td>0,15</td>
      <td>0,33</td>
      <td>0,43</td>
      <td>0,2</td>
      <td>0,3</td>
      <td>0,1</td>
      <td>...</td>
      <td>48</td>
      <td>238</td>
      <td>3.787</td>
      <td>0,22</td>
      <td>0,99</td>
      <td>1,2</td>
      <td>0,29</td>
      <td>0,61</td>
      <td>0,19</td>
      <td>1,0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>28</td>
      <td>39</td>
      <td>188</td>
      <td>2.501</td>
      <td>0,16</td>
      <td>0,35</td>
      <td>0,43</td>
      <td>0,2</td>
      <td>0,3</td>
      <td>0,1</td>
      <td>...</td>
      <td>49</td>
      <td>251</td>
      <td>4.254</td>
      <td>0,22</td>
      <td>1,1</td>
      <td>1,3</td>
      <td>0,29</td>
      <td>0,66</td>
      <td>0,21</td>
      <td>1,0</td>
    </tr>
  </tbody>
</table>
<p>9 rows × 31 columns</p>
</div>




```python
# verifichiamo la distribuzione target
ax = sns.countplot(x='benign_0__mal_1',data=df)
# non è troppo sbilanciato

# aggiungo le frequenze
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height + 3,
            '{:1.0f}'.format(height), # '{:1.2f}'.format(height/float(len(df)))
            ha="center")
```


![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_107_0.png)



```python
# heatmap
plt.figure(figsize=(12,12))
sns.heatmap(df.corr())
```




    <matplotlib.axes._subplots.AxesSubplot at 0x26b02b93048>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_108_1.png)



```python
# head e tail delle variabili più correlate con la target
pd.concat([df.corr()['benign_0__mal_1'].sort_values().head(),df.corr()['benign_0__mal_1'].sort_values().tail()])
```




    worst concave points     -0.793566
    worst perimeter          -0.782914
    mean concave points      -0.776614
    worst radius             -0.776454
    mean perimeter           -0.742636
    symmetry error            0.006522
    texture error             0.008303
    mean fractal dimension    0.012838
    smoothness error          0.067016
    benign_0__mal_1           1.000000
    Name: benign_0__mal_1, dtype: float64




```python
# correlate con la target (escludo la target)
df.corr()['benign_0__mal_1'][:-1].sort_values().plot(kind='bar')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x26b02bbbb88>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_110_1.png)


### Model


```python
# X e y (come numpy arrays)
X = df.drop('benign_0__mal_1',axis=1).values
y = df['benign_0__mal_1'].values
```


```python
# train e test
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)
```


```python
# scaling data
scaler = MinMaxScaler()
# scaler.fit(X_train) # stima i parametri
# X_train = scaler.transform(X_train) # applica i parametri
# X_test = scaler.transform(X_test)
X_train = scaler.fit_transform(X_train) # stima e applica i parametri in un solo comando
X_test = scaler.transform(X_test) # avevo erroneamente fatto il fit, non andava messo, si otterranno risultati leggermente diversi
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# definizione struttura neural network per classificazione
model = Sequential()
# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw
model.add(Dense(units=30,activation='relu'))
model.add(Dense(units=15,activation='relu'))
model.add(Dense(units=1,activation='sigmoid'))
# For a binary classification problem
model.compile(loss='binary_crossentropy', optimizer='adam')
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
%%time
# stima modello con overfitting
# https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network
# https://datascience.stackexchange.com/questions/18414/are-there-any-rules-for-choosing-the-size-of-a-mini-batch
model.fit(x=X_train, 
          y=y_train, 
          epochs=600,
          validation_data=(X_test, y_test), verbose=0
          )
```

    Wall time: 35.6 s
    




    <tensorflow.python.keras.callbacks.History at 0x26b07120608>




```python
# loss crossentropy
model_loss = pd.DataFrame(model.history.history)
model_loss.plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x26b05f654c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_119_1.png)


#### Early Stopping


```python
# 'Stop training when a monitored quantity has stopped improving'
help(EarlyStopping)
```

    Help on class EarlyStopping in module tensorflow.python.keras.callbacks:
    
    class EarlyStopping(Callback)
     |  EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)
     |  
     |  Stop training when a monitored quantity has stopped improving.
     |  
     |  Arguments:
     |      monitor: Quantity to be monitored.
     |      min_delta: Minimum change in the monitored quantity
     |          to qualify as an improvement, i.e. an absolute
     |          change of less than min_delta, will count as no
     |          improvement.
     |      patience: Number of epochs with no improvement
     |          after which training will be stopped.
     |      verbose: verbosity mode.
     |      mode: One of `{"auto", "min", "max"}`. In `min` mode,
     |          training will stop when the quantity
     |          monitored has stopped decreasing; in `max`
     |          mode it will stop when the quantity
     |          monitored has stopped increasing; in `auto`
     |          mode, the direction is automatically inferred
     |          from the name of the monitored quantity.
     |      baseline: Baseline value for the monitored quantity.
     |          Training will stop if the model doesn't show improvement over the
     |          baseline.
     |      restore_best_weights: Whether to restore model weights from
     |          the epoch with the best value of the monitored quantity.
     |          If False, the model weights obtained at the last step of
     |          training are used.
     |  
     |  ...


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# definizione struttura neural network per classificazione
model = Sequential()
model.add(Dense(units=30,activation='relu'))
model.add(Dense(units=15,activation='relu'))
model.add(Dense(units=1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
```


```python
# definizione early stopping
early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
%%time
# stima modello con early stop per limitare overfitting
model.fit(x=X_train, 
          y=y_train, 
          epochs=600,
          validation_data=(X_test, y_test), verbose=0,
          callbacks=[early_stop]
          )
```

    Epoch 00047: early stopping
    Wall time: 3.06 s
    




    <tensorflow.python.keras.callbacks.History at 0x26b08b5b2c8>




```python
# loss crossentropy
model_loss = pd.DataFrame(model.history.history)
model_loss.plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x26b08f1bb48>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_127_1.png)


#### DropOut Layers


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# definizione struttura neural network per classificazione
model = Sequential()
model.add(Dense(units=30,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=15,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
%%time
# stima modello con early stop e dropout per limitare overfitting
model.fit(x=X_train, 
          y=y_train, 
          epochs=600,
          validation_data=(X_test, y_test), verbose=0,
          callbacks=[early_stop]
          )
```

    Epoch 00107: early stopping
    Wall time: 8.93 s
    




    <tensorflow.python.keras.callbacks.History at 0x26b091d5b48>




```python
# loss crossentropy
model_loss = pd.DataFrame(model.history.history)
model_loss.plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x26b0583fc08>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_133_1.png)


#### Metrics


```python
# predictions
predictions = model.predict_classes(X_test)
```


```python
# metrics
print('\nConfusion Matrix:')
print(confusion_matrix(y_test,predictions))
print('\nClassification metrics:')
print(classification_report(y_test,predictions))
```

    
    Confusion Matrix:
    [[54  1]
     [ 6 82]]
    
    Classification metrics:
                  precision    recall  f1-score   support
    
               0       0.90      0.98      0.94        55
               1       0.99      0.93      0.96        88
    
        accuracy                           0.95       143
       macro avg       0.94      0.96      0.95       143
    weighted avg       0.95      0.95      0.95       143
    
    

## LendingClub dataset
[Kaggle: Predict default loans with classification](https://www.kaggle.com/wordsforthewise/lending-club)  
Customized by Udemy course


```python
# lib
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report,confusion_matrix

import random as rn
import os

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
```


```python
# df
df = pd.read_csv('Refactored_Py_DS_ML_Bootcamp-master/22-Deep Learning/TensorFlow_FILES/DATA//lending_club_loan_two.csv')
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 396030 entries, 0 to 396029
    Data columns (total 27 columns):
     #   Column                Non-Null Count   Dtype  
    ---  ------                --------------   -----  
     0   loan_amnt             396030 non-null  float64
     1   term                  396030 non-null  object 
     2   int_rate              396030 non-null  float64
     3   installment           396030 non-null  float64
     4   grade                 396030 non-null  object 
     5   sub_grade             396030 non-null  object 
     6   emp_title             373103 non-null  object 
     7   emp_length            377729 non-null  object 
     8   home_ownership        396030 non-null  object 
     9   annual_inc            396030 non-null  float64
     10  verification_status   396030 non-null  object 
     11  issue_d               396030 non-null  object 
     12  loan_status           396030 non-null  object 
     13  purpose               396030 non-null  object 
     14  title                 394275 non-null  object 
     15  dti                   396030 non-null  float64
     16  earliest_cr_line      396030 non-null  object 
     17  open_acc              396030 non-null  float64
     18  pub_rec               396030 non-null  float64
     19  revol_bal             396030 non-null  float64
     20  revol_util            395754 non-null  float64
     21  total_acc             396030 non-null  float64
     22  initial_list_status   396030 non-null  object 
     23  application_type      396030 non-null  object 
     24  mort_acc              358235 non-null  float64
     25  pub_rec_bankruptcies  395495 non-null  float64
     26  address               396030 non-null  object 
    dtypes: float64(12), object(15)
    memory usage: 81.6+ MB
    

### Step 1: EDA


```python
# verifichiamo la distribuzione target
ax = sns.countplot(x='loan_status',data=df)
# un po' sbilanciato, ci aspetteremo un'elevata accuracy ma precision e recall saranno quelle difficili

# aggiungo le frequenze
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height+3000,
            '{:,.0f}'.format(height).replace(",", "X").replace(".", ",").replace("X", "."), # '{:1.2f}'.format(height/float(len(df)))
            ha="center")
```


![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_141_0.png)



```python
# histogram di loan_amnt
plt.figure(figsize=(12,4))
sns.distplot(df['loan_amnt'],kde=False,color='b',bins=40,hist_kws=dict(edgecolor='grey'))
plt.xlim(0,45000)
```




    (0.0, 45000.0)




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_142_1.png)



```python
# heatmap
plt.figure(figsize=(10,8))
sns.heatmap(df.corr(),annot=True,cmap='coolwarm',linecolor='white',linewidths=1)
# plt.ylim(10, 0)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db149a6f88>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_143_1.png)



```python
# mezzo-pairplot (parte 1)
df_pair1 = df[df.columns[0:18]].sample(n=10000, random_state=1311)
sns.pairplot(df_pair1,hue='loan_status',diag_kind='hist',diag_kws=dict(edgecolor='black',alpha=0.6,bins=30),plot_kws=dict(alpha=0.4))
```




    <seaborn.axisgrid.PairGrid at 0x18e3e6a8cc8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_144_1.png)



```python
# mezzo-pairplot (parte 2)
df_pair2 = df.iloc[:, np.r_[12,18:27]].sample(n=10000, random_state=1311) # indexer
sns.pairplot(df_pair2,hue='loan_status',diag_kind='hist',diag_kws=dict(edgecolor='black',alpha=0.6,bins=30),plot_kws=dict(alpha=0.4))
```




    <seaborn.axisgrid.PairGrid at 0x18e0a62d088>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_145_1.png)



```python
# scatterplot 
sns.scatterplot(x='installment',y='loan_amnt',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db16fec1c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_146_1.png)



```python
# boxplot loan_status e loan amount
sns.boxplot(x='loan_status',y='loan_amnt',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db177d3ac8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_147_1.png)



```python
# per mostrare i separatori delle migliaia come punti e decimali come virgola
dot_sep = lambda x: format(round(x,2) if abs(x) < 1 else round(x,1) if abs(x) < 10 else int(x), ',').replace(",", "X").replace(".", ",").replace("X", ".")

# loan amount by loan status
df.groupby('loan_status')['loan_amnt'].describe().applymap(dot_sep)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>loan_status</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Charged Off</th>
      <td>77.673</td>
      <td>15.126</td>
      <td>8.505</td>
      <td>1.000</td>
      <td>8.525</td>
      <td>14.000</td>
      <td>20.000</td>
      <td>40.000</td>
    </tr>
    <tr>
      <th>Fully Paid</th>
      <td>318.357</td>
      <td>13.866</td>
      <td>8.302</td>
      <td>500</td>
      <td>7.500</td>
      <td>12.000</td>
      <td>19.225</td>
      <td>40.000</td>
    </tr>
  </tbody>
</table>
</div>




```python
# countplot per grade stratificato per target
sns.countplot(x='grade',hue='loan_status',data=df,order=sorted(df['grade'].unique()))
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2a843721688>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_149_1.png)



```python
# countplot per subgrade
plt.figure(figsize=(12,4))
sns.countplot(x='sub_grade',data=df,order=sorted(df['sub_grade'].unique()),palette='coolwarm')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db14a6f548>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_150_1.png)



```python
# countplot per subgrade stratificato per target
plt.figure(figsize=(12,4))
sns.countplot(x='sub_grade',data=df,order=sorted(df['sub_grade'].unique()),palette='coolwarm',hue='loan_status')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db16a31888>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_151_1.png)



```python
# countplot per subgrade (F e G) stratificato per target
plt.figure(figsize=(12,4))
df_FG = df[(df['grade']=='G') | (df['grade']=='F')]
# df_FG = df[df['sub_grade'].apply(lambda x: x[0] in ['G','F'])]
sns.countplot(x='sub_grade',data=df_FG,order=sorted(df_FG['sub_grade'].unique()),palette='coolwarm',hue='loan_status')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db1782ed48>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_152_1.png)



```python
# il method map è il più veloce per la rimappatura di una variabile
mapping_dict = {'Fully Paid': 1, 'Charged Off': 0}
df['loan_status'].map(mapping_dict).value_counts() # se il mapping non è esaustivo invece dei NaN si può dare il valore originale con il metodo .fillna(df['loan_status'])

# Risultati identici ma meno performanti:
# df['loan_status'].replace(mapping_dict).value_counts()
# df.replace({'loan_status': mapping_dict})['loan_status'].value_counts() # così devo specificare la colonna e deve agire su tutto il df
# df['loan_status'].replace(to_replace=['Fully Paid', 'Charged Off'], value=[1, 0]).value_counts()
```




    1    318357
    0     77673
    Name: loan_status, dtype: int64




```python
# creo dummy/dicotomizzo la target loan status
df['loan_repaid'] = df['loan_status'].map({'Fully Paid':1,'Charged Off':0})
print(df.shape)
# tabella di contingenza
df.groupby(["loan_repaid", "loan_status"]).size().reset_index(name="Frequenza")
# pd.crosstab(df['loan_repaid'],df['loan_status'])
```

    (396030, 28)
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loan_repaid</th>
      <th>loan_status</th>
      <th>Frequenza</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Charged Off</td>
      <td>77673</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>Fully Paid</td>
      <td>318357</td>
    </tr>
  </tbody>
</table>
</div>




```python
# correlazione target con le altre variabili numeriche
df.corr()['loan_repaid'][:-1].sort_values().plot(kind='bar')
# df.corr()['loan_repaid'].sort_values().drop('loan_repaid').plot(kind='bar')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18e17784e48>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_155_1.png)


### Step 2: Data Preprocessing
Section Goals: Remove or fill any missing data. Remove unnecessary or repetitive features. Convert categorical string features to dummy variables.


```python
# df numero record
len(df)
```




    396030




```python
# missing data
sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db17a29148>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_158_1.png)



```python
# missing data count
missing_counts = pd.concat(
    [df.isnull().sum()[df.isnull().sum()>0].apply(dot_sep),
    (df.isnull().sum()[df.isnull().sum()>0]/len(df)*100).apply(dot_sep)], 
    axis=1)
missing_counts.columns = ['Freq', 'Freq %']
missing_counts
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Freq</th>
      <th>Freq %</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>emp_title</th>
      <td>22.927</td>
      <td>5,8</td>
    </tr>
    <tr>
      <th>emp_length</th>
      <td>18.301</td>
      <td>4,6</td>
    </tr>
    <tr>
      <th>title</th>
      <td>1.755</td>
      <td>0,44</td>
    </tr>
    <tr>
      <th>revol_util</th>
      <td>276</td>
      <td>0,07</td>
    </tr>
    <tr>
      <th>mort_acc</th>
      <td>37.795</td>
      <td>9,5</td>
    </tr>
    <tr>
      <th>pub_rec_bankruptcies</th>
      <td>535</td>
      <td>0,14</td>
    </tr>
  </tbody>
</table>
</div>




```python
# employement job titles univoci
print(df['emp_title'].value_counts())
print('\nUnivoci:',dot_sep(df['emp_title'].nunique()))
# troppe per creare dummy, la rimuovo ma si potrebbero raggruppare
df.drop('emp_title',inplace=True,axis=1)
print(df.shape)
```

    Teacher                     4389
    Manager                     4250
    Registered Nurse            1856
    RN                          1846
    Supervisor                  1830
                                ... 
    Annunciation                   1
    Atos Inc                       1
    chevy parts maneger            1
    Architectural Intern           1
    GroupSystems Corporation       1
    Name: emp_title, Length: 173105, dtype: int64
    
    Univoci: 173.105
    (396030, 27)
    


```python
# sorted(df['emp_length'].dropna().unique())
emp_length_order = ['Missing','< 1 year', '1 year', '2 years', '3 years', 
                    '4 years', '5 years', '6 years', '7 years', 
                    '8 years', '9 years', '10+ years']
```


```python
# countplot emp_length
plt.figure(figsize=(12,4))
ax = sns.countplot(x='emp_length',data=df[['emp_length']].fillna('Missing'),order=emp_length_order)

# aggiungo le frequenze
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height + 1000,
            '{:,.0f}'.format(height).replace(",", "X").replace(".", ",").replace("X", "."),
            ha="center")
```


![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_162_0.png)



```python
# countplot emp_length
plt.figure(figsize=(12,4))
sns.countplot(x='emp_length',data=df[['emp_length','loan_status']].fillna('Missing'),order=emp_length_order, hue='loan_status')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db203a15c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_163_1.png)



```python
# tasso di insucesso per ogni emp_length
# emp_len = df[df['loan_status']=="Charged Off"].groupby("emp_length").count()['loan_status']/df.groupby("emp_length").count()['loan_status']
emp_len = pd.DataFrame(df[['emp_length','loan_status']].fillna('Missing').groupby(['emp_length','loan_status']).size().groupby(level=0).apply(lambda x: x / x.sum()).xs('Charged Off',level='loan_status'),columns=['% Failure']).reset_index()
emp_len
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>emp_length</th>
      <th>% Failure</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1 year</td>
      <td>0.199135</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10+ years</td>
      <td>0.184186</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2 years</td>
      <td>0.193262</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3 years</td>
      <td>0.195231</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4 years</td>
      <td>0.192385</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5 years</td>
      <td>0.192187</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6 years</td>
      <td>0.189194</td>
    </tr>
    <tr>
      <th>7</th>
      <td>7 years</td>
      <td>0.194774</td>
    </tr>
    <tr>
      <th>8</th>
      <td>8 years</td>
      <td>0.199760</td>
    </tr>
    <tr>
      <th>9</th>
      <td>9 years</td>
      <td>0.200470</td>
    </tr>
    <tr>
      <th>10</th>
      <td>&lt; 1 year</td>
      <td>0.206872</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Missing</td>
      <td>0.275286</td>
    </tr>
  </tbody>
</table>
</div>




```python
# barplot tasso
plt.figure(figsize=(12,4))
# emp_len.plot(kind='bar')
sns.barplot(x='emp_length',y='% Failure',data=emp_len,order=emp_length_order,palette='coolwarm')
# non c'è una forte evidenza, quindi rimuovo 'emp_length'
df.drop('emp_length',axis=1,inplace=True)
df.shape
```




    (396030, 26)




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_165_1.png)



```python
# missing data count
missing_counts = pd.concat(
    [df.isnull().sum()[df.isnull().sum()>0].apply(dot_sep),
    (df.isnull().sum()[df.isnull().sum()>0]/len(df)*100).apply(dot_sep)], 
    axis=1)
missing_counts.columns = ['Freq', 'Freq %']
missing_counts
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Freq</th>
      <th>Freq %</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>title</th>
      <td>1.755</td>
      <td>0,44</td>
    </tr>
    <tr>
      <th>revol_util</th>
      <td>276</td>
      <td>0,07</td>
    </tr>
    <tr>
      <th>mort_acc</th>
      <td>37.795</td>
      <td>9,5</td>
    </tr>
    <tr>
      <th>pub_rec_bankruptcies</th>
      <td>535</td>
      <td>0,14</td>
    </tr>
  </tbody>
</table>
</div>




```python
# title e purpose
print(df['purpose'].value_counts().head(7), end='\n\n')
print(df['title'].value_counts().head(7))
```

    debt_consolidation    234507
    credit_card            83019
    home_improvement       24030
    other                  21185
    major_purchase          8790
    small_business          5701
    car                     4697
    Name: purpose, dtype: int64
    
    Debt consolidation         152472
    Credit card refinancing     51487
    Home improvement            15264
    Other                       12930
    Debt Consolidation          11608
    Major purchase               4769
    Consolidation                3852
    Name: title, dtype: int64
    


```python
# title e purpose
print('Purpose nunique:',df['purpose'].nunique())
print('Title nunique:',df['title'].nunique())
# rimuovo title che è una sottocategoria e non avrebbe senso rendere dummy
df.drop('title',axis=1,inplace=True)
print(df.shape)
```

    Purpose nunique: 14
    Title nunique: 48817
    (396030, 25)
    


```python
# missing data count
missing_counts = pd.concat(
    [df.isnull().sum()[df.isnull().sum()>0].apply(dot_sep),
    (df.isnull().sum()[df.isnull().sum()>0]/len(df)*100).apply(dot_sep)], 
    axis=1)
missing_counts.columns = ['Freq', 'Freq %']
missing_counts
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Freq</th>
      <th>Freq %</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>revol_util</th>
      <td>276</td>
      <td>0,07</td>
    </tr>
    <tr>
      <th>mort_acc</th>
      <td>37.795</td>
      <td>9,5</td>
    </tr>
    <tr>
      <th>pub_rec_bankruptcies</th>
      <td>535</td>
      <td>0,14</td>
    </tr>
  </tbody>
</table>
</div>




```python
# mort_acc (acconto del mutuo ipotecario)
print('mort_acc nunique:', df['mort_acc'].nunique())
print(df['mort_acc'].value_counts().head().apply(dot_sep))
```

    mort_acc nunique: 33
    0.0    139.777
    1.0     60.416
    2.0     49.948
    3.0     38.049
    4.0     27.887
    Name: mort_acc, dtype: object
    


```python
# mort_acc correlations
df.corr()['mort_acc'].sort_values()
```




    int_rate               -0.082583
    dti                    -0.025439
    revol_util              0.007514
    pub_rec                 0.011552
    pub_rec_bankruptcies    0.027239
    loan_repaid             0.073111
    open_acc                0.109205
    installment             0.193694
    revol_bal               0.194925
    loan_amnt               0.222315
    annual_inc              0.236320
    total_acc               0.381072
    mort_acc                1.000000
    Name: mort_acc, dtype: float64




```python
df[['mort_acc','total_acc']].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mort_acc</th>
      <th>total_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>25.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.0</td>
      <td>27.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>26.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>13.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>43.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Mean of mort_acc column per total_acc
print('Mean of mort_acc column per total_acc:')
print(df.groupby('total_acc').mean()['mort_acc'])
```

    Mean of mort_acc column per total_acc:
    total_acc
    2.0      0.000000
    3.0      0.052023
    4.0      0.066743
    5.0      0.103289
    6.0      0.151293
               ...   
    124.0    1.000000
    129.0    1.000000
    135.0    3.000000
    150.0    2.000000
    151.0    0.000000
    Name: mort_acc, Length: 118, dtype: float64
    


```python
# fillna by group per mort_acc

# lambda function con due variabili
# def fill_mort_acc(total_acc,mort_acc):
#     if np.isnan(mort_acc):
#         return total_acc_avg[total_acc]
#     else:
#         return mort_acc
# df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)

df['mort_acc'] = df[['mort_acc','total_acc']].groupby('total_acc').transform(lambda x: x.fillna(x.mean()))
df.shape
```




    (396030, 25)




```python
# rimuovo record con missing inferiori allo 0.5%
print('Pre rimozione missing:',len(df))
df.dropna(inplace=True)
print('Post rimozione missing:',len(df))
df.shape
```

    Pre rimozione missing: 396030
    Post rimozione missing: 395219
    




    (395219, 25)




```python
# verifica missing rimasti
df.isnull().sum().sum()
```




    0



### Step 3: Categorical Variables and Dummy Variables
String values due to the categorical columns


```python
# distribuzione tipi colonne
df.dtypes.value_counts()
```




    float64    12
    object     12
    int64       1
    dtype: int64




```python
# seleziono colonne non numeriche
df.select_dtypes(['object']).columns
# df.select_dtypes(exclude=['float64','int64']).columns
```




    Index(['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status',
           'issue_d', 'loan_status', 'purpose', 'earliest_cr_line',
           'initial_list_status', 'application_type', 'address'],
          dtype='object')




```python
# codifico term in numeric
print('Pre codifica:', df['term'].unique())
df['term'] = df['term'].map({' 36 months':36,' 60 months':60})
# df['term'] = df['term'].apply(lambda x: int(x[:3]))
print('Post codifica:', df['term'].unique())
```

    Pre codifica: [' 36 months' ' 60 months']
    Post codifica: [36 60]
    


```python
# sub_grade è una sottocategoria di grade ma si può rendere dummy, quindi rimuoviamo grade
print('Univoci grade:', df['grade'].nunique())
print('Univoci sub_grade:', df['sub_grade'].nunique())
df.drop('grade',axis=1,inplace=True)
print(df.shape)
```

    Univoci grade: 7
    Univoci sub_grade: 35
    (395219, 24)
    


```python
# dummyfication
df = pd.get_dummies(df,columns=['sub_grade'],drop_first=True)
# subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True,prefix='sub_grade')
# df = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1)
df.columns
```




    Index(['loan_amnt', 'term', 'int_rate', 'installment', 'home_ownership',
           'annual_inc', 'verification_status', 'issue_d', 'loan_status',
           'purpose', 'dti', 'earliest_cr_line', 'open_acc', 'pub_rec',
           'revol_bal', 'revol_util', 'total_acc', 'initial_list_status',
           'application_type', 'mort_acc', 'pub_rec_bankruptcies', 'address',
           'loan_repaid', 'sub_grade_A2', 'sub_grade_A3', 'sub_grade_A4',
           'sub_grade_A5', 'sub_grade_B1', 'sub_grade_B2', 'sub_grade_B3',
           'sub_grade_B4', 'sub_grade_B5', 'sub_grade_C1', 'sub_grade_C2',
           'sub_grade_C3', 'sub_grade_C4', 'sub_grade_C5', 'sub_grade_D1',
           'sub_grade_D2', 'sub_grade_D3', 'sub_grade_D4', 'sub_grade_D5',
           'sub_grade_E1', 'sub_grade_E2', 'sub_grade_E3', 'sub_grade_E4',
           'sub_grade_E5', 'sub_grade_F1', 'sub_grade_F2', 'sub_grade_F3',
           'sub_grade_F4', 'sub_grade_F5', 'sub_grade_G1', 'sub_grade_G2',
           'sub_grade_G3', 'sub_grade_G4', 'sub_grade_G5'],
          dtype='object')




```python
# verifico le colonne non numeriche rimaste
df.select_dtypes(['object']).columns
```




    Index(['home_ownership', 'verification_status', 'issue_d', 'loan_status',
           'purpose', 'earliest_cr_line', 'initial_list_status',
           'application_type', 'address'],
          dtype='object')




```python
# rimanenti variabili categoriche
print('verification_status nunique:',df['verification_status'].nunique())
print('application_type nunique:',df['application_type'].nunique())
print('initial_list_status nunique:',df['initial_list_status'].nunique())
print('purpose nunique:',df['purpose'].nunique())
```

    verification_status nunique: 3
    application_type nunique: 3
    initial_list_status nunique: 2
    purpose nunique: 14
    


```python
# dummyficate
df = pd.get_dummies(df,columns=['verification_status', 'application_type','initial_list_status','purpose'],drop_first=True)
```


```python
# verifico le colonne non numeriche rimaste
df.select_dtypes(['object']).columns
```




    Index(['home_ownership', 'issue_d', 'loan_status', 'earliest_cr_line',
           'address'],
          dtype='object')




```python
# riduzione mapping sostituzione categorie di home_ownership
print(df['home_ownership'].value_counts())
print('\n')
# df['home_ownership'].map({'NONE':'OTHER','ANY':'OTHER'}).fillna(df['home_ownership']).value_counts()
print(df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER').value_counts())
```

    MORTGAGE    198022
    RENT        159395
    OWN          37660
    OTHER          110
    NONE            29
    ANY              3
    Name: home_ownership, dtype: int64
    
    
    MORTGAGE    198022
    RENT        159395
    OWN          37660
    OTHER          142
    Name: home_ownership, dtype: int64
    


```python
# dummyfication home_ownership
df['home_ownership'] = df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')
df = pd.get_dummies(df,columns=['home_ownership'],drop_first=True)
df.columns
```




    Index(['loan_amnt', 'term', 'int_rate', 'installment', 'annual_inc', 'issue_d',
           'loan_status', 'dti', 'earliest_cr_line', 'open_acc', 'pub_rec',
           'revol_bal', 'revol_util', 'total_acc', 'mort_acc',
           'pub_rec_bankruptcies', 'address', 'loan_repaid', 'sub_grade_A2',
           'sub_grade_A3', 'sub_grade_A4', 'sub_grade_A5', 'sub_grade_B1',
           'sub_grade_B2', 'sub_grade_B3', 'sub_grade_B4', 'sub_grade_B5',
           'sub_grade_C1', 'sub_grade_C2', 'sub_grade_C3', 'sub_grade_C4',
           'sub_grade_C5', 'sub_grade_D1', 'sub_grade_D2', 'sub_grade_D3',
           'sub_grade_D4', 'sub_grade_D5', 'sub_grade_E1', 'sub_grade_E2',
           'sub_grade_E3', 'sub_grade_E4', 'sub_grade_E5', 'sub_grade_F1',
           'sub_grade_F2', 'sub_grade_F3', 'sub_grade_F4', 'sub_grade_F5',
           'sub_grade_G1', 'sub_grade_G2', 'sub_grade_G3', 'sub_grade_G4',
           'sub_grade_G5', 'verification_status_Source Verified',
           'verification_status_Verified', 'application_type_INDIVIDUAL',
           'application_type_JOINT', 'initial_list_status_w',
           'purpose_credit_card', 'purpose_debt_consolidation',
           'purpose_educational', 'purpose_home_improvement', 'purpose_house',
           'purpose_major_purchase', 'purpose_medical', 'purpose_moving',
           'purpose_other', 'purpose_renewable_energy', 'purpose_small_business',
           'purpose_vacation', 'purpose_wedding', 'home_ownership_OTHER',
           'home_ownership_OWN', 'home_ownership_RENT'],
          dtype='object')




```python
# verifico le colonne non numeriche rimaste
df.select_dtypes(['object']).columns
```




    Index(['issue_d', 'loan_status', 'earliest_cr_line', 'address'], dtype='object')




```python
# feature engineering address column
print('address nunique:',df['address'].nunique())
print('\n')
print(df['address'].value_counts().head(10))
```

    address nunique: 392898
    
    
    USS Johnson\nFPO AE 48052     8
    USCGC Smith\nFPO AE 70466     8
    USS Smith\nFPO AP 70466       8
    USNS Johnson\nFPO AE 05113    8
    USNS Johnson\nFPO AP 48052    7
    USNS Johnson\nFPO AA 70466    6
    USNV Brown\nFPO AA 48052      6
    USS Smith\nFPO AP 22690       6
    USCGC Miller\nFPO AA 22690    6
    USCGC Smith\nFPO AA 70466     6
    Name: address, dtype: int64
    


```python
# creo variabile cap (zip code)
df['zip_code'] = df['address'].apply(lambda x: x[-5:])
df['zip_code'].unique()
```




    array(['22690', '05113', '00813', '11650', '30723', '70466', '29597',
           '48052', '86630', '93700'], dtype=object)




```python
# dummy zip code
df.drop('address',axis=1,inplace=True)
df = pd.get_dummies(df,columns=['zip_code'],drop_first=True)
df.columns
```




    Index(['loan_amnt', 'term', 'int_rate', 'installment', 'annual_inc', 'issue_d',
           'loan_status', 'dti', 'earliest_cr_line', 'open_acc', 'pub_rec',
           'revol_bal', 'revol_util', 'total_acc', 'mort_acc',
           'pub_rec_bankruptcies', 'loan_repaid', 'sub_grade_A2', 'sub_grade_A3',
           'sub_grade_A4', 'sub_grade_A5', 'sub_grade_B1', 'sub_grade_B2',
           'sub_grade_B3', 'sub_grade_B4', 'sub_grade_B5', 'sub_grade_C1',
           'sub_grade_C2', 'sub_grade_C3', 'sub_grade_C4', 'sub_grade_C5',
           'sub_grade_D1', 'sub_grade_D2', 'sub_grade_D3', 'sub_grade_D4',
           'sub_grade_D5', 'sub_grade_E1', 'sub_grade_E2', 'sub_grade_E3',
           'sub_grade_E4', 'sub_grade_E5', 'sub_grade_F1', 'sub_grade_F2',
           'sub_grade_F3', 'sub_grade_F4', 'sub_grade_F5', 'sub_grade_G1',
           'sub_grade_G2', 'sub_grade_G3', 'sub_grade_G4', 'sub_grade_G5',
           'verification_status_Source Verified', 'verification_status_Verified',
           'application_type_INDIVIDUAL', 'application_type_JOINT',
           'initial_list_status_w', 'purpose_credit_card',
           'purpose_debt_consolidation', 'purpose_educational',
           'purpose_home_improvement', 'purpose_house', 'purpose_major_purchase',
           'purpose_medical', 'purpose_moving', 'purpose_other',
           'purpose_renewable_energy', 'purpose_small_business',
           'purpose_vacation', 'purpose_wedding', 'home_ownership_OTHER',
           'home_ownership_OWN', 'home_ownership_RENT', 'zip_code_05113',
           'zip_code_11650', 'zip_code_22690', 'zip_code_29597', 'zip_code_30723',
           'zip_code_48052', 'zip_code_70466', 'zip_code_86630', 'zip_code_93700'],
          dtype='object')




```python
# verifico le colonne non numeriche rimaste
df.select_dtypes(['object']).columns
```




    Index(['issue_d', 'loan_status', 'earliest_cr_line'], dtype='object')




```python
# issue_date è 'data leakage' la avremmo solo con la realizzazione della terget, quindi va esclusa
df.drop('issue_d',axis=1,inplace=True)
df.shape
```




    (395219, 80)




```python
# earliest_cr_line possiamo estrarre l'anno
print(df['earliest_cr_line'].head())
print('\n')
print('Lunghezza stringa:',df['earliest_cr_line'].apply(lambda x: len(x)).unique()) # sono tutti costanti da 8, si prende l'anno dalla fine
df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda x: int(x[-4:]))
df.drop('earliest_cr_line',axis=1,inplace=True)
```

    0    Jun-1990
    1    Jul-2004
    2    Aug-2007
    3    Sep-2006
    4    Mar-1999
    Name: earliest_cr_line, dtype: object
    
    
    Lunghezza stringa: [8]
    


```python
# faccio un backup del df, nel caso sbaglio
df_backup = df.copy()
df_backup.shape
```




    (395219, 80)




```python
# verifico le colonne non numeriche rimaste
df.select_dtypes(['object']).columns
```




    Index(['loan_status'], dtype='object')




```python
# elimino loan_status così lascio la target codificata ('loan_repaid')
df.drop('loan_status',axis=1,inplace=True)
df.shape
# siamo pronti!
```




    (395219, 79)



### Step 4: Model
[Tuning NN](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)  
Droput links [1](https://keras.io/api/layers/core_layers/), [2](https://en.wikipedia.org/wiki/Dilution_(neural_networks)), [3](https://towardsdatascience.com/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab)


```python
# potrei lavorare sul campione
df_sample = df.sample(frac=0.1,random_state=101)
print('df intero:',len(df))
print('df ridotto:',len(df_sample))
```

    df intero: 395219
    df ridotto: 39522
    


```python
# X e y (come numpy arrays)
X = df.drop('loan_repaid',axis=1).values
y = df['loan_repaid'].values
```


```python
# train e test
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=101)
```


```python
# normalizing data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test) # avevo erroneamente fatto il fit, non andava messo, si otterranno risultati leggermente diversi
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# definizione struttura neural network per classificazione
model = Sequential()
model.add(Dense(units=78,activation='relu'))
model.add(Dropout(0.5)) # 0.2 da provare
model.add(Dense(units=39,activation='relu'))
model.add(Dropout(0.5)) # 0.2 da provare
model.add(Dense(units=19,activation='relu'))
model.add(Dropout(0.5)) # 0.2 da provare
model.add(Dense(units=1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
%%time
# stimo il modello
model.fit(x=X_train,y=y_train,
          validation_data=(X_test,y_test),
          verbose=2,batch_size=128,epochs=25) # batch_size=256 da provare
```

    Train on 316175 samples, validate on 79044 samples
    Epoch 1/25
    316175/316175 - 8s - loss: 0.3197 - val_loss: 0.2726
	...
    Epoch 25/25
    316175/316175 - 6s - loss: 0.2629 - val_loss: 0.2713
    Wall time: 2min 51s
    




    <tensorflow.python.keras.callbacks.History at 0x2a80300b748>




```python
# save the model
model.save('LendingClub_Keras_Model.h5')  # creates a HDF5 file
```




    'F:\\Udemy\\Python for DS and ML Bootcamp'




```python
# loss crossentropy (campione)
model_loss = pd.DataFrame(model.history.history)
model_loss.plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2a8573fa248>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_209_1.png)



```python
# loss crossentropy
model_loss = pd.DataFrame(model.history.history)
model_loss.plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2a80381b2c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_210_1.png)



```python
# predictions
predictions = model.predict_classes(X_test)
```


```python
# metrics
print('\nConfusion Matrix:')
print(confusion_matrix(y_test,predictions))
print('\nClassification metrics:')
print(classification_report(y_test,predictions))
# non è fantastico perché la recall della classe 0 è bassa
```

    
    Confusion Matrix:
    [[ 7176  8482]
     [  406 62980]]
    
    Classification metrics:
                  precision    recall  f1-score   support
    
               0       0.95      0.46      0.62     15658
               1       0.88      0.99      0.93     63386
    
        accuracy                           0.89     79044
       macro avg       0.91      0.73      0.78     79044
    weighted avg       0.89      0.89      0.87     79044
    
    

#### New data prediction


```python
# df
rn.seed(101)
random_ind = rn.randint(0,len(df))
new_customer = df.drop('loan_repaid',axis=1).iloc[random_ind]
new_customer
```




    loan_amnt           25000.00
    term                   60.00
    int_rate               18.24
    installment           638.11
    annual_inc          61665.00
                          ...   
    zip_code_48052          0.00
    zip_code_70466          0.00
    zip_code_86630          0.00
    zip_code_93700          0.00
    earliest_cr_year     1996.00
    Name: 305323, Length: 78, dtype: float64




```python
# scaling
new_customer = scaler.transform(new_customer.values.reshape(1, 78))
# predict
print('Probabilità Prevista:', model.predict(new_customer))
print('Classe Prevista:', model.predict_classes(new_customer))
print('Classe Osservata:', df.iloc[random_ind]['loan_repaid'])
```

    Probabilità Prevista: [[0.54277164]]
    Classe Prevista: [[1]]
    Classe Osservata: 1.0
    


```python
# reset
%reset -f
```

## TensorBoard
Basato Breast cancer Wisconsin costruiamo i log per tenere traccia


```python
import pandas as pd
import numpy as np

from datetime import datetime
import os

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation,Dropout
from tensorflow.keras.callbacks import EarlyStopping,TensorBoard
```


```python
# df
df = pd.read_csv('Refactored_Py_DS_ML_Bootcamp-master/22-Deep Learning/TensorFlow_FILES/DATA/cancer_classification.csv')
```


```python
# X e y
X = df.drop('benign_0__mal_1',axis=1).values
y = df['benign_0__mal_1'].values
```


```python
# train e test
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)
```


```python
# scaling
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
```


```python
# early stopping
early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)
```


```python
# working directory
os.getcwd()
```




    'F:\\Udemy\\Python for DS and ML Bootcamp'




```python
# time
datetime.now().strftime("%Y-%m-%d--%H%M")
```




    '2020-08-01--1701'




```python
# definizione tensorboard
# WINDOWS: Use "logs\\fit"
# MACOS/LINUX: Use "logs\fit"

log_directory = 'logs\\fit'

# OPTIONAL: ADD A TIMESTAMP FOR UNIQUE FOLDER
timestamp = datetime.now().strftime("%Y-%m-%d--%H%M")
log_directory = log_directory + '\\' + timestamp

board = TensorBoard(
    log_dir=log_directory,
    histogram_freq=1, # dopo ciascuna epoch calcola i pesi
    write_graph=True,
    write_images=True,
    update_freq='epoch',
    profile_batch=2,
    embeddings_freq=1)
```


```python
# definizione modello
model = Sequential()
model.add(Dense(units=30,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=15,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
```


```python
# training modello (lanciarlo solo una volta nella stessa callback log_dir)
model.fit(x=X_train, 
          y=y_train, 
          epochs=600,
          validation_data=(X_test, y_test), verbose=0,
          callbacks=[early_stop,board]
          )
```

    Epoch 00138: early stopping
    




    <tensorflow.python.keras.callbacks.History at 0x2a852471d08>



### Monitoring
Attivo tensorboard dal terminale
```sh
tensorboard --logdir logs\fit
```
Con il servizio attivo posso visitare [http://localhost:6006/](http://localhost:6006/)  
Questo servizio di controllo e diagnostica è più utile per le convolutional neural networks.


```python
print(log_directory)
```

    logs\fit\2020-08-01--1701
    

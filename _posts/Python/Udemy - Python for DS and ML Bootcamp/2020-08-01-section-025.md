---
title: "Learning Python (016)"
excerpt: "Neural Nets and TensorBoard -- House Sales in King County, Breast cancer Wisconsin and LendingClub dataset"
date: 2020-08-01
tags: [udemy, python, coding]
mathjax: "true"

---
*Scritto ed eseguito sul portatile con Windows 10 -- Effetto South Workng*

*Utilizzo l'environment conda py3_tf*  
```console
~$ conda activate py3_tf
~$ conda deactivate
```

*Versione modulo installato*  
```console
~$ pip show tensorflow
Name: tensorflow
Version: 2.2.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/unknown/miniconda3/envs/py3_tf/lib/python3.7/site-packages
Requires: h5py, keras-preprocessing, opt-einsum, wrapt, six, protobuf, tensorboard, wheel, gast, tensorflow-estimator, google-pasta, termcolor, absl-py, astunparse, scipy, numpy, grpcio
Required-by: 
```

Utile per monitorare la GPU
```console
~$ nvidia-smi
```

# Indice
1. Neural Nets and Deep Learning
	* Perceptron Model
	* Neural Networks
	* Activation functions
	* Backpropagation
	* Keras Neural Network Example
	* House Sales in King County (Regression)
	* Breast cancer Wisconsin (Classification)
	* LendingClub dataset (Classification)
1. TensorBoard

# Neural Nets and Deep Learning

## Perceptron Model

<img src="/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/001-NN.png" width="400">
$$\hat{y}=\sum_{i=1}^{n}x_iw_i+b_i$$  
Il termine Bias, è da interpretare come una soglia (se negativo) da superare prima che una variabile possa avere un'impatto positivo.

## Neural Networks  
*blablabla*  
[Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)

## Activation functions  
Hanno l'utilità di vincolare l'output, ad esempio se si vuole ottenere un output di classificazione, ad esempio:
   * la funzione logistica (sigmoid) come funzione d'attivazione (o il *cosh*, *sinh*, *tanh*)  
   * Rectified Linear Unit (ReLU) con dominio $$\max(0,z)$$ , la ReLU è buona per contenere il *vanishing gradient*
   * [others](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)

nb. Z e X sono spesso maiuscoli per indicare un tensore (multiple values)  

### Multiclass Activation Functions
1. Non-Esclusive Classes
Ci può essere una pluri assegnazione di etichetta per ogni osservazione, es multiple tag. La funzione di attivazione logistica funziona bene, hai una probabilità per ogni classe e scelta una soglia assegni una o più etichette
1. Mutually Esclusive Classes
Solo una classe assegnata. Si usa la Softmax Function  
$$\sigma\left (\textbf{z}\right )_i=\frac{e^{z_i}}{\sum_{j=1}^Ke^{z_j}}$$ for $$i=1,...,K$$ eventi. Si ottiene una distribuzione di probabilità la cui somma è uno, la classe scelta è associata al valore di probabilità massimo.

Si struttura la rete affinché abbia più nodi output

### Cost Functions and Gradient Descent
La funzione di costo serve a monitorare l'andamento dell'aggiornamento dei pesi in fase di training.

#### Quadratic Cost Function  
$$C=\frac{1}{2n}\sum_x\left \|y(x)-a^L(x)  \right \|^2$$  
con  
*a =* valori previsti  
*y =* valori osservati 
Si può pensare alla funzione di costo per le NN  
$$C(W,B,S^r,E^r)$$  
con  
$$W =$$ pesi della neural network
$$B =$$ bias  
$$S^r =$$ input per il singolo campione di training  
$$E^r =$$ output per il singolo campione di training  

<img src="/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/002-NN.png" width="400">

Bisogna trovare il $$W_{\min}$$ che minimizzi la funzione di costo. Se è *n-dimensionale* si usa il Gradient Descent. Con una funzione di costo convessa, si ottiene tramite step (tutti parametrizzabili) fino a quando i pesi portano la derivata prima della funzione di costo a 0 (o quasi). Adam come ottimizzatore. Se si parla di due dimensioni passiamo dalle derivate al gradiente, quindi si calcola  
$$\nabla C\left (w_1,w_2,...,w_n\right )$$

#### Cross-Entropy
Per i problemi di classificazione si usa spesso la *cross entropy* loss function.  
$$C=-\left (y \log\left (p\right )+\left (1-y\right ) \log\left (1-p\right )\right )$$  
per un problema Multiclasse  
$$C=-\sum_{c=1}^{M}y_{o,c}\log\left (p_{o,c}\right )$$

## Backpropagation
Chain-rule derivative per aggiornare iterativamente i vari pesi partendo dall'ultimo minimizzando la funzione di costo.  
Hadamard Product (il prodotto come funziona su numpy)
$$\begin{bmatrix}
1\\ 1
\end{bmatrix} \odot \begin{bmatrix}
3\\ 4
\end{bmatrix} = \begin{bmatrix}
1*3\\ 2*4
\end{bmatrix} = \begin{bmatrix}
3\\ 8
\end{bmatrix}$$  

## Keras Neural Network Example


```python
# lib
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

import tensorflow as tf
import random as rn
import os

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import mean_absolute_error,mean_squared_error
from tensorflow.keras.models import load_model
```


```python
# df (obiettivo: predizione del price)
df = pd.read_csv('Refactored_Py_DS_ML_Bootcamp-master/22-Deep Learning/TensorFlow_FILES/DATA/fake_reg.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>feature1</th>
      <th>feature2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>461.527929</td>
      <td>999.787558</td>
      <td>999.766096</td>
    </tr>
    <tr>
      <th>1</th>
      <td>548.130011</td>
      <td>998.861615</td>
      <td>1001.042403</td>
    </tr>
    <tr>
      <th>2</th>
      <td>410.297162</td>
      <td>1000.070267</td>
      <td>998.844015</td>
    </tr>
    <tr>
      <th>3</th>
      <td>540.382220</td>
      <td>999.952251</td>
      <td>1000.440940</td>
    </tr>
    <tr>
      <th>4</th>
      <td>546.024553</td>
      <td>1000.446011</td>
      <td>1000.338531</td>
    </tr>
  </tbody>
</table>
</div>




```python
# pairplot
sns.set_style('whitegrid')
sns.pairplot(df, palette='red')
```




    <seaborn.axisgrid.PairGrid at 0x2722a98fec8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_19_1.png)



```python
# X e y come np array
X = df[['feature1','feature2']].values
y = df['price'].values
```


```python
# train test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print(X_train.shape)
print(X_test.shape)
```

    (700, 2)
    (300, 2)
    


```python
# min max scaling -- help
help(MinMaxScaler)
```

    Help on class MinMaxScaler in module sklearn.preprocessing._data:
    
    class MinMaxScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
     |  MinMaxScaler(feature_range=(0, 1), *, copy=True)
     |  
     |  Transform features by scaling each feature to a given range.
     |  
     |  This estimator scales and translates each feature individually such
     |  that it is in the given range on the training set, e.g. between
     |  zero and one.
     |  
     |  The transformation is given by::
     |  
     |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
     |      X_scaled = X_std * (max - min) + min
     |  
     |  where min, max = feature_range.
     |  
     |  This transformation is often used as an alternative to zero mean,
     |  unit variance scaling.
     |  
     |  Read more in the :ref:`User Guide <preprocessing_scaler>`.
     |  
     |  Parameters
     |  ----------
     |  feature_range : tuple (min, max), default=(0, 1)
     |      Desired range of transformed data.
     |  
     |  copy : bool, default=True
     |      Set to False to perform inplace row normalization and avoid a
     |      copy (if the input is already a numpy array).
     |  
     |  Attributes
     |  ----------
     |  min_ : ndarray of shape (n_features,)
     |      Per feature adjustment for minimum. Equivalent to
     |      ``min - X.min(axis=0) * self.scale_``
     |  
     |  scale_ : ndarray of shape (n_features,)
     |      Per feature relative scaling of the data. Equivalent to
     |      ``(max - min) / (X.max(axis=0) - X.min(axis=0))``
     |  
     |      .. versionadded:: 0.17
     |         *scale_* attribute.
     |  
     |  data_min_ : ndarray of shape (n_features,)
     |      Per feature minimum seen in the data
     |  
     |      .. versionadded:: 0.17
     |         *data_min_*
     |  
     |  data_max_ : ndarray of shape (n_features,)
     |      Per feature maximum seen in the data
     |  
     |      .. versionadded:: 0.17
     |         *data_max_*
     |  
     |  data_range_ : ndarray of shape (n_features,)
     |      Per feature range ``(data_max_ - data_min_)`` seen in the data
     |  
     |      .. versionadded:: 0.17
     |         *data_range_*
     |  
     |  n_samples_seen_ : int
     |      The number of samples processed by the estimator.
     |      It will be reset on new calls to fit, but increments across
     |      ``partial_fit`` calls.
     |  
     |  Examples
     |  --------
     |  >>> from sklearn.preprocessing import MinMaxScaler
     |  >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
     |  >>> scaler = MinMaxScaler()
     |  >>> print(scaler.fit(data))
     |  MinMaxScaler()
     |  >>> print(scaler.data_max_)
     |  [ 1. 18.]
     |  >>> print(scaler.transform(data))
     |  [[0.   0.  ]
     |   [0.25 0.25]
     |   [0.5  0.5 ]
     |   [1.   1.  ]]
     |  >>> print(scaler.transform([[2, 2]]))
     |  [[1.5 0. ]]
     |  
     |  See also
     |  --------
     |  minmax_scale: Equivalent function without the estimator API.
     |  
     |  Notes
     |  -----
     |  NaNs are treated as missing values: disregarded in fit, and maintained in
     |  transform.
     |  
     |  For a comparison of the different scalers, transformers, and normalizers,
     |  see :ref:`examples/preprocessing/plot_all_scaling.py
     |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.
     |  
     |  Method resolution order:
     |      MinMaxScaler
     |      sklearn.base.TransformerMixin
     |      sklearn.base.BaseEstimator
     |      builtins.object
     |  
     |  Methods defined here:
     |  
     |  __init__(self, feature_range=(0, 1), *, copy=True)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  fit(self, X, y=None)
     |      Compute the minimum and maximum to be used for later scaling.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          The data used to compute the per-feature minimum and maximum
     |          used for later scaling along the features axis.
     |      
     |      y : None
     |          Ignored.
     |      
     |      Returns
     |      -------
     |      self : object
     |          Fitted scaler.
     |  
     |  inverse_transform(self, X)
     |      Undo the scaling of X according to feature_range.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          Input data that will be transformed. It cannot be sparse.
     |      
     |      Returns
     |      -------
     |      Xt : array-like of shape (n_samples, n_features)
     |          Transformed data.
     |  
     |  partial_fit(self, X, y=None)
     |      Online computation of min and max on X for later scaling.
     |      
     |      All of X is processed as a single batch. This is intended for cases
     |      when :meth:`fit` is not feasible due to very large number of
     |      `n_samples` or because X is read from a continuous stream.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          The data used to compute the mean and standard deviation
     |          used for later scaling along the features axis.
     |      
     |      y : None
     |          Ignored.
     |      
     |      Returns
     |      -------
     |      self : object
     |          Transformer instance.
     |  
     |  transform(self, X)
     |      Scale features of X according to feature_range.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          Input data that will be transformed.
     |      
     |      Returns
     |      -------
     |      Xt : array-like of shape (n_samples, n_features)
     |          Transformed data.
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |  
     |  fit_transform(self, X, y=None, **fit_params)
     |      Fit to data, then transform it.
     |      
     |      Fits transformer to X and y with optional parameters fit_params
     |      and returns a transformed version of X.
     |      
     |      Parameters
     |      ----------
     |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)
     |      
     |      y : ndarray of shape (n_samples,), default=None
     |          Target values.
     |      
     |      **fit_params : dict
     |          Additional fit parameters.
     |      
     |      Returns
     |      -------
     |      X_new : ndarray array of shape (n_samples, n_features_new)
     |          Transformed array.
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.base.TransformerMixin:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |  
     |  __getstate__(self)
     |  
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |  
     |  __setstate__(self, state)
     |  
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |      
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |      
     |      Returns
     |      -------
     |      params : mapping of string to any
     |          Parameter names mapped to their values.
     |  
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |      
     |      The method works on simple estimators as well as on nested objects
     |      (such as pipelines). The latter have parameters of the form
     |      ``<component>__<parameter>`` so that it's possible to update each
     |      component of a nested object.
     |      
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |      
     |      Returns
     |      -------
     |      self : object
     |          Estimator instance.
    
    


```python
# min max scaling
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
```


```python
print('Train Min-Max: ', X_train.min(), X_train.max())
print('Test Min-Max: ', X_test.min(), X_test.max())
```

    Train Min-Max:  0.0 1.0
    Test Min-Max:  -0.014108392024496652 1.0186515935232023
    

### Choosing an optimizer and loss
Keep in mind what kind of problem you are trying to solve:

    # For a multi-class classification problem
    model.compile(optimizer='rmsprop',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # For a binary classification problem
    model.compile(optimizer='rmsprop',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    # For a mean squared error regression problem
    model.compile(optimizer='rmsprop',
                  loss='mse')


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# define model multiple line (si possono passare i Dense anche dentro il Sequential come lista)
model = Sequential()
model.add(Dense(units=4,activation='relu')) # units sono i nodi
model.add(Dense(units=4,activation='relu'))
model.add(Dense(units=4,activation='relu'))
model.add(Dense(units=1)) # final layer (vogliamo solo il price)

model.compile(optimizer='rmsprop',loss='mse')
# nb. questo passo mangia la GPU
```

### Training
Below are some common definitions that are necessary to know and understand to correctly utilize Keras:

* Sample: one element of a dataset.
    * Example: one image is a sample in a convolutional network
    * Example: one audio file is a sample for a speech recognition model
* Batch: a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model.A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluation/prediction).
* Epoch: an arbitrary cutoff, generally defined as "one pass over the entire dataset", used to separate training into distinct phases, which is useful for logging and periodic evaluation.
* When using validation_data or validation_split with the fit method of Keras models, evaluation will be run at the end of every epoch.
* Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of t


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# training model
model.fit(X_train,y_train,epochs=250)
```

    Train on 700 samples
    Epoch 1/250
    700/700 [==============================] - 1s 2ms/sample - loss: 256557.3441
    Epoch 2/250
    700/700 [==============================] - 0s 73us/sample - loss: 256365.9997
    Epoch 3/250
    700/700 [==============================] - 0s 68us/sample - loss: 256158.6498
    Epoch 4/250
    700/700 [==============================] - 0s 75us/sample - loss: 255923.2195
    Epoch 5/250
    700/700 [==============================] - 0s 83us/sample - loss: 255654.7080
    Epoch 6/250
    700/700 [==============================] - 0s 74us/sample - loss: 255354.8995
    Epoch 7/250
    700/700 [==============================] - 0s 66us/sample - loss: 255020.3320
    Epoch 8/250
    700/700 [==============================] - 0s 72us/sample - loss: 254645.5840
    Epoch 9/250
    700/700 [==============================] - 0s 74us/sample - loss: 254230.7418
    Epoch 10/250
    700/700 [==============================] - 0s 75us/sample - loss: 253769.1959
    Epoch 11/250
    700/700 [==============================] - 0s 69us/sample - loss: 253257.4187
    Epoch 12/250
    700/700 [==============================] - 0s 67us/sample - loss: 252696.5814
    Epoch 13/250
    700/700 [==============================] - 0s 84us/sample - loss: 252077.5242
    Epoch 14/250
    700/700 [==============================] - 0s 72us/sample - loss: 251400.1788
    Epoch 15/250
    700/700 [==============================] - 0s 74us/sample - loss: 250658.1226
    Epoch 16/250
    700/700 [==============================] - 0s 61us/sample - loss: 249847.7483
    Epoch 17/250
    700/700 [==============================] - 0s 93us/sample - loss: 248977.0079
    Epoch 18/250
    700/700 [==============================] - 0s 73us/sample - loss: 248026.4869
    Epoch 19/250
    700/700 [==============================] - 0s 92us/sample - loss: 246998.6104
    Epoch 20/250
    700/700 [==============================] - 0s 58us/sample - loss: 245890.6352
    Epoch 21/250
    700/700 [==============================] - 0s 65us/sample - loss: 244699.1787
    Epoch 22/250
    700/700 [==============================] - 0s 64us/sample - loss: 243426.1079
    Epoch 23/250
    700/700 [==============================] - 0s 74us/sample - loss: 242059.2684
    Epoch 24/250
    700/700 [==============================] - 0s 62us/sample - loss: 240591.6402
    Epoch 25/250
    700/700 [==============================] - 0s 71us/sample - loss: 239025.6155
    Epoch 26/250
    700/700 [==============================] - 0s 132us/sample - loss: 237360.0448
    Epoch 27/250
    700/700 [==============================] - 0s 90us/sample - loss: 235587.8129
    Epoch 28/250
    700/700 [==============================] - 0s 107us/sample - loss: 233713.2329
    Epoch 29/250
    700/700 [==============================] - 0s 92us/sample - loss: 231721.2879
    Epoch 30/250
    700/700 [==============================] - 0s 61us/sample - loss: 229616.7087
    Epoch 31/250
    700/700 [==============================] - 0s 55us/sample - loss: 227382.8453
    Epoch 32/250
    700/700 [==============================] - 0s 68us/sample - loss: 225034.2004
    Epoch 33/250
    700/700 [==============================] - 0s 59us/sample - loss: 222546.9086
    Epoch 34/250
    700/700 [==============================] - 0s 83us/sample - loss: 219959.0287
    Epoch 35/250
    700/700 [==============================] - 0s 64us/sample - loss: 217223.5641
    Epoch 36/250
    700/700 [==============================] - 0s 63us/sample - loss: 214359.6463
    Epoch 37/250
    700/700 [==============================] - 0s 74us/sample - loss: 211359.0312
    Epoch 38/250
    700/700 [==============================] - 0s 52us/sample - loss: 208228.8815
    Epoch 39/250
    700/700 [==============================] - 0s 57us/sample - loss: 204941.9124
    Epoch 40/250
    700/700 [==============================] - 0s 61us/sample - loss: 201538.2153
    Epoch 41/250
    700/700 [==============================] - 0s 60us/sample - loss: 197966.1373
    Epoch 42/250
    700/700 [==============================] - 0s 75us/sample - loss: 194256.7309
    Epoch 43/250
    700/700 [==============================] - 0s 59us/sample - loss: 190405.0219
    Epoch 44/250
    700/700 [==============================] - 0s 72us/sample - loss: 186390.1360
    Epoch 45/250
    700/700 [==============================] - 0s 75us/sample - loss: 182257.3056
    Epoch 46/250
    700/700 [==============================] - 0s 54us/sample - loss: 177977.2505
    Epoch 47/250
    700/700 [==============================] - 0s 71us/sample - loss: 173554.4310
    Epoch 48/250
    700/700 [==============================] - 0s 53us/sample - loss: 168996.6990
    Epoch 49/250
    700/700 [==============================] - 0s 56us/sample - loss: 164302.2541
    Epoch 50/250
    700/700 [==============================] - 0s 58us/sample - loss: 159494.9529
    Epoch 51/250
    700/700 [==============================] - 0s 55us/sample - loss: 154536.9980
    Epoch 52/250
    700/700 [==============================] - 0s 57us/sample - loss: 149479.9488
    Epoch 53/250
    700/700 [==============================] - 0s 50us/sample - loss: 144262.2974
    Epoch 54/250
    700/700 [==============================] - 0s 60us/sample - loss: 138945.1366
    Epoch 55/250
    700/700 [==============================] - 0s 72us/sample - loss: 133552.5608
    Epoch 56/250
    700/700 [==============================] - 0s 67us/sample - loss: 128048.0034
    Epoch 57/250
    700/700 [==============================] - 0s 61us/sample - loss: 122415.6429
    Epoch 58/250
    700/700 [==============================] - 0s 61us/sample - loss: 116708.1201
    Epoch 59/250
    700/700 [==============================] - 0s 62us/sample - loss: 110971.3287
    Epoch 60/250
    700/700 [==============================] - 0s 56us/sample - loss: 105166.4148
    Epoch 61/250
    700/700 [==============================] - 0s 61us/sample - loss: 99328.9084
    Epoch 62/250
    700/700 [==============================] - 0s 65us/sample - loss: 93428.3595
    Epoch 63/250
    700/700 [==============================] - 0s 76us/sample - loss: 87508.5771
    Epoch 64/250
    700/700 [==============================] - 0s 65us/sample - loss: 81621.9076
    Epoch 65/250
    700/700 [==============================] - 0s 53us/sample - loss: 75737.7612
    Epoch 66/250
    700/700 [==============================] - 0s 67us/sample - loss: 69922.6813
    Epoch 67/250
    700/700 [==============================] - 0s 72us/sample - loss: 64109.7103
    Epoch 68/250
    700/700 [==============================] - 0s 55us/sample - loss: 58404.9186
    Epoch 69/250
    700/700 [==============================] - 0s 56us/sample - loss: 52819.8126
    Epoch 70/250
    700/700 [==============================] - 0s 69us/sample - loss: 47357.6502
    Epoch 71/250
    700/700 [==============================] - 0s 53us/sample - loss: 42063.8598
    Epoch 72/250
    700/700 [==============================] - 0s 60us/sample - loss: 36971.9441
    Epoch 73/250
    700/700 [==============================] - 0s 66us/sample - loss: 32101.8153
    Epoch 74/250
    700/700 [==============================] - 0s 54us/sample - loss: 27432.6732
    Epoch 75/250
    700/700 [==============================] - 0s 64us/sample - loss: 23081.0471
    Epoch 76/250
    700/700 [==============================] - 0s 58us/sample - loss: 19085.4416
    Epoch 77/250
    700/700 [==============================] - 0s 79us/sample - loss: 15384.9586
    Epoch 78/250
    700/700 [==============================] - 0s 80us/sample - loss: 12102.8133
    Epoch 79/250
    700/700 [==============================] - 0s 75us/sample - loss: 9307.3283
    Epoch 80/250
    700/700 [==============================] - 0s 69us/sample - loss: 6946.5624
    Epoch 81/250
    700/700 [==============================] - 0s 66us/sample - loss: 5095.0729
    Epoch 82/250
    700/700 [==============================] - 0s 66us/sample - loss: 3720.1949
    Epoch 83/250
    700/700 [==============================] - 0s 63us/sample - loss: 2890.1505
    Epoch 84/250
    700/700 [==============================] - 0s 71us/sample - loss: 2528.2632
    Epoch 85/250
    700/700 [==============================] - 0s 68us/sample - loss: 2423.8017
    Epoch 86/250
    700/700 [==============================] - 0s 66us/sample - loss: 2386.3562
    Epoch 87/250
    700/700 [==============================] - 0s 59us/sample - loss: 2347.7979
    Epoch 88/250
    700/700 [==============================] - 0s 69us/sample - loss: 2311.6716
    Epoch 89/250
    700/700 [==============================] - 0s 59us/sample - loss: 2276.9312
    Epoch 90/250
    700/700 [==============================] - 0s 55us/sample - loss: 2240.4855
    Epoch 91/250
    700/700 [==============================] - 0s 55us/sample - loss: 2203.3506
    Epoch 92/250
    700/700 [==============================] - 0s 64us/sample - loss: 2162.3985
    Epoch 93/250
    700/700 [==============================] - 0s 68us/sample - loss: 2122.6441
    Epoch 94/250
    700/700 [==============================] - 0s 72us/sample - loss: 2083.1749
    Epoch 95/250
    700/700 [==============================] - ETA: 0s - loss: 2065.09 - 0s 87us/sample - loss: 2051.3126
    Epoch 96/250
    700/700 [==============================] - 0s 99us/sample - loss: 2013.6883
    Epoch 97/250
    700/700 [==============================] - 0s 93us/sample - loss: 1976.0542
    Epoch 98/250
    700/700 [==============================] - 0s 77us/sample - loss: 1937.3496
    Epoch 99/250
    700/700 [==============================] - 0s 65us/sample - loss: 1904.9895
    Epoch 100/250
    700/700 [==============================] - 0s 86us/sample - loss: 1871.1480
    Epoch 101/250
    700/700 [==============================] - 0s 91us/sample - loss: 1836.4310
    Epoch 102/250
    700/700 [==============================] - 0s 78us/sample - loss: 1798.5505
    Epoch 103/250
    700/700 [==============================] - 0s 89us/sample - loss: 1761.8974
    Epoch 104/250
    700/700 [==============================] - 0s 75us/sample - loss: 1723.1837
    Epoch 105/250
    700/700 [==============================] - 0s 70us/sample - loss: 1694.0047
    Epoch 106/250
    700/700 [==============================] - 0s 76us/sample - loss: 1655.8914
    Epoch 107/250
    700/700 [==============================] - 0s 92us/sample - loss: 1615.9681
    Epoch 108/250
    700/700 [==============================] - 0s 76us/sample - loss: 1586.2345
    Epoch 109/250
    700/700 [==============================] - 0s 72us/sample - loss: 1551.5979
    Epoch 110/250
    700/700 [==============================] - 0s 76us/sample - loss: 1520.8581
    Epoch 111/250
    700/700 [==============================] - 0s 89us/sample - loss: 1487.7759
    Epoch 112/250
    700/700 [==============================] - 0s 76us/sample - loss: 1456.0973
    Epoch 113/250
    700/700 [==============================] - 0s 85us/sample - loss: 1423.7688
    Epoch 114/250
    700/700 [==============================] - 0s 65us/sample - loss: 1391.1948
    Epoch 115/250
    700/700 [==============================] - 0s 63us/sample - loss: 1359.8803
    Epoch 116/250
    700/700 [==============================] - 0s 67us/sample - loss: 1333.3646
    Epoch 117/250
    700/700 [==============================] - 0s 89us/sample - loss: 1303.6058
    Epoch 118/250
    700/700 [==============================] - 0s 83us/sample - loss: 1272.6797
    Epoch 119/250
    700/700 [==============================] - 0s 61us/sample - loss: 1242.9546
    Epoch 120/250
    700/700 [==============================] - 0s 75us/sample - loss: 1217.2157
    Epoch 121/250
    700/700 [==============================] - 0s 59us/sample - loss: 1189.5947
    Epoch 122/250
    700/700 [==============================] - 0s 67us/sample - loss: 1159.2947
    Epoch 123/250
    700/700 [==============================] - 0s 71us/sample - loss: 1128.1098
    Epoch 124/250
    700/700 [==============================] - 0s 69us/sample - loss: 1099.3142
    Epoch 125/250
    700/700 [==============================] - 0s 75us/sample - loss: 1069.9252
    Epoch 126/250
    700/700 [==============================] - 0s 79us/sample - loss: 1040.7061
    Epoch 127/250
    700/700 [==============================] - 0s 68us/sample - loss: 1011.7024
    Epoch 128/250
    700/700 [==============================] - 0s 75us/sample - loss: 984.1979
    Epoch 129/250
    700/700 [==============================] - 0s 108us/sample - loss: 954.4640
    Epoch 130/250
    700/700 [==============================] - 0s 90us/sample - loss: 924.2602
    Epoch 131/250
    700/700 [==============================] - 0s 63us/sample - loss: 896.8618
    Epoch 132/250
    700/700 [==============================] - 0s 70us/sample - loss: 870.3333
    Epoch 133/250
    700/700 [==============================] - 0s 72us/sample - loss: 839.4912
    Epoch 134/250
    700/700 [==============================] - 0s 67us/sample - loss: 817.6085
    Epoch 135/250
    700/700 [==============================] - 0s 59us/sample - loss: 792.1771
    Epoch 136/250
    700/700 [==============================] - 0s 61us/sample - loss: 768.9512
    Epoch 137/250
    700/700 [==============================] - 0s 65us/sample - loss: 746.2227
    Epoch 138/250
    700/700 [==============================] - 0s 59us/sample - loss: 722.2922
    Epoch 139/250
    700/700 [==============================] - 0s 66us/sample - loss: 694.6458
    Epoch 140/250
    700/700 [==============================] - 0s 69us/sample - loss: 669.7885
    Epoch 141/250
    700/700 [==============================] - 0s 74us/sample - loss: 641.7248
    Epoch 142/250
    700/700 [==============================] - 0s 68us/sample - loss: 618.5866
    Epoch 143/250
    700/700 [==============================] - 0s 77us/sample - loss: 598.3818
    Epoch 144/250
    700/700 [==============================] - 0s 71us/sample - loss: 577.6742
    Epoch 145/250
    700/700 [==============================] - 0s 64us/sample - loss: 556.4603
    Epoch 146/250
    700/700 [==============================] - 0s 62us/sample - loss: 533.4915
    Epoch 147/250
    700/700 [==============================] - 0s 66us/sample - loss: 509.4372
    Epoch 148/250
    700/700 [==============================] - 0s 55us/sample - loss: 485.4684
    Epoch 149/250
    700/700 [==============================] - 0s 65us/sample - loss: 463.7410
    Epoch 150/250
    700/700 [==============================] - 0s 65us/sample - loss: 442.5130
    Epoch 151/250
    700/700 [==============================] - 0s 55us/sample - loss: 422.5809
    Epoch 152/250
    700/700 [==============================] - 0s 63us/sample - loss: 401.9947
    Epoch 153/250
    700/700 [==============================] - 0s 69us/sample - loss: 383.9001
    Epoch 154/250
    700/700 [==============================] - 0s 58us/sample - loss: 365.0881
    Epoch 155/250
    700/700 [==============================] - 0s 61us/sample - loss: 349.3824
    Epoch 156/250
    700/700 [==============================] - 0s 66us/sample - loss: 329.1944
    Epoch 157/250
    700/700 [==============================] - 0s 56us/sample - loss: 313.6379
    Epoch 158/250
    700/700 [==============================] - 0s 57us/sample - loss: 296.6388
    Epoch 159/250
    700/700 [==============================] - 0s 65us/sample - loss: 280.7384
    Epoch 160/250
    700/700 [==============================] - 0s 56us/sample - loss: 266.6624
    Epoch 161/250
    700/700 [==============================] - 0s 59us/sample - loss: 250.2260
    Epoch 162/250
    700/700 [==============================] - 0s 70us/sample - loss: 234.8921
    Epoch 163/250
    700/700 [==============================] - 0s 62us/sample - loss: 220.2897
    Epoch 164/250
    700/700 [==============================] - 0s 64us/sample - loss: 207.5316
    Epoch 165/250
    700/700 [==============================] - 0s 59us/sample - loss: 194.0116
    Epoch 166/250
    700/700 [==============================] - 0s 60us/sample - loss: 179.7813
    Epoch 167/250
    700/700 [==============================] - 0s 70us/sample - loss: 168.5026
    Epoch 168/250
    700/700 [==============================] - 0s 69us/sample - loss: 157.9104
    Epoch 169/250
    700/700 [==============================] - 0s 55us/sample - loss: 148.4958
    Epoch 170/250
    700/700 [==============================] - 0s 66us/sample - loss: 137.7661
    Epoch 171/250
    700/700 [==============================] - 0s 64us/sample - loss: 128.9394
    Epoch 172/250
    700/700 [==============================] - 0s 62us/sample - loss: 119.4833
    Epoch 173/250
    700/700 [==============================] - 0s 58us/sample - loss: 109.9355
    Epoch 174/250
    700/700 [==============================] - 0s 63us/sample - loss: 101.0612
    Epoch 175/250
    700/700 [==============================] - 0s 75us/sample - loss: 94.2695
    Epoch 176/250
    700/700 [==============================] - 0s 88us/sample - loss: 87.7625
    Epoch 177/250
    700/700 [==============================] - 0s 66us/sample - loss: 80.5053
    Epoch 178/250
    700/700 [==============================] - 0s 70us/sample - loss: 74.0996
    Epoch 179/250
    700/700 [==============================] - 0s 66us/sample - loss: 69.1117
    Epoch 180/250
    700/700 [==============================] - 0s 69us/sample - loss: 64.2305
    Epoch 181/250
    700/700 [==============================] - 0s 62us/sample - loss: 59.7163
    Epoch 182/250
    700/700 [==============================] - 0s 63us/sample - loss: 55.0499
    Epoch 183/250
    700/700 [==============================] - 0s 56us/sample - loss: 51.3635
    Epoch 184/250
    700/700 [==============================] - 0s 72us/sample - loss: 46.4991
    Epoch 185/250
    700/700 [==============================] - 0s 79us/sample - loss: 42.8861
    Epoch 186/250
    700/700 [==============================] - 0s 76us/sample - loss: 40.1651
    Epoch 187/250
    700/700 [==============================] - 0s 83us/sample - loss: 38.1288
    Epoch 188/250
    700/700 [==============================] - 0s 86us/sample - loss: 35.9889
    Epoch 189/250
    700/700 [==============================] - 0s 73us/sample - loss: 34.2360
    Epoch 190/250
    700/700 [==============================] - 0s 83us/sample - loss: 32.2045
    Epoch 191/250
    700/700 [==============================] - 0s 96us/sample - loss: 31.5038
    Epoch 192/250
    700/700 [==============================] - 0s 74us/sample - loss: 30.1006
    Epoch 193/250
    700/700 [==============================] - 0s 90us/sample - loss: 29.1163
    Epoch 194/250
    700/700 [==============================] - 0s 85us/sample - loss: 27.8808
    Epoch 195/250
    700/700 [==============================] - 0s 80us/sample - loss: 28.0472
    Epoch 196/250
    700/700 [==============================] - 0s 71us/sample - loss: 26.9983
    Epoch 197/250
    700/700 [==============================] - 0s 73us/sample - loss: 26.4400
    Epoch 198/250
    700/700 [==============================] - 0s 63us/sample - loss: 26.1896
    Epoch 199/250
    700/700 [==============================] - 0s 90us/sample - loss: 25.5106
    Epoch 200/250
    700/700 [==============================] - 0s 84us/sample - loss: 25.5542
    Epoch 201/250
    700/700 [==============================] - 0s 89us/sample - loss: 25.3593
    Epoch 202/250
    700/700 [==============================] - 0s 70us/sample - loss: 25.0375
    Epoch 203/250
    700/700 [==============================] - 0s 94us/sample - loss: 24.9155
    Epoch 204/250
    700/700 [==============================] - 0s 72us/sample - loss: 24.9700
    Epoch 205/250
    700/700 [==============================] - 0s 75us/sample - loss: 24.8726
    Epoch 206/250
    700/700 [==============================] - 0s 76us/sample - loss: 24.5003
    Epoch 207/250
    700/700 [==============================] - 0s 105us/sample - loss: 24.4264
    Epoch 208/250
    700/700 [==============================] - 0s 74us/sample - loss: 24.8051
    Epoch 209/250
    700/700 [==============================] - 0s 75us/sample - loss: 24.3323
    Epoch 210/250
    700/700 [==============================] - 0s 90us/sample - loss: 24.2731
    Epoch 211/250
    700/700 [==============================] - 0s 67us/sample - loss: 24.4667
    Epoch 212/250
    700/700 [==============================] - 0s 78us/sample - loss: 24.2725
    Epoch 213/250
    700/700 [==============================] - 0s 77us/sample - loss: 24.9458
    Epoch 214/250
    700/700 [==============================] - 0s 78us/sample - loss: 24.1059
    Epoch 215/250
    700/700 [==============================] - 0s 65us/sample - loss: 24.2996
    Epoch 216/250
    700/700 [==============================] - 0s 75us/sample - loss: 24.1617
    Epoch 217/250
    700/700 [==============================] - 0s 70us/sample - loss: 24.4785
    Epoch 218/250
    700/700 [==============================] - 0s 69us/sample - loss: 24.1339
    Epoch 219/250
    700/700 [==============================] - 0s 61us/sample - loss: 24.3195
    Epoch 220/250
    700/700 [==============================] - 0s 61us/sample - loss: 24.2163
    Epoch 221/250
    700/700 [==============================] - 0s 75us/sample - loss: 24.2704
    Epoch 222/250
    700/700 [==============================] - 0s 73us/sample - loss: 24.4361
    Epoch 223/250
    700/700 [==============================] - 0s 71us/sample - loss: 24.3258
    Epoch 224/250
    700/700 [==============================] - 0s 66us/sample - loss: 24.1105
    Epoch 225/250
    700/700 [==============================] - 0s 75us/sample - loss: 24.4735
    Epoch 226/250
    700/700 [==============================] - 0s 63us/sample - loss: 24.0847
    Epoch 227/250
    700/700 [==============================] - 0s 67us/sample - loss: 24.2584
    Epoch 228/250
    700/700 [==============================] - 0s 72us/sample - loss: 24.0567
    Epoch 229/250
    700/700 [==============================] - 0s 106us/sample - loss: 24.3104
    Epoch 230/250
    700/700 [==============================] - 0s 66us/sample - loss: 24.4904
    Epoch 231/250
    700/700 [==============================] - 0s 63us/sample - loss: 24.2910
    Epoch 232/250
    700/700 [==============================] - 0s 55us/sample - loss: 24.1725
    Epoch 233/250
    700/700 [==============================] - 0s 61us/sample - loss: 24.1787
    Epoch 234/250
    700/700 [==============================] - 0s 63us/sample - loss: 24.3555
    Epoch 235/250
    700/700 [==============================] - 0s 63us/sample - loss: 24.1782
    Epoch 236/250
    700/700 [==============================] - 0s 69us/sample - loss: 24.3392
    Epoch 237/250
    700/700 [==============================] - 0s 70us/sample - loss: 24.0679
    Epoch 238/250
    700/700 [==============================] - 0s 58us/sample - loss: 24.1370
    Epoch 239/250
    700/700 [==============================] - 0s 75us/sample - loss: 24.1981
    Epoch 240/250
    700/700 [==============================] - 0s 75us/sample - loss: 24.2388
    Epoch 241/250
    700/700 [==============================] - 0s 71us/sample - loss: 24.2840
    Epoch 242/250
    700/700 [==============================] - 0s 60us/sample - loss: 24.2535
    Epoch 243/250
    700/700 [==============================] - 0s 66us/sample - loss: 24.4276
    Epoch 244/250
    700/700 [==============================] - 0s 65us/sample - loss: 24.4682
    Epoch 245/250
    700/700 [==============================] - 0s 66us/sample - loss: 24.6274
    Epoch 246/250
    700/700 [==============================] - 0s 80us/sample - loss: 24.3133
    Epoch 247/250
    700/700 [==============================] - 0s 65us/sample - loss: 24.3111
    Epoch 248/250
    700/700 [==============================] - 0s 72us/sample - loss: 24.0814
    Epoch 249/250
    700/700 [==============================] - 0s 61us/sample - loss: 24.7239
    Epoch 250/250
    700/700 [==============================] - 0s 58us/sample - loss: 24.1132
    




    <tensorflow.python.keras.callbacks.History at 0x27236051688>



### Evaluation


```python
# loss trend
loss = model.history.history['loss']
```


```python
# plot loss trend
sns.lineplot(x=range(len(loss)),y=loss)
plt.title("Training Loss per Epoch")
```




    Text(0.5, 1.0, 'Training Loss per Epoch')




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_33_1.png)



```python
# Loss (in questo caso MSE)
training_score = model.evaluate(X_train,y_train,verbose=0)
test_score = model.evaluate(X_test,y_test,verbose=0)
print('training Score:', training_score)
print('test Score:', test_score)
```

    training Score: 23.728277675083707
    test Score: 25.146427205403647
    


```python
# predictions
test_predictions = model.predict(X_test)
```


```python
# previsti
test_predictions = pd.Series(test_predictions.reshape(300,))
```


```python
# osservati
pred_df = pd.DataFrame(y_test,columns=['Test Y'])
pred_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Test Y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>402.296319</td>
    </tr>
    <tr>
      <th>1</th>
      <td>624.156198</td>
    </tr>
    <tr>
      <th>2</th>
      <td>582.455066</td>
    </tr>
    <tr>
      <th>3</th>
      <td>578.588606</td>
    </tr>
    <tr>
      <th>4</th>
      <td>371.224104</td>
    </tr>
  </tbody>
</table>
</div>




```python
# previsti e osservati
pred_df = pd.concat([pred_df,test_predictions],axis=1)
pred_df.columns = ['Test Y','Model Predictions']
pred_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Test Y</th>
      <th>Model Predictions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>402.296319</td>
      <td>405.533844</td>
    </tr>
    <tr>
      <th>1</th>
      <td>624.156198</td>
      <td>623.994934</td>
    </tr>
    <tr>
      <th>2</th>
      <td>582.455066</td>
      <td>592.561340</td>
    </tr>
    <tr>
      <th>3</th>
      <td>578.588606</td>
      <td>572.621155</td>
    </tr>
    <tr>
      <th>4</th>
      <td>371.224104</td>
      <td>366.802795</td>
    </tr>
  </tbody>
</table>
</div>




```python
# scatter predict vs observed
sns.set_style('whitegrid')
sns.scatterplot(x='Test Y',y='Model Predictions',data=pred_df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x27238970d08>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_39_1.png)



```python
# distribution errors
pred_df['Error'] = pred_df['Test Y'] - pred_df['Model Predictions']
sns.distplot(pred_df['Error'],bins=50)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x27235ddbd88>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_40_1.png)



```python
# metrics
print('MAE:',mean_absolute_error(pred_df['Test Y'],pred_df['Model Predictions']))
print('MSE:',mean_squared_error(pred_df['Test Y'],pred_df['Model Predictions']))
print('MSE (from model.evaluate):',test_score)
print('RMSE:',test_score**0.5)
```

    MAE: 4.023428708666904
    MSE: 25.14642937056938
    MSE (from model.evaluate): 25.146427205403647
    RMSE: 5.014621342175663
    


```python
# Il MAE di circa 4, un errore di meno dell'1% della media
df['price'].describe()
```




    count    1000.000000
    mean      498.673029
    std        93.785431
    min       223.346793
    25%       433.025732
    50%       502.382117
    75%       564.921588
    max       774.407854
    Name: price, dtype: float64



### New observation to predict


```python
# [[Feature1, Feature2]]
new_gem = [[998,1000]]
# scaling
new_gem = scaler.transform(new_gem)
# predict
print(model.predict(new_gem))
```

    [[419.92566]]
    

### Saving model


```python
# working directory
os.getcwd()
```




    'F:\\Udemy\\Python for DS and ML Bootcamp'




```python
# save
model.save('Keras_Neural_Network_Example.h5')  # creates a HDF5 file
```


```python
# load
later_model = load_model(r'F:\GitHub\AlbGri.github.io\assets\files\Udemy\Python for DS and ML Bootcamp\Keras_Neural_Network_Example.h5')
```

    WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.
    


```python
# prediction with loaded model
print(later_model.predict(new_gem))
```

    [[420.05133]]
    

## House Sales in King County
[Kaggle: Predict house price using regression](https://www.kaggle.com/harlfoxem/housesalesprediction)


```python
# lib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score
```


```python
# df
df = pd.read_csv('Refactored_Py_DS_ML_Bootcamp-master/22-Deep Learning/TensorFlow_FILES/DATA//kc_house_data.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>date</th>
      <th>price</th>
      <th>bedrooms</th>
      <th>bathrooms</th>
      <th>sqft_living</th>
      <th>sqft_lot</th>
      <th>floors</th>
      <th>waterfront</th>
      <th>view</th>
      <th>...</th>
      <th>grade</th>
      <th>sqft_above</th>
      <th>sqft_basement</th>
      <th>yr_built</th>
      <th>yr_renovated</th>
      <th>zipcode</th>
      <th>lat</th>
      <th>long</th>
      <th>sqft_living15</th>
      <th>sqft_lot15</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7129300520</td>
      <td>10/13/2014</td>
      <td>221900.0</td>
      <td>3</td>
      <td>1.00</td>
      <td>1180</td>
      <td>5650</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>7</td>
      <td>1180</td>
      <td>0</td>
      <td>1955</td>
      <td>0</td>
      <td>98178</td>
      <td>47.5112</td>
      <td>-122.257</td>
      <td>1340</td>
      <td>5650</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6414100192</td>
      <td>12/9/2014</td>
      <td>538000.0</td>
      <td>3</td>
      <td>2.25</td>
      <td>2570</td>
      <td>7242</td>
      <td>2.0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>7</td>
      <td>2170</td>
      <td>400</td>
      <td>1951</td>
      <td>1991</td>
      <td>98125</td>
      <td>47.7210</td>
      <td>-122.319</td>
      <td>1690</td>
      <td>7639</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5631500400</td>
      <td>2/25/2015</td>
      <td>180000.0</td>
      <td>2</td>
      <td>1.00</td>
      <td>770</td>
      <td>10000</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>6</td>
      <td>770</td>
      <td>0</td>
      <td>1933</td>
      <td>0</td>
      <td>98028</td>
      <td>47.7379</td>
      <td>-122.233</td>
      <td>2720</td>
      <td>8062</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2487200875</td>
      <td>12/9/2014</td>
      <td>604000.0</td>
      <td>4</td>
      <td>3.00</td>
      <td>1960</td>
      <td>5000</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>7</td>
      <td>1050</td>
      <td>910</td>
      <td>1965</td>
      <td>0</td>
      <td>98136</td>
      <td>47.5208</td>
      <td>-122.393</td>
      <td>1360</td>
      <td>5000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1954400510</td>
      <td>2/18/2015</td>
      <td>510000.0</td>
      <td>3</td>
      <td>2.00</td>
      <td>1680</td>
      <td>8080</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>8</td>
      <td>1680</td>
      <td>0</td>
      <td>1987</td>
      <td>0</td>
      <td>98074</td>
      <td>47.6168</td>
      <td>-122.045</td>
      <td>1800</td>
      <td>7503</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>




```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 21597 entries, 0 to 21596
    Data columns (total 21 columns):
     #   Column         Non-Null Count  Dtype  
    ---  ------         --------------  -----  
     0   id             21597 non-null  int64  
     1   date           21597 non-null  object 
     2   price          21597 non-null  float64
     3   bedrooms       21597 non-null  int64  
     4   bathrooms      21597 non-null  float64
     5   sqft_living    21597 non-null  int64  
     6   sqft_lot       21597 non-null  int64  
     7   floors         21597 non-null  float64
     8   waterfront     21597 non-null  int64  
     9   view           21597 non-null  int64  
     10  condition      21597 non-null  int64  
     11  grade          21597 non-null  int64  
     12  sqft_above     21597 non-null  int64  
     13  sqft_basement  21597 non-null  int64  
     14  yr_built       21597 non-null  int64  
     15  yr_renovated   21597 non-null  int64  
     16  zipcode        21597 non-null  int64  
     17  lat            21597 non-null  float64
     18  long           21597 non-null  float64
     19  sqft_living15  21597 non-null  int64  
     20  sqft_lot15     21597 non-null  int64  
    dtypes: float64(5), int64(15), object(1)
    memory usage: 3.5+ MB
    


```python
# converto id a stringa (così non la escludiamo tra le misure di sintesi)
df['id'] = df['id'].apply(str)
```

### EDA


```python
# check missing
df.isnull().sum().sum()
```




    0




```python
# per mostrare i separatori delle migliaia come punti e decimali come virgola
dot_sep = lambda x: format(round(x,2) if abs(x) < 1 else round(x,1) if abs(x) < 10 else int(x), ',').replace(",", "X").replace(".", ",").replace("X", ".")
```


```python
# describe, potrei usare il .transpose, ma preferico così e miglioro i decimali
df.describe(percentiles=[0.25,0.5,0.75,0.999]).applymap(dot_sep)
# df.describe(percentiles=[0.25,0.5,0.75,0.999]).style.format("{:.1f}")
# sono presenti forti outliers
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>bedrooms</th>
      <th>bathrooms</th>
      <th>sqft_living</th>
      <th>sqft_lot</th>
      <th>floors</th>
      <th>waterfront</th>
      <th>view</th>
      <th>condition</th>
      <th>grade</th>
      <th>sqft_above</th>
      <th>sqft_basement</th>
      <th>yr_built</th>
      <th>yr_renovated</th>
      <th>lat</th>
      <th>long</th>
      <th>sqft_living15</th>
      <th>sqft_lot15</th>
      <th>month</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
      <td>21.597</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>540.296</td>
      <td>3,4</td>
      <td>2,1</td>
      <td>2.080</td>
      <td>15.099</td>
      <td>1,5</td>
      <td>0,01</td>
      <td>0,23</td>
      <td>3,4</td>
      <td>7,7</td>
      <td>1.788</td>
      <td>291</td>
      <td>1.970</td>
      <td>84</td>
      <td>47</td>
      <td>-122</td>
      <td>1.986</td>
      <td>12.758</td>
      <td>6,6</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>std</th>
      <td>367.368</td>
      <td>0,93</td>
      <td>0,77</td>
      <td>918</td>
      <td>41.412</td>
      <td>0,54</td>
      <td>0,09</td>
      <td>0,77</td>
      <td>0,65</td>
      <td>1,2</td>
      <td>827</td>
      <td>442</td>
      <td>29</td>
      <td>401</td>
      <td>0,14</td>
      <td>0,14</td>
      <td>685</td>
      <td>27.274</td>
      <td>3,1</td>
      <td>0,47</td>
    </tr>
    <tr>
      <th>min</th>
      <td>78.000</td>
      <td>1,0</td>
      <td>0,5</td>
      <td>370</td>
      <td>520</td>
      <td>1,0</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>1,0</td>
      <td>3,0</td>
      <td>370</td>
      <td>0,0</td>
      <td>1.900</td>
      <td>0,0</td>
      <td>47</td>
      <td>-122</td>
      <td>399</td>
      <td>651</td>
      <td>1,0</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>322.000</td>
      <td>3,0</td>
      <td>1,8</td>
      <td>1.430</td>
      <td>5.040</td>
      <td>1,0</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>3,0</td>
      <td>7,0</td>
      <td>1.190</td>
      <td>0,0</td>
      <td>1.951</td>
      <td>0,0</td>
      <td>47</td>
      <td>-122</td>
      <td>1.490</td>
      <td>5.100</td>
      <td>4,0</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>450.000</td>
      <td>3,0</td>
      <td>2,2</td>
      <td>1.910</td>
      <td>7.618</td>
      <td>1,5</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>3,0</td>
      <td>7,0</td>
      <td>1.560</td>
      <td>0,0</td>
      <td>1.975</td>
      <td>0,0</td>
      <td>47</td>
      <td>-122</td>
      <td>1.840</td>
      <td>7.620</td>
      <td>6,0</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>645.000</td>
      <td>4,0</td>
      <td>2,5</td>
      <td>2.550</td>
      <td>10.685</td>
      <td>2,0</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>4,0</td>
      <td>8,0</td>
      <td>2.210</td>
      <td>560</td>
      <td>1.997</td>
      <td>0,0</td>
      <td>47</td>
      <td>-122</td>
      <td>2.360</td>
      <td>10.083</td>
      <td>9,0</td>
      <td>2.015</td>
    </tr>
    <tr>
      <th>99.9%</th>
      <td>3.480.600</td>
      <td>8,0</td>
      <td>5,5</td>
      <td>7.290</td>
      <td>495.972</td>
      <td>3,0</td>
      <td>1,0</td>
      <td>4,0</td>
      <td>5,0</td>
      <td>12</td>
      <td>6.114</td>
      <td>2.372</td>
      <td>2.015</td>
      <td>2.014</td>
      <td>47</td>
      <td>-121</td>
      <td>5.012</td>
      <td>303.191</td>
      <td>12</td>
      <td>2.015</td>
    </tr>
    <tr>
      <th>max</th>
      <td>7.700.000</td>
      <td>33</td>
      <td>8,0</td>
      <td>13.540</td>
      <td>1.651.359</td>
      <td>3,5</td>
      <td>1,0</td>
      <td>4,0</td>
      <td>5,0</td>
      <td>13</td>
      <td>9.410</td>
      <td>4.820</td>
      <td>2.015</td>
      <td>2.015</td>
      <td>47</td>
      <td>-121</td>
      <td>6.210</td>
      <td>871.200</td>
      <td>12</td>
      <td>2.015</td>
    </tr>
  </tbody>
</table>
</div>




```python
# distribuzione (continua) del price
sns.set_style('whitegrid')
plt.figure(figsize=(12,8))
sns.distplot(df['price'])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba0584148>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_59_1.png)



```python
# distribuzione (discreta) bedrooms
sns.countplot(df['bedrooms'])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba2b8e148>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_60_1.png)



```python
# correlazioni con target
df.corr()['price'].sort_values()[-10:]
```




    lat              0.306692
    bedrooms         0.308787
    sqft_basement    0.323799
    view             0.397370
    bathrooms        0.525906
    sqft_living15    0.585241
    sqft_above       0.605368
    grade            0.667951
    sqft_living      0.701917
    price            1.000000
    Name: price, dtype: float64




```python
# scatterplot price e sqft_living
plt.figure(figsize=(12,8))
sns.scatterplot(x='price',y='sqft_living',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba1a2c0c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_62_1.png)



```python
# boxplot bedrooms e price
sns.boxplot(x='bedrooms',y='price',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba29eb148>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_63_1.png)



```python
# boxplot waterfront e price
sns.boxplot(x='waterfront',y='price',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba4e746c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_64_1.png)


#### Geographical Properties 


```python
# il prezzo varia in funzione della longitudine?
plt.figure(figsize=(12,8))
sns.scatterplot(x='price',y='long',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba2eda608>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_66_1.png)



```python
# il prezzo varia in funzione della latitudine?
plt.figure(figsize=(12,8))
sns.scatterplot(x='price',y='lat',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba2f71108>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_67_1.png)



```python
# plot latitudine e longitudine (King County)
plt.figure(figsize=(12,8))
sns.scatterplot(x='long',y='lat',data=df,hue='price')
# il colore non è distribuito bene perché ci sono forti outliers che spingono la distribuzione verso il basso
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba2ebc508>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_68_1.png)



```python
# top 10 outliers per price
df.select_dtypes(include=np.number).sort_values('price',ascending=False).head(10).applymap(dot_sep)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>bedrooms</th>
      <th>bathrooms</th>
      <th>sqft_living</th>
      <th>sqft_lot</th>
      <th>floors</th>
      <th>waterfront</th>
      <th>view</th>
      <th>condition</th>
      <th>grade</th>
      <th>sqft_above</th>
      <th>sqft_basement</th>
      <th>yr_built</th>
      <th>yr_renovated</th>
      <th>lat</th>
      <th>long</th>
      <th>sqft_living15</th>
      <th>sqft_lot15</th>
      <th>month</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7245</th>
      <td>7.700.000</td>
      <td>6</td>
      <td>8,0</td>
      <td>12.050</td>
      <td>27.600</td>
      <td>2,5</td>
      <td>0</td>
      <td>3</td>
      <td>4</td>
      <td>13</td>
      <td>8.570</td>
      <td>3.480</td>
      <td>1.910</td>
      <td>1.987</td>
      <td>47</td>
      <td>-122</td>
      <td>3.940</td>
      <td>8.800</td>
      <td>10</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>3910</th>
      <td>7.060.000</td>
      <td>5</td>
      <td>4,5</td>
      <td>10.040</td>
      <td>37.325</td>
      <td>2,0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>11</td>
      <td>7.680</td>
      <td>2.360</td>
      <td>1.940</td>
      <td>2.001</td>
      <td>47</td>
      <td>-122</td>
      <td>3.930</td>
      <td>25.449</td>
      <td>6</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>9245</th>
      <td>6.890.000</td>
      <td>6</td>
      <td>7,8</td>
      <td>9.890</td>
      <td>31.374</td>
      <td>2,0</td>
      <td>0</td>
      <td>4</td>
      <td>3</td>
      <td>13</td>
      <td>8.860</td>
      <td>1.030</td>
      <td>2.001</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>4.540</td>
      <td>42.730</td>
      <td>9</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>4407</th>
      <td>5.570.000</td>
      <td>5</td>
      <td>5,8</td>
      <td>9.200</td>
      <td>35.069</td>
      <td>2,0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>13</td>
      <td>6.200</td>
      <td>3.000</td>
      <td>2.001</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>3.560</td>
      <td>24.345</td>
      <td>8</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>1446</th>
      <td>5.350.000</td>
      <td>5</td>
      <td>5,0</td>
      <td>8.000</td>
      <td>23.985</td>
      <td>2,0</td>
      <td>0</td>
      <td>4</td>
      <td>3</td>
      <td>12</td>
      <td>6.720</td>
      <td>1.280</td>
      <td>2.009</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>4.600</td>
      <td>21.750</td>
      <td>4</td>
      <td>2.015</td>
    </tr>
    <tr>
      <th>1313</th>
      <td>5.300.000</td>
      <td>6</td>
      <td>6,0</td>
      <td>7.390</td>
      <td>24.829</td>
      <td>2,0</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
      <td>12</td>
      <td>5.000</td>
      <td>2.390</td>
      <td>1.991</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>4.320</td>
      <td>24.619</td>
      <td>4</td>
      <td>2.015</td>
    </tr>
    <tr>
      <th>1162</th>
      <td>5.110.000</td>
      <td>5</td>
      <td>5,2</td>
      <td>8.010</td>
      <td>45.517</td>
      <td>2,0</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
      <td>12</td>
      <td>5.990</td>
      <td>2.020</td>
      <td>1.999</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>3.430</td>
      <td>26.788</td>
      <td>10</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>8085</th>
      <td>4.670.000</td>
      <td>5</td>
      <td>6,8</td>
      <td>9.640</td>
      <td>13.068</td>
      <td>1,0</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
      <td>12</td>
      <td>4.820</td>
      <td>4.820</td>
      <td>1.983</td>
      <td>2.009</td>
      <td>47</td>
      <td>-122</td>
      <td>3.270</td>
      <td>10.454</td>
      <td>6</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>2624</th>
      <td>4.500.000</td>
      <td>5</td>
      <td>5,5</td>
      <td>6.640</td>
      <td>40.014</td>
      <td>2,0</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
      <td>12</td>
      <td>6.350</td>
      <td>290</td>
      <td>2.004</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>3.030</td>
      <td>23.408</td>
      <td>8</td>
      <td>2.014</td>
    </tr>
    <tr>
      <th>8629</th>
      <td>4.490.000</td>
      <td>4</td>
      <td>3,0</td>
      <td>6.430</td>
      <td>27.517</td>
      <td>2,0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>12</td>
      <td>6.430</td>
      <td>0</td>
      <td>2.001</td>
      <td>0</td>
      <td>47</td>
      <td>-122</td>
      <td>3.720</td>
      <td>14.592</td>
      <td>6</td>
      <td>2.014</td>
    </tr>
  </tbody>
</table>
</div>




```python
# escludo l'1% di coda del dataset, cioè 216 osservazioni
non_top_1_perc = df.sort_values('price',ascending=False).iloc[216:]
```


```python
# plot latitudine e longitudine senza l'1% di coda
plt.figure(figsize=(12,8))
sns.scatterplot(x='long',y='lat',
                data=non_top_1_perc,hue='price',
                palette='RdYlGn',edgecolor=None,alpha=0.2)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba4fdd148>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_71_1.png)


### Feature Engineering


```python
# drop id
df = df.drop('id',axis=1)
```


```python
# engineering date
# convertiamo da string a time così è più semplice estrarre le info
df['date_string'] = df['date']
df['date'] = pd.to_datetime(df['date_string'])
df['month'] = df['date'].apply(lambda x: x.month)
df['year'] = df['date'].apply(lambda x: x.year)
df[['date_string','date','month','year']].info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 21597 entries, 0 to 21596
    Data columns (total 4 columns):
     #   Column       Non-Null Count  Dtype         
    ---  ------       --------------  -----         
     0   date_string  21597 non-null  object        
     1   date         21597 non-null  datetime64[ns]
     2   month        21597 non-null  int64         
     3   year         21597 non-null  int64         
    dtypes: datetime64[ns](1), int64(2), object(1)
    memory usage: 675.0+ KB
    


```python
# boxplot anno price
sns.boxplot(x='year',y='price',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba4e86308>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_75_1.png)



```python
# boxplot mese price
sns.boxplot(x='month',y='price',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba5505408>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_76_1.png)



```python
# andamento prezzo medio per mese
df.groupby('month').mean()['price'].plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba197cac8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_77_1.png)



```python
# andamento prezzo medio per anno
df.groupby('year').mean()['price'].plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18ba5090648>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_78_1.png)



```python
# escludo le variabili con le date complete
df = df.drop(['date','date_string'],axis=1)
df.columns
```




    Index(['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',
           'waterfront', 'view', 'condition', 'grade', 'sqft_above',
           'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',
           'sqft_living15', 'sqft_lot15', 'month', 'year'],
          dtype='object')




```python
# lo zip code va lavorato o escluso. lo escludiamo.
# se si include nel modello verrebbe considerato come numerico
# ci sono 70 zipcode diversi, quindi è un problema renderli dummy, si potrebbero raggruppare come zipcode più ricchi e meno ricchi, oppure fare raggruppamenti geografici come nord/centro/sud
df = df.drop('zipcode',axis=1)
```


```python
# la maggior parte dei yr_renoveted sono 0, si potrebbe discretizzare come chi ha rinnovato e chi no.
# inoltre c'è una correlazione, più è recente e più sono i casi
df['yr_renovated'].value_counts()
```




    0       20683
    2014       91
    2013       37
    2003       36
    2000       35
            ...  
    1934        1
    1959        1
    1951        1
    1948        1
    1944        1
    Name: yr_renovated, Length: 70, dtype: int64




```python
# la maggior parte dei sqft_basement sono 0, si potrebbe discretizzare come chi ha rinnovato e chi no
df['sqft_basement'].value_counts()
```




    0       13110
    600       221
    700       218
    500       214
    800       206
            ...  
    792         1
    2590        1
    935         1
    2390        1
    248         1
    Name: sqft_basement, Length: 306, dtype: int64



### Models


```python
# X e y
X = df.drop('price',axis=1)
y = df['price']
```


```python
# train test
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)
```


```python
# scaling Min Max
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train) # attenzione il fit solo sul train
X_test = scaler.transform(X_test)
print(X_train.shape)
print(X_test.shape)
```

    (15117, 19)
    (6480, 19)
    


```python
# definisco il modello
model = Sequential()

model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(1))

model.compile(optimizer='adam',loss='mse')
```


```python
# stimo il modello
model.fit(x=X_train,y=y_train.values,
          validation_data=(X_test,y_test.values),
          verbose=2,batch_size=128,epochs=400)
# il validation non viene utilizzato per il tuning solo per monitorare
```

    Train on 15117 samples, validate on 6480 samples
    Epoch 1/400
    15117/15117 - 0s - loss: 96382199451.0191 - val_loss: 92658238633.4025
    Epoch 2/400
    15117/15117 - 0s - loss: 94994401516.6776 - val_loss: 91311054099.5951
    Epoch 3/400
    15117/15117 - 1s - loss: 93410875942.7124 - val_loss: 89613097076.3062
    Epoch 4/400
    15117/15117 - 1s - loss: 91594379472.6340 - val_loss: 87911643191.6247
    Epoch 5/400
    15117/15117 - 1s - loss: 89785268532.0738 - val_loss: 86077981784.4938
    Epoch 6/400
    15117/15117 - 1s - loss: 87903035541.8371 - val_loss: 84208413122.0543
    Epoch 7/400
    15117/15117 - 0s - loss: 85900009524.8359 - val_loss: 82272137772.2469
    Epoch 8/400
    15117/15117 - 0s - loss: 83779520137.4072 - val_loss: 80173681211.4173
    Epoch 9/400
    15117/15117 - 1s - loss: 81564374363.5652 - val_loss: 78042089891.7136
    Epoch 10/400
    15117/15117 - 0s - loss: 79268053848.4832 - val_loss: 75732221891.3185
    Epoch 11/400
    15117/15117 - 1s - loss: 76915655088.5768 - val_loss: 73418169386.9827
    Epoch 12/400
    15117/15117 - 1s - loss: 74413598587.9103 - val_loss: 70981437455.1704
    Epoch 13/400
    15117/15117 - 1s - loss: 71885786211.3721 - val_loss: 68494647639.8617
    Epoch 14/400
    15117/15117 - 1s - loss: 69327291430.8141 - val_loss: 66017974014.1037
    Epoch 15/400
    15117/15117 - 1s - loss: 66752439184.0286 - val_loss: 63623139196.5235
    Epoch 16/400
    15117/15117 - 1s - loss: 64203828571.7007 - val_loss: 61173071609.0469
    Epoch 17/400
    15117/15117 - 1s - loss: 61777478090.7238 - val_loss: 58942340412.0494
    Epoch 18/400
    15117/15117 - 1s - loss: 59597344277.3037 - val_loss: 56880981606.4000
    Epoch 19/400
    15117/15117 - 1s - loss: 57576110078.5098 - val_loss: 55069754517.1753
    Epoch 20/400
    15117/15117 - 1s - loss: 55813330578.8905 - val_loss: 53573955755.9309
    Epoch 21/400
    15117/15117 - 1s - loss: 54325671621.0169 - val_loss: 52198033878.2815
    Epoch 22/400
    15117/15117 - 0s - loss: 53059952824.0450 - val_loss: 51114856609.8173
    Epoch 23/400
    15117/15117 - 0s - loss: 52015786524.1453 - val_loss: 50228348007.6642
    Epoch 24/400
    15117/15117 - 0s - loss: 51149181050.5386 - val_loss: 49483616513.8963
    Epoch 25/400
    15117/15117 - 0s - loss: 50404563013.4995 - val_loss: 48787835744.7111
    Epoch 26/400
    15117/15117 - 0s - loss: 49776502584.9848 - val_loss: 48215932108.8000
    Epoch 27/400
    15117/15117 - 1s - loss: 49240923008.7197 - val_loss: 47712266492.8395
    Epoch 28/400
    15117/15117 - 1s - loss: 48687526852.0516 - val_loss: 47283625478.3210
    Epoch 29/400
    15117/15117 - 1s - loss: 48266899480.6567 - val_loss: 46842397202.9630
    Epoch 30/400
    15117/15117 - 1s - loss: 47850088874.6159 - val_loss: 46445780531.8321
    Epoch 31/400
    15117/15117 - 0s - loss: 47413770742.4828 - val_loss: 46083885890.3704
    Epoch 32/400
    15117/15117 - 1s - loss: 47056228243.0768 - val_loss: 45733869997.8272
    Epoch 33/400
    15117/15117 - 0s - loss: 46675184100.6676 - val_loss: 45375999853.3531
    Epoch 34/400
    15117/15117 - 1s - loss: 46348657301.1259 - val_loss: 45024434828.3259
    Epoch 35/400
    15117/15117 - 0s - loss: 45983062871.5348 - val_loss: 44712840609.1852
    Epoch 36/400
    15117/15117 - 0s - loss: 45660238486.4807 - val_loss: 44403158074.1531
    Epoch 37/400
    15117/15117 - 0s - loss: 45368088331.6002 - val_loss: 44101016229.6099
    Epoch 38/400
    15117/15117 - 0s - loss: 45044097153.5156 - val_loss: 43825999164.0494
    Epoch 39/400
    15117/15117 - 0s - loss: 44731511926.6098 - val_loss: 43539596153.9951
    Epoch 40/400
    15117/15117 - 1s - loss: 44456134486.3155 - val_loss: 43277840834.0543
    Epoch 41/400
    15117/15117 - 1s - loss: 44201133009.0574 - val_loss: 42960031744.0000
    Epoch 42/400
    15117/15117 - 1s - loss: 43903007555.2133 - val_loss: 42682613891.4765
    Epoch 43/400
    15117/15117 - 0s - loss: 43578369197.0714 - val_loss: 42388351658.6667
    Epoch 44/400
    15117/15117 - 0s - loss: 43292981732.8031 - val_loss: 42144875112.9284
    Epoch 45/400
    15117/15117 - 0s - loss: 43009766911.2210 - val_loss: 41813878242.9235
    Epoch 46/400
    15117/15117 - 0s - loss: 42701147395.3700 - val_loss: 41521007492.1086
    Epoch 47/400
    15117/15117 - 0s - loss: 42452158896.4414 - val_loss: 41218798700.7210
    Epoch 48/400
    15117/15117 - 0s - loss: 42141818179.8568 - val_loss: 40929734286.8543
    Epoch 49/400
    15117/15117 - 0s - loss: 41826336970.1311 - val_loss: 40663645833.7975
    Epoch 50/400
    15117/15117 - 0s - loss: 41526693100.8809 - val_loss: 40343598835.9901
    Epoch 51/400
    15117/15117 - 0s - loss: 41251028730.8688 - val_loss: 40046360960.3160
    Epoch 52/400
    15117/15117 - 0s - loss: 40966983839.0495 - val_loss: 39691217204.4642
    Epoch 53/400
    15117/15117 - 0s - loss: 40623971579.2414 - val_loss: 39378883935.4469
    Epoch 54/400
    15117/15117 - 0s - loss: 40349222910.5098 - val_loss: 39067485383.7432
    Epoch 55/400
    15117/15117 - 1s - loss: 40001878185.4812 - val_loss: 38780266566.7951
    Epoch 56/400
    15117/15117 - 0s - loss: 39715802923.8436 - val_loss: 38454007251.7531
    Epoch 57/400
    15117/15117 - 0s - loss: 39397293327.8338 - val_loss: 38162901522.9630
    Epoch 58/400
    15117/15117 - 0s - loss: 39099080386.1719 - val_loss: 37927751417.0469
    Epoch 59/400
    15117/15117 - 0s - loss: 38882954586.3460 - val_loss: 37633700588.4049
    Epoch 60/400
    15117/15117 - 0s - loss: 38622394431.5385 - val_loss: 37407136022.1235
    Epoch 61/400
    15117/15117 - 0s - loss: 38447245874.6344 - val_loss: 37194798770.2519
    Epoch 62/400
    15117/15117 - 0s - loss: 38187949399.2300 - val_loss: 37161104659.5951
    Epoch 63/400
    15117/15117 - 0s - loss: 38106408696.5319 - val_loss: 36852668395.7728
    Epoch 64/400
    15117/15117 - 0s - loss: 37926267591.6587 - val_loss: 36708621916.2864
    Epoch 65/400
    15117/15117 - 0s - loss: 37774724998.3420 - val_loss: 36564405872.5136
    Epoch 66/400
    15117/15117 - 1s - loss: 37653751088.4837 - val_loss: 36454824380.9975
    Epoch 67/400
    15117/15117 - 0s - loss: 37538029741.1391 - val_loss: 36341487049.6395
    Epoch 68/400
    15117/15117 - 0s - loss: 37457998645.2592 - val_loss: 36240663625.3235
    Epoch 69/400
    15117/15117 - 0s - loss: 37323114620.2321 - val_loss: 36194452426.9037
    Epoch 70/400
    15117/15117 - 0s - loss: 37287700630.6500 - val_loss: 36134378498.5284
    Epoch 71/400
    15117/15117 - 0s - loss: 37177817626.9260 - val_loss: 36020861509.5309
    Epoch 72/400
    15117/15117 - 1s - loss: 37073063647.9767 - val_loss: 35957249638.4000
    Epoch 73/400
    15117/15117 - 1s - loss: 37012036411.3557 - val_loss: 35867675524.1086
    Epoch 74/400
    15117/15117 - 0s - loss: 36963503487.0601 - val_loss: 35775103744.6321
    Epoch 75/400
    15117/15117 - 0s - loss: 36890368560.2974 - val_loss: 35724740638.3407
    Epoch 76/400
    15117/15117 - 0s - loss: 36825623111.0914 - val_loss: 35649876301.7481
    Epoch 77/400
    15117/15117 - 1s - loss: 36752853493.9409 - val_loss: 35620320784.4346
    Epoch 78/400
    15117/15117 - 1s - loss: 36707304935.9190 - val_loss: 35533669995.4568
    Epoch 79/400
    15117/15117 - 0s - loss: 36650586227.1551 - val_loss: 35625816943.8815
    Epoch 80/400
    15117/15117 - 0s - loss: 36579882462.4357 - val_loss: 35507855258.8642
    Epoch 81/400
    15117/15117 - 0s - loss: 36540232030.2071 - val_loss: 35411452192.2370
    Epoch 82/400
    15117/15117 - 0s - loss: 36467540756.2029 - val_loss: 35326041636.6617
    Epoch 83/400
    15117/15117 - 0s - loss: 36396966415.9524 - val_loss: 35273838774.0444
    Epoch 84/400
    15117/15117 - 0s - loss: 36369798389.5514 - val_loss: 35230308266.0346
    Epoch 85/400
    15117/15117 - 1s - loss: 36314326696.8377 - val_loss: 35197720854.1235
    Epoch 86/400
    15117/15117 - 0s - loss: 36294566424.8261 - val_loss: 35156696000.7901
    Epoch 87/400
    15117/15117 - 0s - loss: 36228823907.8632 - val_loss: 35088967376.5926
    Epoch 88/400
    15117/15117 - 0s - loss: 36165337484.3368 - val_loss: 35061500652.4049
    Epoch 89/400
    15117/15117 - 1s - loss: 36135523979.0329 - val_loss: 35052024799.1309
    Epoch 90/400
    15117/15117 - 1s - loss: 36098325772.8533 - val_loss: 34956481715.5161
    Epoch 91/400
    15117/15117 - 0s - loss: 36034789331.1572 - val_loss: 34889820165.0568
    Epoch 92/400
    15117/15117 - 0s - loss: 35975936646.4267 - val_loss: 34843400404.3852
    Epoch 93/400
    15117/15117 - 1s - loss: 35921833928.4546 - val_loss: 34846331699.2000
    Epoch 94/400
    15117/15117 - 0s - loss: 35892720542.3891 - val_loss: 34881328621.0370
    Epoch 95/400
    15117/15117 - 1s - loss: 35866604788.8740 - val_loss: 34705517171.0420
    Epoch 96/400
    15117/15117 - 1s - loss: 35779354753.5156 - val_loss: 34696081003.4568
    Epoch 97/400
    15117/15117 - 1s - loss: 35720441217.7697 - val_loss: 34692212033.1062
    Epoch 98/400
    15117/15117 - 1s - loss: 35670033619.0048 - val_loss: 34554802540.0889
    Epoch 99/400
    15117/15117 - 1s - loss: 35675143933.5106 - val_loss: 34513613103.4074
    Epoch 100/400
    15117/15117 - 0s - loss: 35619031428.0728 - val_loss: 34516276737.2642
    Epoch 101/400
    15117/15117 - 0s - loss: 35543584112.0222 - val_loss: 34374561498.7062
    Epoch 102/400
    15117/15117 - 0s - loss: 35483151714.2036 - val_loss: 34303661801.8765
    Epoch 103/400
    15117/15117 - 1s - loss: 35414349849.6728 - val_loss: 34243086442.1926
    Epoch 104/400
    15117/15117 - 1s - loss: 35338422488.0852 - val_loss: 34168177479.4272
    Epoch 105/400
    15117/15117 - 0s - loss: 35277191110.6934 - val_loss: 34102363558.2420
    Epoch 106/400
    15117/15117 - 1s - loss: 35181959946.6518 - val_loss: 34104759526.0840
    Epoch 107/400
    15117/15117 - 1s - loss: 35154307302.3102 - val_loss: 33956388783.0914
    Epoch 108/400
    15117/15117 - 1s - loss: 35108652818.2385 - val_loss: 33910747601.2247
    Epoch 109/400
    15117/15117 - 0s - loss: 35042236498.3020 - val_loss: 33817658977.3432
    Epoch 110/400
    15117/15117 - 1s - loss: 34941988307.7330 - val_loss: 33736785763.2395
    Epoch 111/400
    15117/15117 - 1s - loss: 34869969672.2133 - val_loss: 33701185353.9556
    Epoch 112/400
    15117/15117 - 0s - loss: 34778433339.2202 - val_loss: 33608226816.0000
    Epoch 113/400
    15117/15117 - 0s - loss: 34722936994.7074 - val_loss: 33523722214.7160
    Epoch 114/400
    15117/15117 - 1s - loss: 34676566420.1267 - val_loss: 33480191620.7407
    Epoch 115/400
    15117/15117 - 0s - loss: 34622140247.3994 - val_loss: 33417095355.1012
    Epoch 116/400
    15117/15117 - 0s - loss: 34515787455.0220 - val_loss: 33327248351.1309
    Epoch 117/400
    15117/15117 - 0s - loss: 34527766469.6096 - val_loss: 33266678230.2815
    Epoch 118/400
    15117/15117 - 0s - loss: 34436009380.4517 - val_loss: 33221566762.3506
    Epoch 119/400
    15117/15117 - 0s - loss: 34375272688.2678 - val_loss: 33177433742.8543
    Epoch 120/400
    15117/15117 - 1s - loss: 34324169779.2102 - val_loss: 33096796650.5086
    Epoch 121/400
    15117/15117 - 0s - loss: 34270737448.0333 - val_loss: 33055997592.9679
    Epoch 122/400
    15117/15117 - 0s - loss: 34219058442.5502 - val_loss: 32986769969.3037
    Epoch 123/400
    15117/15117 - 0s - loss: 34186728899.1371 - val_loss: 32936072740.6617
    Epoch 124/400
    15117/15117 - 1s - loss: 34120449327.8063 - val_loss: 32989201729.1062
    Epoch 125/400
    15117/15117 - 0s - loss: 34102841919.6401 - val_loss: 32899263490.5284
    Epoch 126/400
    15117/15117 - 1s - loss: 34028145757.7160 - val_loss: 32817324001.6593
    Epoch 127/400
    15117/15117 - 0s - loss: 33985590435.3171 - val_loss: 32839641553.2247
    Epoch 128/400
    15117/15117 - 1s - loss: 33949278829.4312 - val_loss: 32693208961.5802
    Epoch 129/400
    15117/15117 - 0s - loss: 33900594134.4764 - val_loss: 32656656920.0198
    Epoch 130/400
    15117/15117 - 1s - loss: 33857432664.3985 - val_loss: 32602881107.4370
    Epoch 131/400
    15117/15117 - 1s - loss: 33833140522.5905 - val_loss: 32596770593.5012
    Epoch 132/400
    15117/15117 - 1s - loss: 33819343222.4574 - val_loss: 32552365645.1161
    Epoch 133/400
    15117/15117 - 1s - loss: 33774578175.4242 - val_loss: 32497370099.3580
    Epoch 134/400
    15117/15117 - 1s - loss: 33680378652.9073 - val_loss: 32473397318.7951
    Epoch 135/400
    15117/15117 - 0s - loss: 33706037484.6776 - val_loss: 32419003455.2099
    Epoch 136/400
    15117/15117 - 1s - loss: 33647157947.1948 - val_loss: 32367347009.1062
    Epoch 137/400
    15117/15117 - 0s - loss: 33556004034.6122 - val_loss: 32499263639.7037
    Epoch 138/400
    15117/15117 - 1s - loss: 33582652267.8563 - val_loss: 32297780231.5852
    Epoch 139/400
    15117/15117 - 1s - loss: 33537556566.5696 - val_loss: 32269941165.8272
    Epoch 140/400
    15117/15117 - 0s - loss: 33561267431.4618 - val_loss: 32313410911.4469
    Epoch 141/400
    15117/15117 - 0s - loss: 33518527720.6811 - val_loss: 32316860860.9975
    Epoch 142/400
    15117/15117 - 1s - loss: 33513696164.4855 - val_loss: 32180047963.0222
    Epoch 143/400
    15117/15117 - 0s - loss: 33448370938.1237 - val_loss: 32132477165.6691
    Epoch 144/400
    15117/15117 - 1s - loss: 33456552947.3329 - val_loss: 32127305611.6938
    Epoch 145/400
    15117/15117 - 0s - loss: 33422075509.3566 - val_loss: 32111520780.6420
    Epoch 146/400
    15117/15117 - 1s - loss: 33342879685.3386 - val_loss: 32021408861.5506
    Epoch 147/400
    15117/15117 - 1s - loss: 33309877002.6857 - val_loss: 32112171614.8148
    Epoch 148/400
    15117/15117 - 1s - loss: 33276879240.0355 - val_loss: 31982532491.6938
    Epoch 149/400
    15117/15117 - 1s - loss: 33261782121.7395 - val_loss: 31937626031.0914
    Epoch 150/400
    15117/15117 - 0s - loss: 33244380709.2899 - val_loss: 31897099645.7877
    Epoch 151/400
    15117/15117 - 1s - loss: 33204531397.5249 - val_loss: 31952423625.0074
    Epoch 152/400
    15117/15117 - 1s - loss: 33159851328.4699 - val_loss: 31809906662.7160
    Epoch 153/400
    15117/15117 - 0s - loss: 33164213061.6519 - val_loss: 31826173112.5728
    Epoch 154/400
    15117/15117 - 0s - loss: 33149994229.4159 - val_loss: 31788988026.6272
    Epoch 155/400
    15117/15117 - 0s - loss: 33055861106.8672 - val_loss: 31751747998.6568
    Epoch 156/400
    15117/15117 - 0s - loss: 33064231776.6795 - val_loss: 31677189274.2321
    Epoch 157/400
    15117/15117 - 0s - loss: 33035544087.0649 - val_loss: 31654303213.0370
    Epoch 158/400
    15117/15117 - 0s - loss: 33014836943.7195 - val_loss: 31677854838.8346
    Epoch 159/400
    15117/15117 - 0s - loss: 33012951491.2726 - val_loss: 31571845858.2914
    Epoch 160/400
    15117/15117 - 0s - loss: 33002356857.5225 - val_loss: 31571258107.5753
    Epoch 161/400
    15117/15117 - 0s - loss: 32940346050.2396 - val_loss: 31532592747.4568
    Epoch 162/400
    15117/15117 - 0s - loss: 32924320056.5446 - val_loss: 31485340467.2000
    Epoch 163/400
    15117/15117 - 0s - loss: 32888096524.8872 - val_loss: 31443621336.8099
    Epoch 164/400
    15117/15117 - 0s - loss: 32856383318.9252 - val_loss: 31428956483.6346
    Epoch 165/400
    15117/15117 - 0s - loss: 32829652860.5199 - val_loss: 31404898339.3975
    Epoch 166/400
    15117/15117 - 1s - loss: 32848878983.5952 - val_loss: 31450880399.4864
    Epoch 167/400
    15117/15117 - 0s - loss: 32851097056.0614 - val_loss: 31319563610.3901
    Epoch 168/400
    15117/15117 - 0s - loss: 32830653255.4131 - val_loss: 31320461327.1704
    Epoch 169/400
    15117/15117 - 0s - loss: 32756458012.6872 - val_loss: 31348614161.6988
    Epoch 170/400
    15117/15117 - 1s - loss: 32725146752.2286 - val_loss: 31252310005.8864
    Epoch 171/400
    15117/15117 - 1s - loss: 32728221920.7557 - val_loss: 31228474577.8568
    Epoch 172/400
    15117/15117 - 1s - loss: 32666303424.5969 - val_loss: 31160083529.3235
    Epoch 173/400
    15117/15117 - 0s - loss: 32685651930.4730 - val_loss: 31181256800.0790
    Epoch 174/400
    15117/15117 - 0s - loss: 32650936005.4233 - val_loss: 31128630006.5185
    Epoch 175/400
    15117/15117 - 1s - loss: 32653752904.3106 - val_loss: 31092346809.2049
    Epoch 176/400
    15117/15117 - 0s - loss: 32596710778.5894 - val_loss: 31070367374.8543
    Epoch 177/400
    15117/15117 - 0s - loss: 32580758086.4140 - val_loss: 31061738741.2543
    Epoch 178/400
    15117/15117 - 0s - loss: 32579106477.8504 - val_loss: 31044900257.1852
    Epoch 179/400
    15117/15117 - 0s - loss: 32524330684.8883 - val_loss: 30998606112.2370
    Epoch 180/400
    15117/15117 - 1s - loss: 32509149385.3182 - val_loss: 30977768526.3802
    Epoch 181/400
    15117/15117 - 0s - loss: 32519192971.0498 - val_loss: 30917689960.9284
    Epoch 182/400
    15117/15117 - 0s - loss: 32488585662.6664 - val_loss: 30986470822.2420
    Epoch 183/400
    15117/15117 - 0s - loss: 32508361810.8101 - val_loss: 30930998481.8568
    Epoch 184/400
    15117/15117 - 0s - loss: 32458454360.5848 - val_loss: 30884880823.9407
    Epoch 185/400
    15117/15117 - 1s - loss: 32439450562.0194 - val_loss: 30889956839.9802
    Epoch 186/400
    15117/15117 - 0s - loss: 32420746063.1014 - val_loss: 30814415485.1556
    Epoch 187/400
    15117/15117 - 0s - loss: 32412182624.0529 - val_loss: 30874558170.7062
    Epoch 188/400
    15117/15117 - 1s - loss: 32388199103.4623 - val_loss: 30789690572.8000
    Epoch 189/400
    15117/15117 - 0s - loss: 32394888274.4375 - val_loss: 30853651842.8444
    Epoch 190/400
    15117/15117 - 1s - loss: 32365784697.8273 - val_loss: 30700442843.9704
    Epoch 191/400
    15117/15117 - 1s - loss: 32342847068.6660 - val_loss: 30743840287.6049
    Epoch 192/400
    15117/15117 - 0s - loss: 32278152494.8580 - val_loss: 30670390168.3358
    Epoch 193/400
    15117/15117 - 0s - loss: 32287609340.8502 - val_loss: 30601137852.3654
    Epoch 194/400
    15117/15117 - 0s - loss: 32287648226.7032 - val_loss: 30600067350.1235
    Epoch 195/400
    15117/15117 - 0s - loss: 32236937336.1000 - val_loss: 30578090307.6346
    Epoch 196/400
    15117/15117 - 0s - loss: 32213621030.5262 - val_loss: 30600426478.3012
    Epoch 197/400
    15117/15117 - 1s - loss: 32213320922.1174 - val_loss: 30556590039.5457
    Epoch 198/400
    15117/15117 - 0s - loss: 32151398462.8611 - val_loss: 30510362138.5481
    Epoch 199/400
    15117/15117 - 0s - loss: 32163023889.2733 - val_loss: 30500078852.4247
    Epoch 200/400
    15117/15117 - 0s - loss: 32144396295.1803 - val_loss: 30458437212.2864
    Epoch 201/400
    15117/15117 - 0s - loss: 32099550475.9727 - val_loss: 30638830174.8148
    Epoch 202/400
    15117/15117 - 1s - loss: 32120172713.8877 - val_loss: 30404807460.0296
    Epoch 203/400
    15117/15117 - 0s - loss: 32079561130.1417 - val_loss: 30452079613.4716
    Epoch 204/400
    15117/15117 - 0s - loss: 32063493873.9951 - val_loss: 30422756142.1432
    Epoch 205/400
    15117/15117 - 0s - loss: 32078783177.1489 - val_loss: 30317234620.9975
    Epoch 206/400
    15117/15117 - 1s - loss: 32011063606.9188 - val_loss: 30286845780.0691
    Epoch 207/400
    15117/15117 - 0s - loss: 32014917424.2805 - val_loss: 30333206821.2938
    Epoch 208/400
    15117/15117 - 0s - loss: 32050411801.9268 - val_loss: 30292856907.8519
    Epoch 209/400
    15117/15117 - 0s - loss: 32004084841.6040 - val_loss: 30389932532.6222
    Epoch 210/400
    15117/15117 - 0s - loss: 31973731159.5348 - val_loss: 30246889021.9457
    Epoch 211/400
    15117/15117 - 0s - loss: 31941007444.1310 - val_loss: 30203081022.5778
    Epoch 212/400
    15117/15117 - 1s - loss: 31947657632.9970 - val_loss: 30179470854.3210
    Epoch 213/400
    15117/15117 - 0s - loss: 31934019085.9880 - val_loss: 30121629994.3506
    Epoch 214/400
    15117/15117 - 1s - loss: 31891701909.5662 - val_loss: 30199564839.1901
    Epoch 215/400
    15117/15117 - 0s - loss: 31870442644.5501 - val_loss: 30190855170.5284
    Epoch 216/400
    15117/15117 - 1s - loss: 31885774259.2186 - val_loss: 30121017308.6025
    Epoch 217/400
    15117/15117 - 0s - loss: 31883980103.1083 - val_loss: 30056250653.7086
    Epoch 218/400
    15117/15117 - 0s - loss: 31823757070.0388 - val_loss: 30022893067.3778
    Epoch 219/400
    15117/15117 - 0s - loss: 31786091260.0204 - val_loss: 29997039929.5210
    Epoch 220/400
    15117/15117 - 0s - loss: 31765212732.1855 - val_loss: 30007002766.8543
    Epoch 221/400
    15117/15117 - 1s - loss: 31775649963.1070 - val_loss: 29945241564.6025
    Epoch 222/400
    15117/15117 - 0s - loss: 31735229491.3456 - val_loss: 30009338523.4963
    Epoch 223/400
    15117/15117 - 0s - loss: 31731361875.8600 - val_loss: 29915092784.6716
    Epoch 224/400
    15117/15117 - 0s - loss: 31702922816.7917 - val_loss: 29940795953.3037
    Epoch 225/400
    15117/15117 - 1s - loss: 31715762084.9597 - val_loss: 30008074674.8839
    Epoch 226/400
    15117/15117 - 0s - loss: 31666888841.9152 - val_loss: 29860745529.5210
    Epoch 227/400
    15117/15117 - 1s - loss: 31665978213.5228 - val_loss: 29860679432.2173
    Epoch 228/400
    15117/15117 - 1s - loss: 31680815080.8335 - val_loss: 29794565183.2099
    Epoch 229/400
    15117/15117 - 0s - loss: 31628983220.0654 - val_loss: 29823692623.0123
    Epoch 230/400
    15117/15117 - 0s - loss: 31684664345.8083 - val_loss: 30055303567.4864
    Epoch 231/400
    15117/15117 - 0s - loss: 31682615177.9999 - val_loss: 29782461149.2346
    Epoch 232/400
    15117/15117 - 1s - loss: 31644598131.3753 - val_loss: 29759400932.1876
    Epoch 233/400
    15117/15117 - 0s - loss: 31588524446.0165 - val_loss: 29723706608.1975
    Epoch 234/400
    15117/15117 - 1s - loss: 31564011150.2843 - val_loss: 29703112410.7062
    Epoch 235/400
    15117/15117 - 0s - loss: 31551132813.2344 - val_loss: 29699699350.4395
    Epoch 236/400
    15117/15117 - 1s - loss: 31527584098.4068 - val_loss: 29675249689.2840
    Epoch 237/400
    15117/15117 - 0s - loss: 31514676425.9279 - val_loss: 29626825912.5728
    Epoch 238/400
    15117/15117 - 0s - loss: 31541906236.1686 - val_loss: 29623554720.5531
    Epoch 239/400
    15117/15117 - 0s - loss: 31505671806.2981 - val_loss: 29839937490.4889
    Epoch 240/400
    15117/15117 - 0s - loss: 31466861462.5315 - val_loss: 29580484026.4691
    Epoch 241/400
    15117/15117 - 0s - loss: 31450890957.7551 - val_loss: 29721106472.4543
    Epoch 242/400
    15117/15117 - 0s - loss: 31479004159.3904 - val_loss: 29614169828.8198
    Epoch 243/400
    15117/15117 - 1s - loss: 31441276018.6810 - val_loss: 29556154459.0222
    Epoch 244/400
    15117/15117 - 0s - loss: 31419770647.6576 - val_loss: 29509336248.5728
    Epoch 245/400
    15117/15117 - 0s - loss: 31397602399.0368 - val_loss: 29563192762.4691
    Epoch 246/400
    15117/15117 - 0s - loss: 31391722057.4622 - val_loss: 29460166433.5012
    Epoch 247/400
    15117/15117 - 0s - loss: 31357126748.2596 - val_loss: 29505953812.2272
    Epoch 248/400
    15117/15117 - 0s - loss: 31377676800.2371 - val_loss: 29458117447.4272
    Epoch 249/400
    15117/15117 - 0s - loss: 31342986515.8643 - val_loss: 29481420006.0840
    Epoch 250/400
    15117/15117 - 0s - loss: 31336613507.5817 - val_loss: 29431361596.6815
    Epoch 251/400
    15117/15117 - 0s - loss: 31347780469.3397 - val_loss: 29384939479.5457
    Epoch 252/400
    15117/15117 - 0s - loss: 31301204655.7470 - val_loss: 29392894204.8395
    Epoch 253/400
    15117/15117 - 0s - loss: 31283812432.8795 - val_loss: 29614380782.9333
    Epoch 254/400
    15117/15117 - 0s - loss: 31288708731.3176 - val_loss: 29501030101.6494
    Epoch 255/400
    15117/15117 - 0s - loss: 31261668768.4551 - val_loss: 29317917890.6864
    Epoch 256/400
    15117/15117 - 1s - loss: 31283342790.2531 - val_loss: 29325649783.4667
    Epoch 257/400
    15117/15117 - 0s - loss: 31246273519.2686 - val_loss: 29299652623.1704
    Epoch 258/400
    15117/15117 - 0s - loss: 31250550072.0704 - val_loss: 29374587845.8469
    Epoch 259/400
    15117/15117 - 0s - loss: 31219838134.7241 - val_loss: 29393745318.2420
    Epoch 260/400
    15117/15117 - 1s - loss: 31255405196.5909 - val_loss: 29220021048.2568
    Epoch 261/400
    15117/15117 - 0s - loss: 31213894080.1905 - val_loss: 29258744091.1802
    Epoch 262/400
    15117/15117 - 1s - loss: 31171800105.4220 - val_loss: 29239954904.8099
    Epoch 263/400
    15117/15117 - 0s - loss: 31169182211.2176 - val_loss: 29283175173.6889
    Epoch 264/400
    15117/15117 - 0s - loss: 31164321862.6511 - val_loss: 29177410254.0642
    Epoch 265/400
    15117/15117 - 0s - loss: 31160138646.6669 - val_loss: 29300618202.0741
    Epoch 266/400
    15117/15117 - 0s - loss: 31136234581.6212 - val_loss: 29168994546.7259
    Epoch 267/400
    15117/15117 - 0s - loss: 31123615188.2749 - val_loss: 29160304928.2370
    Epoch 268/400
    15117/15117 - 0s - loss: 31113875878.9580 - val_loss: 29077978372.4247
    Epoch 269/400
    15117/15117 - 0s - loss: 31127295052.2056 - val_loss: 29146043182.1432
    Epoch 270/400
    15117/15117 - 0s - loss: 31096204784.4541 - val_loss: 29097371390.1037
    Epoch 271/400
    15117/15117 - 0s - loss: 31055717089.4670 - val_loss: 29068690566.0049
    Epoch 272/400
    15117/15117 - 0s - loss: 31078008493.7149 - val_loss: 29035163597.4321
    Epoch 273/400
    15117/15117 - 0s - loss: 31072624334.2293 - val_loss: 29082797119.2099
    Epoch 274/400
    15117/15117 - 0s - loss: 31054589368.4006 - val_loss: 29082904985.6000
    Epoch 275/400
    15117/15117 - 0s - loss: 31037981700.5385 - val_loss: 29061136469.9654
    Epoch 276/400
    15117/15117 - 1s - loss: 31033748276.5141 - val_loss: 29073894958.7753
    Epoch 277/400
    15117/15117 - 0s - loss: 31040024804.1426 - val_loss: 28999788144.5136
    Epoch 278/400
    15117/15117 - 0s - loss: 31002192142.0726 - val_loss: 29078248308.9383
    Epoch 279/400
    15117/15117 - 1s - loss: 30999562141.4408 - val_loss: 29026543803.1012
    Epoch 280/400
    15117/15117 - 1s - loss: 30954945438.9987 - val_loss: 29067491919.6444
    Epoch 281/400
    15117/15117 - 0s - loss: 30994941768.1582 - val_loss: 28936822000.1975
    Epoch 282/400
    15117/15117 - 0s - loss: 31063223905.7125 - val_loss: 29005493139.2790
    Epoch 283/400
    15117/15117 - 1s - loss: 30975074903.8227 - val_loss: 28879479375.6444
    Epoch 284/400
    15117/15117 - 1s - loss: 30947251155.1572 - val_loss: 28905287113.6395
    Epoch 285/400
    15117/15117 - 0s - loss: 30948623945.6823 - val_loss: 28887165241.5210
    Epoch 286/400
    15117/15117 - 0s - loss: 30913117069.9626 - val_loss: 28972073556.7012
    Epoch 287/400
    15117/15117 - 0s - loss: 30928486296.9023 - val_loss: 28944066216.1383
    Epoch 288/400
    15117/15117 - 1s - loss: 30897454282.4698 - val_loss: 28986663508.7012
    Epoch 289/400
    15117/15117 - 1s - loss: 30900665758.8294 - val_loss: 28794086589.6296
    Epoch 290/400
    15117/15117 - 0s - loss: 30856651859.9278 - val_loss: 28820780186.2321
    Epoch 291/400
    15117/15117 - 0s - loss: 30864012023.2110 - val_loss: 28847218465.5012
    Epoch 292/400
    15117/15117 - 0s - loss: 30827163076.0177 - val_loss: 28800990443.1407
    Epoch 293/400
    15117/15117 - 0s - loss: 30824707928.3138 - val_loss: 28764842469.4519
    Epoch 294/400
    15117/15117 - 0s - loss: 30808299648.2964 - val_loss: 28821795377.3037
    Epoch 295/400
    15117/15117 - 0s - loss: 30808336993.6448 - val_loss: 28762178618.1531
    Epoch 296/400
    15117/15117 - 1s - loss: 30795065044.7999 - val_loss: 28709923893.0963
    Epoch 297/400
    15117/15117 - 0s - loss: 30783520923.3917 - val_loss: 28777768874.0346
    Epoch 298/400
    15117/15117 - 0s - loss: 30793315197.0619 - val_loss: 28671690650.8642
    Epoch 299/400
    15117/15117 - 0s - loss: 30780910809.2368 - val_loss: 28744985642.9827
    Epoch 300/400
    15117/15117 - 0s - loss: 30730057292.9846 - val_loss: 29007355954.5679
    Epoch 301/400
    15117/15117 - 0s - loss: 30856403492.1723 - val_loss: 28745739276.6420
    Epoch 302/400
    15117/15117 - 0s - loss: 30747627343.4740 - val_loss: 28714740763.8124
    Epoch 303/400
    15117/15117 - 0s - loss: 30743646337.7866 - val_loss: 28706931304.9284
    Epoch 304/400
    15117/15117 - 0s - loss: 30758299543.2766 - val_loss: 28615069610.0346
    Epoch 305/400
    15117/15117 - 0s - loss: 30723025473.9432 - val_loss: 28570402886.7951
    Epoch 306/400
    15117/15117 - 0s - loss: 30664183044.0474 - val_loss: 28555384518.4790
    Epoch 307/400
    15117/15117 - 0s - loss: 30642402582.8447 - val_loss: 28551187954.0938
    Epoch 308/400
    15117/15117 - 0s - loss: 30681753069.2026 - val_loss: 28610615311.1704
    Epoch 309/400
    15117/15117 - 1s - loss: 30660929539.1498 - val_loss: 28565303012.8198
    Epoch 310/400
    15117/15117 - 0s - loss: 30644601427.2842 - val_loss: 28552869579.5358
    Epoch 311/400
    15117/15117 - 0s - loss: 30640831392.1503 - val_loss: 28538744902.7951
    Epoch 312/400
    15117/15117 - 0s - loss: 30605267886.6463 - val_loss: 28477594752.9481
    Epoch 313/400
    15117/15117 - 0s - loss: 30586558412.0786 - val_loss: 28497673744.4346
    Epoch 314/400
    15117/15117 - 0s - loss: 30565745922.8958 - val_loss: 28453045184.7901
    Epoch 315/400
    15117/15117 - 0s - loss: 30565341926.5473 - val_loss: 28571571278.3802
    Epoch 316/400
    15117/15117 - 0s - loss: 30583935293.9636 - val_loss: 28418162779.0222
    Epoch 317/400
    15117/15117 - 0s - loss: 30539795481.0462 - val_loss: 28545992612.9778
    Epoch 318/400
    15117/15117 - 0s - loss: 30561424856.4747 - val_loss: 28404630540.6420
    Epoch 319/400
    15117/15117 - 0s - loss: 30545064092.3400 - val_loss: 28438870400.3160
    Epoch 320/400
    15117/15117 - 0s - loss: 30617606081.3421 - val_loss: 28449375760.4346
    Epoch 321/400
    15117/15117 - 0s - loss: 30552294173.8895 - val_loss: 28380631793.4617
    Epoch 322/400
    15117/15117 - 0s - loss: 30524515455.9577 - val_loss: 28382479483.8914
    Epoch 323/400
    15117/15117 - 0s - loss: 30476172662.2880 - val_loss: 28465059147.2198
    Epoch 324/400
    15117/15117 - 1s - loss: 30494940332.5295 - val_loss: 28324749145.1259
    Epoch 325/400
    15117/15117 - 1s - loss: 30496681947.7600 - val_loss: 28294857138.8839
    Epoch 326/400
    15117/15117 - 1s - loss: 30475034713.4146 - val_loss: 28323286397.7877
    Epoch 327/400
    15117/15117 - 0s - loss: 30498148420.8899 - val_loss: 28300493376.4741
    Epoch 328/400
    15117/15117 - 1s - loss: 30452896281.8422 - val_loss: 28306543689.3235
    Epoch 329/400
    15117/15117 - 1s - loss: 30414550551.8778 - val_loss: 28234074607.5654
    Epoch 330/400
    15117/15117 - 1s - loss: 30372275140.0516 - val_loss: 28309429073.5407
    Epoch 331/400
    15117/15117 - 0s - loss: 30406209019.4277 - val_loss: 28338454947.7136
    Epoch 332/400
    15117/15117 - 1s - loss: 30358639263.1512 - val_loss: 28274023021.9852
    Epoch 333/400
    15117/15117 - 0s - loss: 30407204270.6124 - val_loss: 28191584822.3605
    Epoch 334/400
    15117/15117 - 1s - loss: 30391821569.4733 - val_loss: 28244395298.7654
    Epoch 335/400
    15117/15117 - 0s - loss: 30354910270.9289 - val_loss: 28175038868.5432
    Epoch 336/400
    15117/15117 - 1s - loss: 30334568016.9134 - val_loss: 28204559311.9605
    Epoch 337/400
    15117/15117 - 0s - loss: 30311353607.1633 - val_loss: 28179725592.6519
    Epoch 338/400
    15117/15117 - 0s - loss: 30335527654.2086 - val_loss: 28168000160.5531
    Epoch 339/400
    15117/15117 - 0s - loss: 30294694004.5776 - val_loss: 28172753088.1580
    Epoch 340/400
    15117/15117 - 0s - loss: 30290705104.3292 - val_loss: 28136956788.9383
    Epoch 341/400
    15117/15117 - 0s - loss: 30296003861.9303 - val_loss: 28136771321.0469
    Epoch 342/400
    15117/15117 - 0s - loss: 30302682805.4371 - val_loss: 28135340623.6444
    Epoch 343/400
    15117/15117 - 1s - loss: 30260136009.0219 - val_loss: 28208574858.4296
    Epoch 344/400
    15117/15117 - 0s - loss: 30253894699.9622 - val_loss: 28129751667.0420
    Epoch 345/400
    15117/15117 - 0s - loss: 30291949776.0921 - val_loss: 28040967640.8099
    Epoch 346/400
    15117/15117 - 0s - loss: 30210169249.8099 - val_loss: 28082471205.2938
    Epoch 347/400
    15117/15117 - 0s - loss: 30223052560.1048 - val_loss: 28013991951.1704
    Epoch 348/400
    15117/15117 - 0s - loss: 30216497944.1318 - val_loss: 28058420449.0272
    Epoch 349/400
    15117/15117 - 0s - loss: 30213122478.8156 - val_loss: 27970074300.3654
    Epoch 350/400
    15117/15117 - 0s - loss: 30162114295.9561 - val_loss: 28148819937.6593
    Epoch 351/400
    15117/15117 - 1s - loss: 30173520638.6283 - val_loss: 27941060815.3284
    Epoch 352/400
    15117/15117 - 0s - loss: 30152112959.5555 - val_loss: 27985565448.2173
    Epoch 353/400
    15117/15117 - 1s - loss: 30144138108.8586 - val_loss: 27944158852.7407
    Epoch 354/400
    15117/15117 - 0s - loss: 30105428899.1646 - val_loss: 27936855234.6864
    Epoch 355/400
    15117/15117 - 0s - loss: 30111708380.4882 - val_loss: 27904942980.1086
    Epoch 356/400
    15117/15117 - 0s - loss: 30078203069.4641 - val_loss: 27945916390.7160
    Epoch 357/400
    15117/15117 - 1s - loss: 30097687528.7658 - val_loss: 27943533368.2568
    Epoch 358/400
    15117/15117 - 0s - loss: 30075058843.9675 - val_loss: 27867905830.5580
    Epoch 359/400
    15117/15117 - 0s - loss: 30046066010.2105 - val_loss: 27839719707.1802
    Epoch 360/400
    15117/15117 - 0s - loss: 30051817896.2450 - val_loss: 28064850309.3728
    Epoch 361/400
    15117/15117 - 0s - loss: 30098086733.6450 - val_loss: 27801436594.8839
    Epoch 362/400
    15117/15117 - 0s - loss: 29988494153.0388 - val_loss: 27776880791.7037
    Epoch 363/400
    15117/15117 - 0s - loss: 29978809103.8677 - val_loss: 27850496237.6691
    Epoch 364/400
    15117/15117 - 0s - loss: 30053654271.0009 - val_loss: 27737326791.7432
    Epoch 365/400
    15117/15117 - 1s - loss: 29953017918.9966 - val_loss: 27783360683.9309
    Epoch 366/400
    15117/15117 - 0s - loss: 29995678151.2014 - val_loss: 27702973629.6296
    Epoch 367/400
    15117/15117 - 0s - loss: 29956884430.2801 - val_loss: 27687872360.2963
    Epoch 368/400
    15117/15117 - 0s - loss: 29944092735.6740 - val_loss: 27798962782.8148
    Epoch 369/400
    15117/15117 - 0s - loss: 29945528571.6478 - val_loss: 27692253194.1136
    Epoch 370/400
    15117/15117 - 0s - loss: 29889021824.2456 - val_loss: 27696133461.3333
    Epoch 371/400
    15117/15117 - 0s - loss: 29892324245.1090 - val_loss: 27631663058.4889
    Epoch 372/400
    15117/15117 - 0s - loss: 29868505340.5284 - val_loss: 27613904238.6173
    Epoch 373/400
    15117/15117 - 0s - loss: 29865625274.1787 - val_loss: 27571374535.1111
    Epoch 374/400
    15117/15117 - 0s - loss: 29871232669.5932 - val_loss: 27622179443.0420
    Epoch 375/400
    15117/15117 - 0s - loss: 29869303884.6798 - val_loss: 27543974848.7901
    Epoch 376/400
    15117/15117 - 0s - loss: 29849328800.4720 - val_loss: 27565010084.3457
    Epoch 377/400
    15117/15117 - 0s - loss: 29783060215.2787 - val_loss: 27494700661.5704
    Epoch 378/400
    15117/15117 - 0s - loss: 29823125884.1474 - val_loss: 27490401727.5259
    Epoch 379/400
    15117/15117 - 0s - loss: 29737528964.9364 - val_loss: 27458872094.9728
    Epoch 380/400
    15117/15117 - 0s - loss: 29885012566.9421 - val_loss: 27489482418.2519
    Epoch 381/400
    15117/15117 - 0s - loss: 29766570705.0065 - val_loss: 27521214104.9679
    Epoch 382/400
    15117/15117 - 0s - loss: 29782005293.0121 - val_loss: 27428663437.5901
    Epoch 383/400
    15117/15117 - 0s - loss: 29675071534.8749 - val_loss: 27508872133.8469
    Epoch 384/400
    15117/15117 - 0s - loss: 29681066891.4224 - val_loss: 27453495513.4420
    Epoch 385/400
    15117/15117 - 0s - loss: 29662169229.2344 - val_loss: 27494721859.6346
    Epoch 386/400
    15117/15117 - 0s - loss: 29625930530.5635 - val_loss: 27402251286.7556
    Epoch 387/400
    15117/15117 - 0s - loss: 29658169965.0925 - val_loss: 27345191169.8963
    Epoch 388/400
    15117/15117 - 0s - loss: 29608757997.8631 - val_loss: 27315878413.9062
    Epoch 389/400
    15117/15117 - 0s - loss: 29587062468.3395 - val_loss: 27568016330.9037
    Epoch 390/400
    15117/15117 - 0s - loss: 29587522808.1254 - val_loss: 27289794236.3654
    Epoch 391/400
    15117/15117 - 1s - loss: 29552206517.3693 - val_loss: 27263435647.0519
    Epoch 392/400
    15117/15117 - 0s - loss: 29560209898.0867 - val_loss: 27298413585.6988
    Epoch 393/400
    15117/15117 - 0s - loss: 29583902307.6769 - val_loss: 27253922156.0889
    Epoch 394/400
    15117/15117 - 0s - loss: 29508609251.8039 - val_loss: 27287940219.8914
    Epoch 395/400
    15117/15117 - 1s - loss: 29511055772.7972 - val_loss: 27179846046.6568
    Epoch 396/400
    15117/15117 - 0s - loss: 29492277743.5057 - val_loss: 27250345807.0123
    Epoch 397/400
    15117/15117 - 0s - loss: 29472323514.1279 - val_loss: 27184283946.3506
    Epoch 398/400
    15117/15117 - 0s - loss: 29447539161.6262 - val_loss: 27292158017.7383
    Epoch 399/400
    15117/15117 - 0s - loss: 29409902538.3513 - val_loss: 27112177249.3432
    Epoch 400/400
    15117/15117 - 0s - loss: 29373463675.0805 - val_loss: 27157900968.1383
    




    <tensorflow.python.keras.callbacks.History at 0x18bafce7548>




```python
# andamento loss (mse)
losses = pd.DataFrame(model.history.history)
losses.plot()
# val_loss è sul test set, utile per capire se sto facendo overfit. non stiamo facendo overfit, potremmo continuare con le epochs.
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18bafd17f48>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_89_1.png)



```python
# predictions
predictions = model.predict(X_test)
```


```python
# metrics
print('MAE:',dot_sep(mean_absolute_error(y_test,predictions)))
print('MSE:',dot_sep(mean_squared_error(y_test,predictions)))
print('RMSE:',dot_sep(mean_squared_error(y_test,predictions)**0.5))
print('Explained Var Score:',dot_sep(explained_variance_score(y_test,predictions)))
```

    MAE: 99.996
    MSE: 27.157.901.022
    RMSE: 164.796
    Explained Var Score: 0,8
    


```python
print('Media Price:',dot_sep(df['price'].mean()))
print('Mediana Price:',dot_sep(df['price'].median()))
# il MAE è di circa 100k, quindi un 20% della media
```

    Media Price: 540.296
    Mediana Price: 450.000
    


```python
# plot osservate vs previste
plt.scatter(y_test,predictions,edgecolors='black',alpha=0.5)
plt.plot(y_test,y_test,'r')
```




    [<matplotlib.lines.Line2D at 0x18bafcd3948>]




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_93_1.png)



```python
# distribuzione errori (devo formattarli uguali)
print(type(y_test))
print(type(predictions))
print(type(y_test.values.reshape(6480, 1)))
```

    <class 'pandas.core.series.Series'>
    <class 'numpy.ndarray'>
    <class 'numpy.ndarray'>
    


```python
# distribuzione errori
errors = y_test.values.reshape(6480, 1) - predictions
sns.distplot(errors)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18bb3093288>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_95_1.png)


### Prediction on new observation


```python
# esempio da prevedere
single_house = df.drop('price',axis=1).iloc[0]
single_house
```




    bedrooms            3.0000
    bathrooms           1.0000
    sqft_living      1180.0000
    sqft_lot         5650.0000
    floors              1.0000
    waterfront          0.0000
    view                0.0000
    condition           3.0000
    grade               7.0000
    sqft_above       1180.0000
    sqft_basement       0.0000
    yr_built         1955.0000
    yr_renovated        0.0000
    lat                47.5112
    long             -122.2570
    sqft_living15    1340.0000
    sqft_lot15       5650.0000
    month              10.0000
    year             2014.0000
    Name: 0, dtype: float64




```python
# bisogna scalarlo e renderlo in formato vettore
single_house = scaler.transform(single_house.values.reshape(-1, 19))
single_house
```




    array([[0.2       , 0.08      , 0.08376422, 0.00310751, 0.        ,
            0.        , 0.        , 0.5       , 0.4       , 0.10785619,
            0.        , 0.47826087, 0.        , 0.57149751, 0.21760797,
            0.16193426, 0.00582059, 0.81818182, 0.        ]])




```python
# sappiamo qual è il target osservato
df.head(1)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>bedrooms</th>
      <th>bathrooms</th>
      <th>sqft_living</th>
      <th>sqft_lot</th>
      <th>floors</th>
      <th>waterfront</th>
      <th>view</th>
      <th>condition</th>
      <th>grade</th>
      <th>sqft_above</th>
      <th>sqft_basement</th>
      <th>yr_built</th>
      <th>yr_renovated</th>
      <th>lat</th>
      <th>long</th>
      <th>sqft_living15</th>
      <th>sqft_lot15</th>
      <th>month</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>221900.0</td>
      <td>3</td>
      <td>1.0</td>
      <td>1180</td>
      <td>5650</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>7</td>
      <td>1180</td>
      <td>0</td>
      <td>1955</td>
      <td>0</td>
      <td>47.5112</td>
      <td>-122.257</td>
      <td>1340</td>
      <td>5650</td>
      <td>10</td>
      <td>2014</td>
    </tr>
  </tbody>
</table>
</div>




```python
# previsione
model.predict(single_house)
# si potrebbe migliorare il modello escludendo l'1% degli outliers e facendo più feature engineering
```




    array([[294433.44]], dtype=float32)



## Breast cancer Wisconsin
Keras Classification


```python
# lib
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report,confusion_matrix

import random as rn
import os

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation,Dropout
from tensorflow.keras.callbacks import EarlyStopping
```


```python
# df
df = pd.read_csv('Refactored_Py_DS_ML_Bootcamp-master/22-Deep Learning/TensorFlow_FILES/DATA/cancer_classification.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
      <th>benign_0__mal_1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.3001</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>...</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.1622</td>
      <td>0.6656</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.0869</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>...</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.1238</td>
      <td>0.1866</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.1974</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>...</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.1444</td>
      <td>0.4245</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.2414</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>0.09744</td>
      <td>...</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.2098</td>
      <td>0.8663</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.1980</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>0.05883</td>
      <td>...</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.1374</td>
      <td>0.2050</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>
</div>




```python
# controllo missing
df.isnull().sum().sum()
```




    0




```python
# per mostrare i separatori delle migliaia come punti e decimali come virgola
dot_sep = lambda x: format(round(x,2) if abs(x) < 1 else round(x,1) if abs(x) < 10 else int(x), ',').replace(",", "X").replace(".", ",").replace("X", ".")
```


```python
# describe, potrei usare il .transpose, ma preferico così e miglioro i decimali
df.describe(percentiles=[0.25,0.5,0.75,0.999]).applymap(dot_sep)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
      <th>benign_0__mal_1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>...</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
      <td>569</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>14</td>
      <td>19</td>
      <td>91</td>
      <td>654</td>
      <td>0,1</td>
      <td>0,1</td>
      <td>0,09</td>
      <td>0,05</td>
      <td>0,18</td>
      <td>0,06</td>
      <td>...</td>
      <td>25</td>
      <td>107</td>
      <td>880</td>
      <td>0,13</td>
      <td>0,25</td>
      <td>0,27</td>
      <td>0,11</td>
      <td>0,29</td>
      <td>0,08</td>
      <td>0,63</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3,5</td>
      <td>4,3</td>
      <td>24</td>
      <td>351</td>
      <td>0,01</td>
      <td>0,05</td>
      <td>0,08</td>
      <td>0,04</td>
      <td>0,03</td>
      <td>0,01</td>
      <td>...</td>
      <td>6,1</td>
      <td>33</td>
      <td>569</td>
      <td>0,02</td>
      <td>0,16</td>
      <td>0,21</td>
      <td>0,07</td>
      <td>0,06</td>
      <td>0,02</td>
      <td>0,48</td>
    </tr>
    <tr>
      <th>min</th>
      <td>7,0</td>
      <td>9,7</td>
      <td>43</td>
      <td>143</td>
      <td>0,05</td>
      <td>0,02</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>0,11</td>
      <td>0,05</td>
      <td>...</td>
      <td>12</td>
      <td>50</td>
      <td>185</td>
      <td>0,07</td>
      <td>0,03</td>
      <td>0,0</td>
      <td>0,0</td>
      <td>0,16</td>
      <td>0,06</td>
      <td>0,0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>11</td>
      <td>16</td>
      <td>75</td>
      <td>420</td>
      <td>0,09</td>
      <td>0,06</td>
      <td>0,03</td>
      <td>0,02</td>
      <td>0,16</td>
      <td>0,06</td>
      <td>...</td>
      <td>21</td>
      <td>84</td>
      <td>515</td>
      <td>0,12</td>
      <td>0,15</td>
      <td>0,11</td>
      <td>0,06</td>
      <td>0,25</td>
      <td>0,07</td>
      <td>0,0</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>13</td>
      <td>18</td>
      <td>86</td>
      <td>551</td>
      <td>0,1</td>
      <td>0,09</td>
      <td>0,06</td>
      <td>0,03</td>
      <td>0,18</td>
      <td>0,06</td>
      <td>...</td>
      <td>25</td>
      <td>97</td>
      <td>686</td>
      <td>0,13</td>
      <td>0,21</td>
      <td>0,23</td>
      <td>0,1</td>
      <td>0,28</td>
      <td>0,08</td>
      <td>1,0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>15</td>
      <td>21</td>
      <td>104</td>
      <td>782</td>
      <td>0,11</td>
      <td>0,13</td>
      <td>0,13</td>
      <td>0,07</td>
      <td>0,2</td>
      <td>0,07</td>
      <td>...</td>
      <td>29</td>
      <td>125</td>
      <td>1.084</td>
      <td>0,15</td>
      <td>0,34</td>
      <td>0,38</td>
      <td>0,16</td>
      <td>0,32</td>
      <td>0,09</td>
      <td>1,0</td>
    </tr>
    <tr>
      <th>99.9%</th>
      <td>27</td>
      <td>36</td>
      <td>187</td>
      <td>2.499</td>
      <td>0,15</td>
      <td>0,33</td>
      <td>0,43</td>
      <td>0,2</td>
      <td>0,3</td>
      <td>0,1</td>
      <td>...</td>
      <td>48</td>
      <td>238</td>
      <td>3.787</td>
      <td>0,22</td>
      <td>0,99</td>
      <td>1,2</td>
      <td>0,29</td>
      <td>0,61</td>
      <td>0,19</td>
      <td>1,0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>28</td>
      <td>39</td>
      <td>188</td>
      <td>2.501</td>
      <td>0,16</td>
      <td>0,35</td>
      <td>0,43</td>
      <td>0,2</td>
      <td>0,3</td>
      <td>0,1</td>
      <td>...</td>
      <td>49</td>
      <td>251</td>
      <td>4.254</td>
      <td>0,22</td>
      <td>1,1</td>
      <td>1,3</td>
      <td>0,29</td>
      <td>0,66</td>
      <td>0,21</td>
      <td>1,0</td>
    </tr>
  </tbody>
</table>
<p>9 rows × 31 columns</p>
</div>




```python
# verifichiamo la distribuzione target
ax = sns.countplot(x='benign_0__mal_1',data=df)
# non è troppo sbilanciato

# aggiungo le frequenze
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height + 3,
            '{:1.0f}'.format(height), # '{:1.2f}'.format(height/float(len(df)))
            ha="center")
```


![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_107_0.png)



```python
# heatmap
plt.figure(figsize=(12,12))
sns.heatmap(df.corr())
```




    <matplotlib.axes._subplots.AxesSubplot at 0x26b02b93048>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_108_1.png)



```python
# head e tail delle variabili più correlate con la target
pd.concat([df.corr()['benign_0__mal_1'].sort_values().head(),df.corr()['benign_0__mal_1'].sort_values().tail()])
```




    worst concave points     -0.793566
    worst perimeter          -0.782914
    mean concave points      -0.776614
    worst radius             -0.776454
    mean perimeter           -0.742636
    symmetry error            0.006522
    texture error             0.008303
    mean fractal dimension    0.012838
    smoothness error          0.067016
    benign_0__mal_1           1.000000
    Name: benign_0__mal_1, dtype: float64




```python
# correlate con la target (escludo la target)
df.corr()['benign_0__mal_1'][:-1].sort_values().plot(kind='bar')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x26b02bbbb88>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_110_1.png)


### Model


```python
# X e y (come numpy arrays)
X = df.drop('benign_0__mal_1',axis=1).values
y = df['benign_0__mal_1'].values
```


```python
# train e test
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)
```


```python
# scaling data
scaler = MinMaxScaler()
# scaler.fit(X_train) # stima i parametri
# X_train = scaler.transform(X_train) # applica i parametri
# X_test = scaler.transform(X_test)
X_train = scaler.fit_transform(X_train) # stima e applica i parametri in un solo comando
X_test = scaler.transform(X_test) # avevo erroneamente fatto il fit, non andava messo, si otterranno risultati leggermente diversi
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# definizione struttura neural network per classificazione
model = Sequential()
# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw
model.add(Dense(units=30,activation='relu'))
model.add(Dense(units=15,activation='relu'))
model.add(Dense(units=1,activation='sigmoid'))
# For a binary classification problem
model.compile(loss='binary_crossentropy', optimizer='adam')
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
%%time
# stima modello con overfitting
# https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network
# https://datascience.stackexchange.com/questions/18414/are-there-any-rules-for-choosing-the-size-of-a-mini-batch
model.fit(x=X_train, 
          y=y_train, 
          epochs=600,
          validation_data=(X_test, y_test), verbose=0
          )
```

    Wall time: 35.6 s
    




    <tensorflow.python.keras.callbacks.History at 0x26b07120608>




```python
# loss crossentropy
model_loss = pd.DataFrame(model.history.history)
model_loss.plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x26b05f654c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_119_1.png)


#### Early Stopping


```python
# 'Stop training when a monitored quantity has stopped improving'
help(EarlyStopping)
```

    Help on class EarlyStopping in module tensorflow.python.keras.callbacks:
    
    class EarlyStopping(Callback)
     |  EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)
     |  
     |  Stop training when a monitored quantity has stopped improving.
     |  
     |  Arguments:
     |      monitor: Quantity to be monitored.
     |      min_delta: Minimum change in the monitored quantity
     |          to qualify as an improvement, i.e. an absolute
     |          change of less than min_delta, will count as no
     |          improvement.
     |      patience: Number of epochs with no improvement
     |          after which training will be stopped.
     |      verbose: verbosity mode.
     |      mode: One of `{"auto", "min", "max"}`. In `min` mode,
     |          training will stop when the quantity
     |          monitored has stopped decreasing; in `max`
     |          mode it will stop when the quantity
     |          monitored has stopped increasing; in `auto`
     |          mode, the direction is automatically inferred
     |          from the name of the monitored quantity.
     |      baseline: Baseline value for the monitored quantity.
     |          Training will stop if the model doesn't show improvement over the
     |          baseline.
     |      restore_best_weights: Whether to restore model weights from
     |          the epoch with the best value of the monitored quantity.
     |          If False, the model weights obtained at the last step of
     |          training are used.
     |  
     |  Example:
     |  
     |  ```python
     |  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)
     |  # This callback will stop the training when there is no improvement in
     |  # the validation loss for three consecutive epochs.
     |  model.fit(data, labels, epochs=100, callbacks=[callback],
     |      validation_data=(val_data, val_labels))
     |  ```
     |  
     |  Method resolution order:
     |      EarlyStopping
     |      Callback
     |      builtins.object
     |  
     |  Methods defined here:
     |  
     |  __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  get_monitor_value(self, logs)
     |  
     |  on_epoch_end(self, epoch, logs=None)
     |      Called at the end of an epoch.
     |      
     |      Subclasses should override for any actions to run. This function should only
     |      be called during TRAIN mode.
     |      
     |      Arguments:
     |          epoch: integer, index of epoch.
     |          logs: dict, metric results for this training epoch, and for the
     |            validation epoch if validation is performed. Validation result keys
     |            are prefixed with `val_`.
     |  
     |  on_train_begin(self, logs=None)
     |      Called at the beginning of training.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          logs: dict. Currently no data is passed to this argument for this method
     |            but that may change in the future.
     |  
     |  on_train_end(self, logs=None)
     |      Called at the end of training.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          logs: dict. Currently no data is passed to this argument for this method
     |            but that may change in the future.
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from Callback:
     |  
     |  on_batch_begin(self, batch, logs=None)
     |      A backwards compatibility alias for `on_train_batch_begin`.
     |  
     |  on_batch_end(self, batch, logs=None)
     |      A backwards compatibility alias for `on_train_batch_end`.
     |  
     |  on_epoch_begin(self, epoch, logs=None)
     |      Called at the start of an epoch.
     |      
     |      Subclasses should override for any actions to run. This function should only
     |      be called during TRAIN mode.
     |      
     |      Arguments:
     |          epoch: integer, index of epoch.
     |          logs: dict. Currently no data is passed to this argument for this method
     |            but that may change in the future.
     |  
     |  on_predict_batch_begin(self, batch, logs=None)
     |      Called at the beginning of a batch in `predict` methods.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          batch: integer, index of batch within the current epoch.
     |          logs: dict. Has keys `batch` and `size` representing the current batch
     |            number and the size of the batch.
     |  
     |  on_predict_batch_end(self, batch, logs=None)
     |      Called at the end of a batch in `predict` methods.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          batch: integer, index of batch within the current epoch.
     |          logs: dict. Metric results for this batch.
     |  
     |  on_predict_begin(self, logs=None)
     |      Called at the beginning of prediction.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          logs: dict. Currently no data is passed to this argument for this method
     |            but that may change in the future.
     |  
     |  on_predict_end(self, logs=None)
     |      Called at the end of prediction.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          logs: dict. Currently no data is passed to this argument for this method
     |            but that may change in the future.
     |  
     |  on_test_batch_begin(self, batch, logs=None)
     |      Called at the beginning of a batch in `evaluate` methods.
     |      
     |      Also called at the beginning of a validation batch in the `fit`
     |      methods, if validation data is provided.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          batch: integer, index of batch within the current epoch.
     |          logs: dict. Has keys `batch` and `size` representing the current batch
     |            number and the size of the batch.
     |  
     |  on_test_batch_end(self, batch, logs=None)
     |      Called at the end of a batch in `evaluate` methods.
     |      
     |      Also called at the end of a validation batch in the `fit`
     |      methods, if validation data is provided.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          batch: integer, index of batch within the current epoch.
     |          logs: dict. Metric results for this batch.
     |  
     |  on_test_begin(self, logs=None)
     |      Called at the beginning of evaluation or validation.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          logs: dict. Currently no data is passed to this argument for this method
     |            but that may change in the future.
     |  
     |  on_test_end(self, logs=None)
     |      Called at the end of evaluation or validation.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          logs: dict. Currently no data is passed to this argument for this method
     |            but that may change in the future.
     |  
     |  on_train_batch_begin(self, batch, logs=None)
     |      Called at the beginning of a training batch in `fit` methods.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          batch: integer, index of batch within the current epoch.
     |          logs: dict. Has keys `batch` and `size` representing the current batch
     |            number and the size of the batch.
     |  
     |  on_train_batch_end(self, batch, logs=None)
     |      Called at the end of a training batch in `fit` methods.
     |      
     |      Subclasses should override for any actions to run.
     |      
     |      Arguments:
     |          batch: integer, index of batch within the current epoch.
     |          logs: dict. Metric results for this batch.
     |  
     |  set_model(self, model)
     |  
     |  set_params(self, params)
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from Callback:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)
    
    


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# definizione struttura neural network per classificazione
model = Sequential()
model.add(Dense(units=30,activation='relu'))
model.add(Dense(units=15,activation='relu'))
model.add(Dense(units=1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
```


```python
# definizione early stopping
early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
%%time
# stima modello con early stop per limitare overfitting
model.fit(x=X_train, 
          y=y_train, 
          epochs=600,
          validation_data=(X_test, y_test), verbose=0,
          callbacks=[early_stop]
          )
```

    Epoch 00047: early stopping
    Wall time: 3.06 s
    




    <tensorflow.python.keras.callbacks.History at 0x26b08b5b2c8>




```python
# loss crossentropy
model_loss = pd.DataFrame(model.history.history)
model_loss.plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x26b08f1bb48>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_127_1.png)


#### DropOut Layers


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# definizione struttura neural network per classificazione
model = Sequential()
model.add(Dense(units=30,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=15,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
%%time
# stima modello con early stop e dropout per limitare overfitting
model.fit(x=X_train, 
          y=y_train, 
          epochs=600,
          validation_data=(X_test, y_test), verbose=0,
          callbacks=[early_stop]
          )
```

    Epoch 00107: early stopping
    Wall time: 8.93 s
    




    <tensorflow.python.keras.callbacks.History at 0x26b091d5b48>




```python
# loss crossentropy
model_loss = pd.DataFrame(model.history.history)
model_loss.plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x26b0583fc08>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_133_1.png)


#### Metrics


```python
# predictions
predictions = model.predict_classes(X_test)
```


```python
# metrics
print('\nConfusion Matrix:')
print(confusion_matrix(y_test,predictions))
print('\nClassification metrics:')
print(classification_report(y_test,predictions))
```

    
    Confusion Matrix:
    [[54  1]
     [ 6 82]]
    
    Classification metrics:
                  precision    recall  f1-score   support
    
               0       0.90      0.98      0.94        55
               1       0.99      0.93      0.96        88
    
        accuracy                           0.95       143
       macro avg       0.94      0.96      0.95       143
    weighted avg       0.95      0.95      0.95       143
    
    

## LendingClub dataset
[Kaggle: Predict default loans with classification](https://www.kaggle.com/wordsforthewise/lending-club)  
Customized by Udemy course


```python
# lib
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report,confusion_matrix

import random as rn
import os

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
```


```python
# df
df = pd.read_csv('Refactored_Py_DS_ML_Bootcamp-master/22-Deep Learning/TensorFlow_FILES/DATA//lending_club_loan_two.csv')
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 396030 entries, 0 to 396029
    Data columns (total 27 columns):
     #   Column                Non-Null Count   Dtype  
    ---  ------                --------------   -----  
     0   loan_amnt             396030 non-null  float64
     1   term                  396030 non-null  object 
     2   int_rate              396030 non-null  float64
     3   installment           396030 non-null  float64
     4   grade                 396030 non-null  object 
     5   sub_grade             396030 non-null  object 
     6   emp_title             373103 non-null  object 
     7   emp_length            377729 non-null  object 
     8   home_ownership        396030 non-null  object 
     9   annual_inc            396030 non-null  float64
     10  verification_status   396030 non-null  object 
     11  issue_d               396030 non-null  object 
     12  loan_status           396030 non-null  object 
     13  purpose               396030 non-null  object 
     14  title                 394275 non-null  object 
     15  dti                   396030 non-null  float64
     16  earliest_cr_line      396030 non-null  object 
     17  open_acc              396030 non-null  float64
     18  pub_rec               396030 non-null  float64
     19  revol_bal             396030 non-null  float64
     20  revol_util            395754 non-null  float64
     21  total_acc             396030 non-null  float64
     22  initial_list_status   396030 non-null  object 
     23  application_type      396030 non-null  object 
     24  mort_acc              358235 non-null  float64
     25  pub_rec_bankruptcies  395495 non-null  float64
     26  address               396030 non-null  object 
    dtypes: float64(12), object(15)
    memory usage: 81.6+ MB
    

### Step 1: EDA


```python
# verifichiamo la distribuzione target
ax = sns.countplot(x='loan_status',data=df)
# un po' sbilanciato, ci aspetteremo un'elevata accuracy ma precision e recall saranno quelle difficili

# aggiungo le frequenze
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height+3000,
            '{:,.0f}'.format(height).replace(",", "X").replace(".", ",").replace("X", "."), # '{:1.2f}'.format(height/float(len(df)))
            ha="center")
```


![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_141_0.png)



```python
# histogram di loan_amnt
plt.figure(figsize=(12,4))
sns.distplot(df['loan_amnt'],kde=False,color='b',bins=40,hist_kws=dict(edgecolor='grey'))
plt.xlim(0,45000)
```




    (0.0, 45000.0)




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_142_1.png)



```python
# heatmap
plt.figure(figsize=(10,8))
sns.heatmap(df.corr(),annot=True,cmap='coolwarm',linecolor='white',linewidths=1)
# plt.ylim(10, 0)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db149a6f88>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_143_1.png)



```python
# mezzo-pairplot (parte 1)
df_pair1 = df[df.columns[0:18]].sample(n=10000, random_state=1311)
sns.pairplot(df_pair1,hue='loan_status',diag_kind='hist',diag_kws=dict(edgecolor='black',alpha=0.6,bins=30),plot_kws=dict(alpha=0.4))
```




    <seaborn.axisgrid.PairGrid at 0x18e3e6a8cc8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_144_1.png)



```python
# mezzo-pairplot (parte 2)
df_pair2 = df.iloc[:, np.r_[12,18:27]].sample(n=10000, random_state=1311) # indexer
sns.pairplot(df_pair2,hue='loan_status',diag_kind='hist',diag_kws=dict(edgecolor='black',alpha=0.6,bins=30),plot_kws=dict(alpha=0.4))
```




    <seaborn.axisgrid.PairGrid at 0x18e0a62d088>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_145_1.png)



```python
# scatterplot 
sns.scatterplot(x='installment',y='loan_amnt',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db16fec1c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_146_1.png)



```python
# boxplot loan_status e loan amount
sns.boxplot(x='loan_status',y='loan_amnt',data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db177d3ac8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_147_1.png)



```python
# per mostrare i separatori delle migliaia come punti e decimali come virgola
dot_sep = lambda x: format(round(x,2) if abs(x) < 1 else round(x,1) if abs(x) < 10 else int(x), ',').replace(",", "X").replace(".", ",").replace("X", ".")

# loan amount by loan status
df.groupby('loan_status')['loan_amnt'].describe().applymap(dot_sep)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>loan_status</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Charged Off</th>
      <td>77.673</td>
      <td>15.126</td>
      <td>8.505</td>
      <td>1.000</td>
      <td>8.525</td>
      <td>14.000</td>
      <td>20.000</td>
      <td>40.000</td>
    </tr>
    <tr>
      <th>Fully Paid</th>
      <td>318.357</td>
      <td>13.866</td>
      <td>8.302</td>
      <td>500</td>
      <td>7.500</td>
      <td>12.000</td>
      <td>19.225</td>
      <td>40.000</td>
    </tr>
  </tbody>
</table>
</div>




```python
# countplot per grade stratificato per target
sns.countplot(x='grade',hue='loan_status',data=df,order=sorted(df['grade'].unique()))
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2a843721688>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_149_1.png)



```python
# countplot per subgrade
plt.figure(figsize=(12,4))
sns.countplot(x='sub_grade',data=df,order=sorted(df['sub_grade'].unique()),palette='coolwarm')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db14a6f548>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_150_1.png)



```python
# countplot per subgrade stratificato per target
plt.figure(figsize=(12,4))
sns.countplot(x='sub_grade',data=df,order=sorted(df['sub_grade'].unique()),palette='coolwarm',hue='loan_status')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db16a31888>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_151_1.png)



```python
# countplot per subgrade (F e G) stratificato per target
plt.figure(figsize=(12,4))
df_FG = df[(df['grade']=='G') | (df['grade']=='F')]
# df_FG = df[df['sub_grade'].apply(lambda x: x[0] in ['G','F'])]
sns.countplot(x='sub_grade',data=df_FG,order=sorted(df_FG['sub_grade'].unique()),palette='coolwarm',hue='loan_status')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db1782ed48>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_152_1.png)



```python
# il method map è il più veloce per la rimappatura di una variabile
mapping_dict = {'Fully Paid': 1, 'Charged Off': 0}
df['loan_status'].map(mapping_dict).value_counts() # se il mapping non è esaustivo invece dei NaN si può dare il valore originale con il metodo .fillna(df['loan_status'])

# Risultati identici ma meno performanti:
# df['loan_status'].replace(mapping_dict).value_counts()
# df.replace({'loan_status': mapping_dict})['loan_status'].value_counts() # così devo specificare la colonna e deve agire su tutto il df
# df['loan_status'].replace(to_replace=['Fully Paid', 'Charged Off'], value=[1, 0]).value_counts()
```




    1    318357
    0     77673
    Name: loan_status, dtype: int64




```python
# creo dummy/dicotomizzo la target loan status
df['loan_repaid'] = df['loan_status'].map({'Fully Paid':1,'Charged Off':0})
print(df.shape)
# tabella di contingenza
df.groupby(["loan_repaid", "loan_status"]).size().reset_index(name="Frequenza")
# pd.crosstab(df['loan_repaid'],df['loan_status'])
```

    (396030, 28)
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loan_repaid</th>
      <th>loan_status</th>
      <th>Frequenza</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Charged Off</td>
      <td>77673</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>Fully Paid</td>
      <td>318357</td>
    </tr>
  </tbody>
</table>
</div>




```python
# correlazione target con le altre variabili numeriche
df.corr()['loan_repaid'][:-1].sort_values().plot(kind='bar')
# df.corr()['loan_repaid'].sort_values().drop('loan_repaid').plot(kind='bar')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x18e17784e48>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_155_1.png)


### Step 2: Data Preprocessing
Section Goals: Remove or fill any missing data. Remove unnecessary or repetitive features. Convert categorical string features to dummy variables.


```python
# df numero record
len(df)
```




    396030




```python
# missing data
sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db17a29148>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_158_1.png)



```python
# missing data count
missing_counts = pd.concat(
    [df.isnull().sum()[df.isnull().sum()>0].apply(dot_sep),
    (df.isnull().sum()[df.isnull().sum()>0]/len(df)*100).apply(dot_sep)], 
    axis=1)
missing_counts.columns = ['Freq', 'Freq %']
missing_counts
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Freq</th>
      <th>Freq %</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>emp_title</th>
      <td>22.927</td>
      <td>5,8</td>
    </tr>
    <tr>
      <th>emp_length</th>
      <td>18.301</td>
      <td>4,6</td>
    </tr>
    <tr>
      <th>title</th>
      <td>1.755</td>
      <td>0,44</td>
    </tr>
    <tr>
      <th>revol_util</th>
      <td>276</td>
      <td>0,07</td>
    </tr>
    <tr>
      <th>mort_acc</th>
      <td>37.795</td>
      <td>9,5</td>
    </tr>
    <tr>
      <th>pub_rec_bankruptcies</th>
      <td>535</td>
      <td>0,14</td>
    </tr>
  </tbody>
</table>
</div>




```python
# employement job titles univoci
print(df['emp_title'].value_counts())
print('\nUnivoci:',dot_sep(df['emp_title'].nunique()))
# troppe per creare dummy, la rimuovo ma si potrebbero raggruppare
df.drop('emp_title',inplace=True,axis=1)
print(df.shape)
```

    Teacher                     4389
    Manager                     4250
    Registered Nurse            1856
    RN                          1846
    Supervisor                  1830
                                ... 
    Annunciation                   1
    Atos Inc                       1
    chevy parts maneger            1
    Architectural Intern           1
    GroupSystems Corporation       1
    Name: emp_title, Length: 173105, dtype: int64
    
    Univoci: 173.105
    (396030, 27)
    


```python
# sorted(df['emp_length'].dropna().unique())
emp_length_order = ['Missing','< 1 year', '1 year', '2 years', '3 years', 
                    '4 years', '5 years', '6 years', '7 years', 
                    '8 years', '9 years', '10+ years']
```


```python
# countplot emp_length
plt.figure(figsize=(12,4))
ax = sns.countplot(x='emp_length',data=df[['emp_length']].fillna('Missing'),order=emp_length_order)

# aggiungo le frequenze
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height + 1000,
            '{:,.0f}'.format(height).replace(",", "X").replace(".", ",").replace("X", "."),
            ha="center")
```


![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_162_0.png)



```python
# countplot emp_length
plt.figure(figsize=(12,4))
sns.countplot(x='emp_length',data=df[['emp_length','loan_status']].fillna('Missing'),order=emp_length_order, hue='loan_status')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2db203a15c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_163_1.png)



```python
# tasso di insucesso per ogni emp_length
# emp_len = df[df['loan_status']=="Charged Off"].groupby("emp_length").count()['loan_status']/df.groupby("emp_length").count()['loan_status']
emp_len = pd.DataFrame(df[['emp_length','loan_status']].fillna('Missing').groupby(['emp_length','loan_status']).size().groupby(level=0).apply(lambda x: x / x.sum()).xs('Charged Off',level='loan_status'),columns=['% Failure']).reset_index()
emp_len
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>emp_length</th>
      <th>% Failure</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1 year</td>
      <td>0.199135</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10+ years</td>
      <td>0.184186</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2 years</td>
      <td>0.193262</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3 years</td>
      <td>0.195231</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4 years</td>
      <td>0.192385</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5 years</td>
      <td>0.192187</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6 years</td>
      <td>0.189194</td>
    </tr>
    <tr>
      <th>7</th>
      <td>7 years</td>
      <td>0.194774</td>
    </tr>
    <tr>
      <th>8</th>
      <td>8 years</td>
      <td>0.199760</td>
    </tr>
    <tr>
      <th>9</th>
      <td>9 years</td>
      <td>0.200470</td>
    </tr>
    <tr>
      <th>10</th>
      <td>&lt; 1 year</td>
      <td>0.206872</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Missing</td>
      <td>0.275286</td>
    </tr>
  </tbody>
</table>
</div>




```python
# barplot tasso
plt.figure(figsize=(12,4))
# emp_len.plot(kind='bar')
sns.barplot(x='emp_length',y='% Failure',data=emp_len,order=emp_length_order,palette='coolwarm')
# non c'è una forte evidenza, quindi rimuovo 'emp_length'
df.drop('emp_length',axis=1,inplace=True)
df.shape
```




    (396030, 26)




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_165_1.png)



```python
# missing data count
missing_counts = pd.concat(
    [df.isnull().sum()[df.isnull().sum()>0].apply(dot_sep),
    (df.isnull().sum()[df.isnull().sum()>0]/len(df)*100).apply(dot_sep)], 
    axis=1)
missing_counts.columns = ['Freq', 'Freq %']
missing_counts
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Freq</th>
      <th>Freq %</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>title</th>
      <td>1.755</td>
      <td>0,44</td>
    </tr>
    <tr>
      <th>revol_util</th>
      <td>276</td>
      <td>0,07</td>
    </tr>
    <tr>
      <th>mort_acc</th>
      <td>37.795</td>
      <td>9,5</td>
    </tr>
    <tr>
      <th>pub_rec_bankruptcies</th>
      <td>535</td>
      <td>0,14</td>
    </tr>
  </tbody>
</table>
</div>




```python
# title e purpose
print(df['purpose'].value_counts().head(7), end='\n\n')
print(df['title'].value_counts().head(7))
```

    debt_consolidation    234507
    credit_card            83019
    home_improvement       24030
    other                  21185
    major_purchase          8790
    small_business          5701
    car                     4697
    Name: purpose, dtype: int64
    
    Debt consolidation         152472
    Credit card refinancing     51487
    Home improvement            15264
    Other                       12930
    Debt Consolidation          11608
    Major purchase               4769
    Consolidation                3852
    Name: title, dtype: int64
    


```python
# title e purpose
print('Purpose nunique:',df['purpose'].nunique())
print('Title nunique:',df['title'].nunique())
# rimuovo title che è una sottocategoria e non avrebbe senso rendere dummy
df.drop('title',axis=1,inplace=True)
print(df.shape)
```

    Purpose nunique: 14
    Title nunique: 48817
    (396030, 25)
    


```python
# missing data count
missing_counts = pd.concat(
    [df.isnull().sum()[df.isnull().sum()>0].apply(dot_sep),
    (df.isnull().sum()[df.isnull().sum()>0]/len(df)*100).apply(dot_sep)], 
    axis=1)
missing_counts.columns = ['Freq', 'Freq %']
missing_counts
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Freq</th>
      <th>Freq %</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>revol_util</th>
      <td>276</td>
      <td>0,07</td>
    </tr>
    <tr>
      <th>mort_acc</th>
      <td>37.795</td>
      <td>9,5</td>
    </tr>
    <tr>
      <th>pub_rec_bankruptcies</th>
      <td>535</td>
      <td>0,14</td>
    </tr>
  </tbody>
</table>
</div>




```python
# mort_acc (acconto del mutuo ipotecario)
print('mort_acc nunique:', df['mort_acc'].nunique())
print(df['mort_acc'].value_counts().head().apply(dot_sep))
```

    mort_acc nunique: 33
    0.0    139.777
    1.0     60.416
    2.0     49.948
    3.0     38.049
    4.0     27.887
    Name: mort_acc, dtype: object
    


```python
# mort_acc correlations
df.corr()['mort_acc'].sort_values()
```




    int_rate               -0.082583
    dti                    -0.025439
    revol_util              0.007514
    pub_rec                 0.011552
    pub_rec_bankruptcies    0.027239
    loan_repaid             0.073111
    open_acc                0.109205
    installment             0.193694
    revol_bal               0.194925
    loan_amnt               0.222315
    annual_inc              0.236320
    total_acc               0.381072
    mort_acc                1.000000
    Name: mort_acc, dtype: float64




```python
df[['mort_acc','total_acc']].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mort_acc</th>
      <th>total_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>25.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.0</td>
      <td>27.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>26.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>13.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>43.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Mean of mort_acc column per total_acc
print('Mean of mort_acc column per total_acc:')
print(df.groupby('total_acc').mean()['mort_acc'])
```

    Mean of mort_acc column per total_acc:
    total_acc
    2.0      0.000000
    3.0      0.052023
    4.0      0.066743
    5.0      0.103289
    6.0      0.151293
               ...   
    124.0    1.000000
    129.0    1.000000
    135.0    3.000000
    150.0    2.000000
    151.0    0.000000
    Name: mort_acc, Length: 118, dtype: float64
    


```python
# fillna by group per mort_acc

# lambda function con due variabili
# def fill_mort_acc(total_acc,mort_acc):
#     if np.isnan(mort_acc):
#         return total_acc_avg[total_acc]
#     else:
#         return mort_acc
# df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)

df['mort_acc'] = df[['mort_acc','total_acc']].groupby('total_acc').transform(lambda x: x.fillna(x.mean()))
df.shape
```




    (396030, 25)




```python
# rimuovo record con missing inferiori allo 0.5%
print('Pre rimozione missing:',len(df))
df.dropna(inplace=True)
print('Post rimozione missing:',len(df))
df.shape
```

    Pre rimozione missing: 396030
    Post rimozione missing: 395219
    




    (395219, 25)




```python
# verifica missing rimasti
df.isnull().sum().sum()
```




    0



### Step 3: Categorical Variables and Dummy Variables
String values due to the categorical columns


```python
# distribuzione tipi colonne
df.dtypes.value_counts()
```




    float64    12
    object     12
    int64       1
    dtype: int64




```python
# seleziono colonne non numeriche
df.select_dtypes(['object']).columns
# df.select_dtypes(exclude=['float64','int64']).columns
```




    Index(['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status',
           'issue_d', 'loan_status', 'purpose', 'earliest_cr_line',
           'initial_list_status', 'application_type', 'address'],
          dtype='object')




```python
# codifico term in numeric
print('Pre codifica:', df['term'].unique())
df['term'] = df['term'].map({' 36 months':36,' 60 months':60})
# df['term'] = df['term'].apply(lambda x: int(x[:3]))
print('Post codifica:', df['term'].unique())
```

    Pre codifica: [' 36 months' ' 60 months']
    Post codifica: [36 60]
    


```python
# sub_grade è una sottocategoria di grade ma si può rendere dummy, quindi rimuoviamo grade
print('Univoci grade:', df['grade'].nunique())
print('Univoci sub_grade:', df['sub_grade'].nunique())
df.drop('grade',axis=1,inplace=True)
print(df.shape)
```

    Univoci grade: 7
    Univoci sub_grade: 35
    (395219, 24)
    


```python
# dummyfication
df = pd.get_dummies(df,columns=['sub_grade'],drop_first=True)
# subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True,prefix='sub_grade')
# df = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1)
df.columns
```




    Index(['loan_amnt', 'term', 'int_rate', 'installment', 'home_ownership',
           'annual_inc', 'verification_status', 'issue_d', 'loan_status',
           'purpose', 'dti', 'earliest_cr_line', 'open_acc', 'pub_rec',
           'revol_bal', 'revol_util', 'total_acc', 'initial_list_status',
           'application_type', 'mort_acc', 'pub_rec_bankruptcies', 'address',
           'loan_repaid', 'sub_grade_A2', 'sub_grade_A3', 'sub_grade_A4',
           'sub_grade_A5', 'sub_grade_B1', 'sub_grade_B2', 'sub_grade_B3',
           'sub_grade_B4', 'sub_grade_B5', 'sub_grade_C1', 'sub_grade_C2',
           'sub_grade_C3', 'sub_grade_C4', 'sub_grade_C5', 'sub_grade_D1',
           'sub_grade_D2', 'sub_grade_D3', 'sub_grade_D4', 'sub_grade_D5',
           'sub_grade_E1', 'sub_grade_E2', 'sub_grade_E3', 'sub_grade_E4',
           'sub_grade_E5', 'sub_grade_F1', 'sub_grade_F2', 'sub_grade_F3',
           'sub_grade_F4', 'sub_grade_F5', 'sub_grade_G1', 'sub_grade_G2',
           'sub_grade_G3', 'sub_grade_G4', 'sub_grade_G5'],
          dtype='object')




```python
# verifico le colonne non numeriche rimaste
df.select_dtypes(['object']).columns
```




    Index(['home_ownership', 'verification_status', 'issue_d', 'loan_status',
           'purpose', 'earliest_cr_line', 'initial_list_status',
           'application_type', 'address'],
          dtype='object')




```python
# rimanenti variabili categoriche
print('verification_status nunique:',df['verification_status'].nunique())
print('application_type nunique:',df['application_type'].nunique())
print('initial_list_status nunique:',df['initial_list_status'].nunique())
print('purpose nunique:',df['purpose'].nunique())
```

    verification_status nunique: 3
    application_type nunique: 3
    initial_list_status nunique: 2
    purpose nunique: 14
    


```python
# dummyficate
df = pd.get_dummies(df,columns=['verification_status', 'application_type','initial_list_status','purpose'],drop_first=True)
```


```python
# verifico le colonne non numeriche rimaste
df.select_dtypes(['object']).columns
```




    Index(['home_ownership', 'issue_d', 'loan_status', 'earliest_cr_line',
           'address'],
          dtype='object')




```python
# riduzione mapping sostituzione categorie di home_ownership
print(df['home_ownership'].value_counts())
print('\n')
# df['home_ownership'].map({'NONE':'OTHER','ANY':'OTHER'}).fillna(df['home_ownership']).value_counts()
print(df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER').value_counts())
```

    MORTGAGE    198022
    RENT        159395
    OWN          37660
    OTHER          110
    NONE            29
    ANY              3
    Name: home_ownership, dtype: int64
    
    
    MORTGAGE    198022
    RENT        159395
    OWN          37660
    OTHER          142
    Name: home_ownership, dtype: int64
    


```python
# dummyfication home_ownership
df['home_ownership'] = df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')
df = pd.get_dummies(df,columns=['home_ownership'],drop_first=True)
df.columns
```




    Index(['loan_amnt', 'term', 'int_rate', 'installment', 'annual_inc', 'issue_d',
           'loan_status', 'dti', 'earliest_cr_line', 'open_acc', 'pub_rec',
           'revol_bal', 'revol_util', 'total_acc', 'mort_acc',
           'pub_rec_bankruptcies', 'address', 'loan_repaid', 'sub_grade_A2',
           'sub_grade_A3', 'sub_grade_A4', 'sub_grade_A5', 'sub_grade_B1',
           'sub_grade_B2', 'sub_grade_B3', 'sub_grade_B4', 'sub_grade_B5',
           'sub_grade_C1', 'sub_grade_C2', 'sub_grade_C3', 'sub_grade_C4',
           'sub_grade_C5', 'sub_grade_D1', 'sub_grade_D2', 'sub_grade_D3',
           'sub_grade_D4', 'sub_grade_D5', 'sub_grade_E1', 'sub_grade_E2',
           'sub_grade_E3', 'sub_grade_E4', 'sub_grade_E5', 'sub_grade_F1',
           'sub_grade_F2', 'sub_grade_F3', 'sub_grade_F4', 'sub_grade_F5',
           'sub_grade_G1', 'sub_grade_G2', 'sub_grade_G3', 'sub_grade_G4',
           'sub_grade_G5', 'verification_status_Source Verified',
           'verification_status_Verified', 'application_type_INDIVIDUAL',
           'application_type_JOINT', 'initial_list_status_w',
           'purpose_credit_card', 'purpose_debt_consolidation',
           'purpose_educational', 'purpose_home_improvement', 'purpose_house',
           'purpose_major_purchase', 'purpose_medical', 'purpose_moving',
           'purpose_other', 'purpose_renewable_energy', 'purpose_small_business',
           'purpose_vacation', 'purpose_wedding', 'home_ownership_OTHER',
           'home_ownership_OWN', 'home_ownership_RENT'],
          dtype='object')




```python
# verifico le colonne non numeriche rimaste
df.select_dtypes(['object']).columns
```




    Index(['issue_d', 'loan_status', 'earliest_cr_line', 'address'], dtype='object')




```python
# feature engineering address column
print('address nunique:',df['address'].nunique())
print('\n')
print(df['address'].value_counts().head(10))
```

    address nunique: 392898
    
    
    USS Johnson\nFPO AE 48052     8
    USCGC Smith\nFPO AE 70466     8
    USS Smith\nFPO AP 70466       8
    USNS Johnson\nFPO AE 05113    8
    USNS Johnson\nFPO AP 48052    7
    USNS Johnson\nFPO AA 70466    6
    USNV Brown\nFPO AA 48052      6
    USS Smith\nFPO AP 22690       6
    USCGC Miller\nFPO AA 22690    6
    USCGC Smith\nFPO AA 70466     6
    Name: address, dtype: int64
    


```python
# creo variabile cap (zip code)
df['zip_code'] = df['address'].apply(lambda x: x[-5:])
df['zip_code'].unique()
```




    array(['22690', '05113', '00813', '11650', '30723', '70466', '29597',
           '48052', '86630', '93700'], dtype=object)




```python
# dummy zip code
df.drop('address',axis=1,inplace=True)
df = pd.get_dummies(df,columns=['zip_code'],drop_first=True)
df.columns
```




    Index(['loan_amnt', 'term', 'int_rate', 'installment', 'annual_inc', 'issue_d',
           'loan_status', 'dti', 'earliest_cr_line', 'open_acc', 'pub_rec',
           'revol_bal', 'revol_util', 'total_acc', 'mort_acc',
           'pub_rec_bankruptcies', 'loan_repaid', 'sub_grade_A2', 'sub_grade_A3',
           'sub_grade_A4', 'sub_grade_A5', 'sub_grade_B1', 'sub_grade_B2',
           'sub_grade_B3', 'sub_grade_B4', 'sub_grade_B5', 'sub_grade_C1',
           'sub_grade_C2', 'sub_grade_C3', 'sub_grade_C4', 'sub_grade_C5',
           'sub_grade_D1', 'sub_grade_D2', 'sub_grade_D3', 'sub_grade_D4',
           'sub_grade_D5', 'sub_grade_E1', 'sub_grade_E2', 'sub_grade_E3',
           'sub_grade_E4', 'sub_grade_E5', 'sub_grade_F1', 'sub_grade_F2',
           'sub_grade_F3', 'sub_grade_F4', 'sub_grade_F5', 'sub_grade_G1',
           'sub_grade_G2', 'sub_grade_G3', 'sub_grade_G4', 'sub_grade_G5',
           'verification_status_Source Verified', 'verification_status_Verified',
           'application_type_INDIVIDUAL', 'application_type_JOINT',
           'initial_list_status_w', 'purpose_credit_card',
           'purpose_debt_consolidation', 'purpose_educational',
           'purpose_home_improvement', 'purpose_house', 'purpose_major_purchase',
           'purpose_medical', 'purpose_moving', 'purpose_other',
           'purpose_renewable_energy', 'purpose_small_business',
           'purpose_vacation', 'purpose_wedding', 'home_ownership_OTHER',
           'home_ownership_OWN', 'home_ownership_RENT', 'zip_code_05113',
           'zip_code_11650', 'zip_code_22690', 'zip_code_29597', 'zip_code_30723',
           'zip_code_48052', 'zip_code_70466', 'zip_code_86630', 'zip_code_93700'],
          dtype='object')




```python
# verifico le colonne non numeriche rimaste
df.select_dtypes(['object']).columns
```




    Index(['issue_d', 'loan_status', 'earliest_cr_line'], dtype='object')




```python
# issue_date è 'data leakage' la avremmo solo con la realizzazione della terget, quindi va esclusa
df.drop('issue_d',axis=1,inplace=True)
df.shape
```




    (395219, 80)




```python
# earliest_cr_line possiamo estrarre l'anno
print(df['earliest_cr_line'].head())
print('\n')
print('Lunghezza stringa:',df['earliest_cr_line'].apply(lambda x: len(x)).unique()) # sono tutti costanti da 8, si prende l'anno dalla fine
df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda x: int(x[-4:]))
df.drop('earliest_cr_line',axis=1,inplace=True)
```

    0    Jun-1990
    1    Jul-2004
    2    Aug-2007
    3    Sep-2006
    4    Mar-1999
    Name: earliest_cr_line, dtype: object
    
    
    Lunghezza stringa: [8]
    


```python
# faccio un backup del df, nel caso sbaglio
df_backup = df.copy()
df_backup.shape
```




    (395219, 80)




```python
# verifico le colonne non numeriche rimaste
df.select_dtypes(['object']).columns
```




    Index(['loan_status'], dtype='object')




```python
# elimino loan_status così lascio la target codificata ('loan_repaid')
df.drop('loan_status',axis=1,inplace=True)
df.shape
# siamo pronti!
```




    (395219, 79)



### Step 4: Model
[Tuning NN](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)  
Droput links [1](https://keras.io/api/layers/core_layers/), [2](https://en.wikipedia.org/wiki/Dilution_(neural_networks)), [3](https://towardsdatascience.com/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab)


```python
# potrei lavorare sul campione
df_sample = df.sample(frac=0.1,random_state=101)
print('df intero:',len(df))
print('df ridotto:',len(df_sample))
```

    df intero: 395219
    df ridotto: 39522
    


```python
# X e y (come numpy arrays)
X = df.drop('loan_repaid',axis=1).values
y = df['loan_repaid'].values
```


```python
# train e test
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=101)
```


```python
# normalizing data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test) # avevo erroneamente fatto il fit, non andava messo, si otterranno risultati leggermente diversi
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
# definizione struttura neural network per classificazione
model = Sequential()
model.add(Dense(units=78,activation='relu'))
model.add(Dropout(0.5)) # 0.2 da provare
model.add(Dense(units=39,activation='relu'))
model.add(Dropout(0.5)) # 0.2 da provare
model.add(Dense(units=19,activation='relu'))
model.add(Dropout(0.5)) # 0.2 da provare
model.add(Dense(units=1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
```


```python
# set seed per ridurre la non determinatezza del fit via GPU
os.environ['PYTHONHASHSEED'] = '13111990'
np.random.seed(13)
rn.seed(11)
tf.random.set_seed(1990)
```


```python
%%time
# stimo il modello
model.fit(x=X_train,y=y_train,
          validation_data=(X_test,y_test),
          verbose=2,batch_size=128,epochs=25) # batch_size=256 da provare
```

    Train on 316175 samples, validate on 79044 samples
    Epoch 1/25
    316175/316175 - 8s - loss: 0.3197 - val_loss: 0.2726
    Epoch 2/25
    316175/316175 - 7s - loss: 0.2708 - val_loss: 0.2666
    Epoch 3/25
    316175/316175 - 7s - loss: 0.2670 - val_loss: 0.2685
    Epoch 4/25
    316175/316175 - 7s - loss: 0.2660 - val_loss: 0.2665
    Epoch 5/25
    316175/316175 - 7s - loss: 0.2652 - val_loss: 0.2663
    Epoch 6/25
    316175/316175 - 7s - loss: 0.2650 - val_loss: 0.2655
    Epoch 7/25
    316175/316175 - 8s - loss: 0.2649 - val_loss: 0.2663
    Epoch 8/25
    316175/316175 - 8s - loss: 0.2647 - val_loss: 0.2674
    Epoch 9/25
    316175/316175 - 7s - loss: 0.2643 - val_loss: 0.2664
    Epoch 10/25
    316175/316175 - 7s - loss: 0.2639 - val_loss: 0.2690
    Epoch 11/25
    316175/316175 - 7s - loss: 0.2638 - val_loss: 0.2696
    Epoch 12/25
    316175/316175 - 7s - loss: 0.2642 - val_loss: 0.2694
    Epoch 13/25
    316175/316175 - 7s - loss: 0.2635 - val_loss: 0.2661
    Epoch 14/25
    316175/316175 - 8s - loss: 0.2639 - val_loss: 0.2697
    Epoch 15/25
    316175/316175 - 7s - loss: 0.2635 - val_loss: 0.2690
    Epoch 16/25
    316175/316175 - 7s - loss: 0.2634 - val_loss: 0.2693
    Epoch 17/25
    316175/316175 - 7s - loss: 0.2635 - val_loss: 0.2706
    Epoch 18/25
    316175/316175 - 6s - loss: 0.2630 - val_loss: 0.2719
    Epoch 19/25
    316175/316175 - 6s - loss: 0.2632 - val_loss: 0.2695
    Epoch 20/25
    316175/316175 - 6s - loss: 0.2630 - val_loss: 0.2708
    Epoch 21/25
    316175/316175 - 6s - loss: 0.2632 - val_loss: 0.2715
    Epoch 22/25
    316175/316175 - 6s - loss: 0.2630 - val_loss: 0.2721
    Epoch 23/25
    316175/316175 - 6s - loss: 0.2624 - val_loss: 0.2715
    Epoch 24/25
    316175/316175 - 6s - loss: 0.2631 - val_loss: 0.2727
    Epoch 25/25
    316175/316175 - 6s - loss: 0.2629 - val_loss: 0.2713
    Wall time: 2min 51s
    




    <tensorflow.python.keras.callbacks.History at 0x2a80300b748>




```python
# save the model
model.save('LendingClub_Keras_Model.h5')  # creates a HDF5 file
```




    'F:\\Udemy\\Python for DS and ML Bootcamp'




```python
# loss crossentropy (campione)
model_loss = pd.DataFrame(model.history.history)
model_loss.plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2a8573fa248>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_209_1.png)



```python
# loss crossentropy
model_loss = pd.DataFrame(model.history.history)
model_loss.plot()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x2a80381b2c8>




![png](/assets/images/Udemy/Python for DS and ML Bootcamp/section-025/output_210_1.png)



```python
# predictions
predictions = model.predict_classes(X_test)
```


```python
# metrics
print('\nConfusion Matrix:')
print(confusion_matrix(y_test,predictions))
print('\nClassification metrics:')
print(classification_report(y_test,predictions))
# non è fantastico perché la recall della classe 0 è bassa
```

    
    Confusion Matrix:
    [[ 7176  8482]
     [  406 62980]]
    
    Classification metrics:
                  precision    recall  f1-score   support
    
               0       0.95      0.46      0.62     15658
               1       0.88      0.99      0.93     63386
    
        accuracy                           0.89     79044
       macro avg       0.91      0.73      0.78     79044
    weighted avg       0.89      0.89      0.87     79044
    
    

#### New data prediction


```python
# df
rn.seed(101)
random_ind = rn.randint(0,len(df))
new_customer = df.drop('loan_repaid',axis=1).iloc[random_ind]
new_customer
```




    loan_amnt           25000.00
    term                   60.00
    int_rate               18.24
    installment           638.11
    annual_inc          61665.00
                          ...   
    zip_code_48052          0.00
    zip_code_70466          0.00
    zip_code_86630          0.00
    zip_code_93700          0.00
    earliest_cr_year     1996.00
    Name: 305323, Length: 78, dtype: float64




```python
# scaling
new_customer = scaler.transform(new_customer.values.reshape(1, 78))
# predict
print('Probabilità Prevista:', model.predict(new_customer))
print('Classe Prevista:', model.predict_classes(new_customer))
print('Classe Osservata:', df.iloc[random_ind]['loan_repaid'])
```

    Probabilità Prevista: [[0.54277164]]
    Classe Prevista: [[1]]
    Classe Osservata: 1.0
    


```python
# reset
%reset -f
```

## TensorBoard
Basato Breast cancer Wisconsin costruiamo i log per tenere traccia


```python
import pandas as pd
import numpy as np

from datetime import datetime
import os

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation,Dropout
from tensorflow.keras.callbacks import EarlyStopping,TensorBoard
```


```python
# df
df = pd.read_csv('Refactored_Py_DS_ML_Bootcamp-master/22-Deep Learning/TensorFlow_FILES/DATA/cancer_classification.csv')
```


```python
# X e y
X = df.drop('benign_0__mal_1',axis=1).values
y = df['benign_0__mal_1'].values
```


```python
# train e test
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)
```


```python
# scaling
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
```


```python
# early stopping
early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)
```


```python
# working directory
os.getcwd()
```




    'F:\\Udemy\\Python for DS and ML Bootcamp'




```python
# time
datetime.now().strftime("%Y-%m-%d--%H%M")
```




    '2020-08-01--1701'




```python
# definizione tensorboard
# WINDOWS: Use "logs\\fit"
# MACOS/LINUX: Use "logs\fit"

log_directory = 'logs\\fit'

# OPTIONAL: ADD A TIMESTAMP FOR UNIQUE FOLDER
timestamp = datetime.now().strftime("%Y-%m-%d--%H%M")
log_directory = log_directory + '\\' + timestamp

board = TensorBoard(
    log_dir=log_directory,
    histogram_freq=1, # dopo ciascuna epoch calcola i pesi
    write_graph=True,
    write_images=True,
    update_freq='epoch',
    profile_batch=2,
    embeddings_freq=1)
```


```python
# definizione modello
model = Sequential()
model.add(Dense(units=30,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=15,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
```


```python
# training modello (lanciarlo solo una volta nella stessa callback log_dir)
model.fit(x=X_train, 
          y=y_train, 
          epochs=600,
          validation_data=(X_test, y_test), verbose=0,
          callbacks=[early_stop,board]
          )
```

    Epoch 00138: early stopping
    




    <tensorflow.python.keras.callbacks.History at 0x2a852471d08>



### Monitoring
Attivo tensorboard dal terminale
```sh
tensorboard --logdir logs\fit
```
Con il servizio attivo posso visitare [http://localhost:6006/](http://localhost:6006/)  
Questo servizio di controllo e diagnostica è più utile per le convolutional neural networks.


```python
print(log_directory)
```

    logs\fit\2020-08-01--1701
    

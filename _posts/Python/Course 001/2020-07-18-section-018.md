---
title: "Learning Python (012)"
excerpt: "kNN and Tree Methods"
date: 2020-07-18
tags: [udemy, python, coding]
mathjax: "true"
---

*Utilizzo l'environment conda py3*  
```console
~$ conda activate py3
~$ conda deactivate
```

*Versione modulo installato*  
```console
~$ pip show pydot
Name: scikit-learn
Name: pydot
Version: 1.4.1
Summary: Python interface to Graphviz's Dot
Home-page: https://github.com/pydot/pydot
Author: Ero Carrera
Author-email: ero@dkbza.org
License: MIT
Location: /home/user/miniconda3/envs/py3/lib/python3.7/site-packages
Requires: pyparsing
Required-by: 
```

# k Nearest Neighbors

<img src="/assets/images/Python/Course 001/section-018/001-kNN.png" width="400">

Parameters
* k
* Distance Metric


```python
# lib
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report,confusion_matrix
```


```python
# df
df = pd.read_csv("Refactored_Py_DS_ML_Bootcamp-master/14-K-Nearest-Neighbors/Classified Data",index_col=0)
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WTT</th>
      <th>PTI</th>
      <th>EQW</th>
      <th>SBI</th>
      <th>LQE</th>
      <th>QWG</th>
      <th>FDJ</th>
      <th>PJF</th>
      <th>HQE</th>
      <th>NXJ</th>
      <th>TARGET CLASS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.913917</td>
      <td>1.162073</td>
      <td>0.567946</td>
      <td>0.755464</td>
      <td>0.780862</td>
      <td>0.352608</td>
      <td>0.759697</td>
      <td>0.643798</td>
      <td>0.879422</td>
      <td>1.231409</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.635632</td>
      <td>1.003722</td>
      <td>0.535342</td>
      <td>0.825645</td>
      <td>0.924109</td>
      <td>0.648450</td>
      <td>0.675334</td>
      <td>1.013546</td>
      <td>0.621552</td>
      <td>1.492702</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.721360</td>
      <td>1.201493</td>
      <td>0.921990</td>
      <td>0.855595</td>
      <td>1.526629</td>
      <td>0.720781</td>
      <td>1.626351</td>
      <td>1.154483</td>
      <td>0.957877</td>
      <td>1.285597</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.234204</td>
      <td>1.386726</td>
      <td>0.653046</td>
      <td>0.825624</td>
      <td>1.142504</td>
      <td>0.875128</td>
      <td>1.409708</td>
      <td>1.380003</td>
      <td>1.522692</td>
      <td>1.153093</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.279491</td>
      <td>0.949750</td>
      <td>0.627280</td>
      <td>0.668976</td>
      <td>1.232537</td>
      <td>0.703727</td>
      <td>1.115596</td>
      <td>0.646691</td>
      <td>1.463812</td>
      <td>1.419167</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



## Scaling data


```python
# standardize variables per il kNN
# StandardScaler() object
scaler = StandardScaler()
# Fit scaler to the features
scaler.fit(df.drop('TARGET CLASS',axis=1))
# .transform() method to transform the features to a scaled version
scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))
# Convert the scaled features to a dataframe
df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])
df_feat.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WTT</th>
      <th>PTI</th>
      <th>EQW</th>
      <th>SBI</th>
      <th>LQE</th>
      <th>QWG</th>
      <th>FDJ</th>
      <th>PJF</th>
      <th>HQE</th>
      <th>NXJ</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.123542</td>
      <td>0.185907</td>
      <td>-0.913431</td>
      <td>0.319629</td>
      <td>-1.033637</td>
      <td>-2.308375</td>
      <td>-0.798951</td>
      <td>-1.482368</td>
      <td>-0.949719</td>
      <td>-0.643314</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.084836</td>
      <td>-0.430348</td>
      <td>-1.025313</td>
      <td>0.625388</td>
      <td>-0.444847</td>
      <td>-1.152706</td>
      <td>-1.129797</td>
      <td>-0.202240</td>
      <td>-1.828051</td>
      <td>0.636759</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.788702</td>
      <td>0.339318</td>
      <td>0.301511</td>
      <td>0.755873</td>
      <td>2.031693</td>
      <td>-0.870156</td>
      <td>2.599818</td>
      <td>0.285707</td>
      <td>-0.682494</td>
      <td>-0.377850</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.982841</td>
      <td>1.060193</td>
      <td>-0.621399</td>
      <td>0.625299</td>
      <td>0.452820</td>
      <td>-0.267220</td>
      <td>1.750208</td>
      <td>1.066491</td>
      <td>1.241325</td>
      <td>-1.026987</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.139275</td>
      <td>-0.640392</td>
      <td>-0.709819</td>
      <td>-0.057175</td>
      <td>0.822886</td>
      <td>-0.936773</td>
      <td>0.596782</td>
      <td>-1.472352</td>
      <td>1.040772</td>
      <td>0.276510</td>
    </tr>
  </tbody>
</table>
</div>



## Training model kNN


```python
# train test
X = df_feat
y = df['TARGET CLASS']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=101)

# X_train, X_test, y_train, y_test = train_test_split(df.drop('TARGET CLASS',axis=1),df['TARGET CLASS'],test_size=0.30,random_state=101)
```


```python
# kNN
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train,y_train)
```




    KNeighborsClassifier(n_neighbors=1)




```python
# predictions
pred = knn.predict(X_test)
```


```python
# confusion matrix
print(confusion_matrix(y_test,pred))
```

    [[151   8]
     [ 15 126]]



```python
# metrics report
print(classification_report(y_test,pred))
```

                  precision    recall  f1-score   support
    
               0       0.91      0.95      0.93       159
               1       0.94      0.89      0.92       141
    
        accuracy                           0.92       300
       macro avg       0.92      0.92      0.92       300
    weighted avg       0.92      0.92      0.92       300
    


## Best k-Value Model


```python
# iterate models and find best k
error_rate = []

for i in range(1,60):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
```


```python
# plot trend k-errors
plt.figure(figsize=(10,6))
plt.plot(range(1,60),error_rate,color='blue',linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
```




    Text(0, 0.5, 'Error Rate')




![png](/assets/images/Python/Course 001/section-018/output_18_1.png)



```python
# model comparisons: k = 1
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train,y_train)
pred = knn.predict(X_test)
print('K = 1')
print('\nConfusion Matrix:')
print(confusion_matrix(y_test,pred))
print('\nClassification metrics:')
print(classification_report(y_test,pred))
```

    K = 1
    
    Confusion Matrix:
    [[151   8]
     [ 15 126]]
    
    Classification metrics:
                  precision    recall  f1-score   support
    
               0       0.91      0.95      0.93       159
               1       0.94      0.89      0.92       141
    
        accuracy                           0.92       300
       macro avg       0.92      0.92      0.92       300
    weighted avg       0.92      0.92      0.92       300
    



```python
# model comparisons: k = 40
knn = KNeighborsClassifier(n_neighbors=40)
knn.fit(X_train,y_train)
pred = knn.predict(X_test)
print('K = 40')
print('\nConfusion Matrix:')
print(confusion_matrix(y_test,pred))
print('\nClassification metrics:')
print(classification_report(y_test,pred))
```

    K = 40
    
    Confusion Matrix:
    [[154   5]
     [  7 134]]
    
    Classification metrics:
                  precision    recall  f1-score   support
    
               0       0.96      0.97      0.96       159
               1       0.96      0.95      0.96       141
    
        accuracy                           0.96       300
       macro avg       0.96      0.96      0.96       300
    weighted avg       0.96      0.96      0.96       300
    


# Tree Methods

**Decision Tree**

<img src="/assets/images/Python/Course 001/section-018/002-Tree.png" width="400">

Se si usa l'algoritmo ID3, lo split deve massimizzare l'Information Gain  
Entropy  
$$H\left (S\right )=-\sum_{i}p_{i}\left (S\right)\log_{2}p_{i}\left (S\right )$$  
Information Gain  
$$IG\left (S,A\right )=H\left (S\right )-\sum_{\nu \in Values\left (A\right )}\frac{\left | S_{\nu}\right |}{S}H\left (S_{\nu}\right )=H\left (S\right )-H\left (S,A\right )$$

**Random Forest**

Bootstrapping rows and sampling columns every tree is generated.


```python
# lib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report,confusion_matrix

from IPython.display import Image  
from six import StringIO  
from sklearn.tree import export_graphviz
import pydot

from sklearn.ensemble import RandomForestClassifier
```


```python
# data
df = pd.read_csv(r'Refactored_Py_DS_ML_Bootcamp-master/15-Decision-Trees-and-Random-Forests/kyphosis.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Kyphosis</th>
      <th>Age</th>
      <th>Number</th>
      <th>Start</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>absent</td>
      <td>71</td>
      <td>3</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>absent</td>
      <td>158</td>
      <td>3</td>
      <td>14</td>
    </tr>
    <tr>
      <th>2</th>
      <td>present</td>
      <td>128</td>
      <td>4</td>
      <td>5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>absent</td>
      <td>2</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>absent</td>
      <td>1</td>
      <td>4</td>
      <td>15</td>
    </tr>
  </tbody>
</table>
</div>




```python
# distribuzione target
print(df['Kyphosis'].value_counts())
sns.countplot(x='Kyphosis',data=df,palette='Set1') # palette='RdBu_r'
# è abbastanza sbilanciato
```

    absent     64
    present    17
    Name: Kyphosis, dtype: int64

    <matplotlib.axes._subplots.AxesSubplot at 0x7f03c04281d0>




![png](/assets/images/Python/Course 001/section-018/output_29_2.png)



```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 81 entries, 0 to 80
    Data columns (total 4 columns):
     #   Column    Non-Null Count  Dtype 
    ---  ------    --------------  ----- 
     0   Kyphosis  81 non-null     object
     1   Age       81 non-null     int64 
     2   Number    81 non-null     int64 
     3   Start     81 non-null     int64 
    dtypes: int64(3), object(1)
    memory usage: 2.7+ KB



```python
# pairplot
sns.pairplot(df,hue='Kyphosis',palette='Set1')
```




    <seaborn.axisgrid.PairGrid at 0x7f03cc1a2290>




![png](/assets/images/Python/Course 001/section-018/output_31_1.png)



```python
# train test split
X = df.drop('Kyphosis',axis=1)
y = df['Kyphosis']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
```

## Decision Tree


```python
# tree
dtree = DecisionTreeClassifier()
dtree.fit(X_train,y_train)
```




    DecisionTreeClassifier()




```python
# predictions
predictions = dtree.predict(X_test)
```


```python
# confusion matrix and classification metrix
print('\nConfusion Matrix:')
print(confusion_matrix(y_test,predictions))
print('\nClassification metrics:')
print(classification_report(y_test,predictions))
```

    
    Confusion Matrix:
    [[17  2]
     [ 2  4]]
    
    Classification metrics:
                  precision    recall  f1-score   support
    
          absent       0.89      0.89      0.89        19
         present       0.67      0.67      0.67         6
    
        accuracy                           0.84        25
       macro avg       0.78      0.78      0.78        25
    weighted avg       0.84      0.84      0.84        25
    


### Tree Visualization


```python
# cols
features = list(df.columns[1:])
features
```




    ['Age', 'Number', 'Start']




```python
# plot tree
dot_data = StringIO()  
export_graphviz(dtree, out_file=dot_data,feature_names=features,filled=True,rounded=True)

graph = pydot.graph_from_dot_data(dot_data.getvalue())  
Image(graph[0].create_png())
```




![png](/assets/images/Python/Course 001/section-018/output_39_0.png)



## Random Forest


```python
# RF
rfc = RandomForestClassifier(n_estimators=300)
rfc.fit(X_train, y_train)
```




    RandomForestClassifier(n_estimators=300)




```python
# predictions
rfc_pred = rfc.predict(X_test)
```


```python
# confusion matrix and classification metrix
print('\nConfusion Matrix:')
print(confusion_matrix(y_test,rfc_pred))
print('\nClassification metrics:')
print(classification_report(y_test,rfc_pred))
```

    
    Confusion Matrix:
    [[18  1]
     [ 3  3]]
    
    Classification metrics:
                  precision    recall  f1-score   support
    
          absent       0.86      0.95      0.90        19
         present       0.75      0.50      0.60         6
    
        accuracy                           0.84        25
       macro avg       0.80      0.72      0.75        25
    weighted avg       0.83      0.84      0.83        25
    


# Altro

## Better confusion matrix


```python
# matrice per absent è yes
pd.DataFrame(confusion_matrix(y_test,rfc_pred, labels=['absent', 'present']),
             index=['TRUE | Yes', 'TRUE | No'],
             columns=['PRED | Yes', 'PRED | No'])
# esempio: precision per absent è 18/(18+3)=0.86
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PRED | Yes</th>
      <th>PRED | No</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>TRUE | Yes</th>
      <td>18</td>
      <td>1</td>
    </tr>
    <tr>
      <th>TRUE | No</th>
      <td>3</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>




```python
# matrice per present è yes
pd.DataFrame(confusion_matrix(y_test,rfc_pred, labels=['present','absent']),
             index=['TRUE | Yes', 'TRUE | No'],
             columns=['PRED | Yes', 'PRED | No'])
# esempio: recall per present è 3/(3+3)=0.5
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PRED | Yes</th>
      <th>PRED | No</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>TRUE | Yes</th>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>TRUE | No</th>
      <td>1</td>
      <td>18</td>
    </tr>
  </tbody>
</table>
</div>




```python
# costruisco una funzione
def confusion_matrix_bella(y_true, y_pred, labels='default'):
    if labels is 'default':
        labels = np.unique([y_true,y_pred])
    cmtx = pd.DataFrame(
        confusion_matrix(y_true, y_pred, labels=labels), 
        index=['TRUE | {:}'.format(x) for x in labels], 
        columns=['PRED | {:}'.format(x) for x in labels]
    )
    return(cmtx)
```


```python
confusion_matrix_bella(y_test,rfc_pred,labels=['absent','present'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PRED | present</th>
      <th>PRED | absent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>TRUE | present</th>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>TRUE | absent</th>
      <td>1</td>
      <td>18</td>
    </tr>
  </tbody>
</table>
</div>



## Better distplot


```python
# better displot function
def distplot_fig(data, x, hue=None, row=None, col=None, height=3, aspect=1, legend=True, hist=False, **kwargs):
    """A figure-level distribution plot with support for hue, col, row arguments."""
    bins = kwargs.pop('bins', None)
    if (bins is None) and hist: 
        # Make sure that the groups have equal-sized bins
        bins = np.histogram_bin_edges(data[x].dropna())
    g = sns.FacetGrid(data, hue=hue, row=row, col=col, height=height, aspect=aspect)
    g.map(sns.distplot, x, bins=bins, hist=hist, **kwargs)
    if legend and (hue is not None) and (hue not in [x, row, col]):
        g.add_legend(title=hue) 
    return g
```


```python
# better distplot
distplot_fig(data=df,x='Age',hue='Kyphosis',kde=False,hist=True,hist_kws=dict(edgecolor='grey'),bins=30, height=4, aspect=2)
```




    <seaborn.axisgrid.FacetGrid at 0x7f03c05ee910>




![png](/assets/images/Python/Course 001/section-018/output_52_1.png)



```python
# multiple displot (seaborn)
plt.figure(figsize=(10,6))
sns.distplot(df[df['Kyphosis']=='absent']['Age'],kde=False,color='b',bins=20,hist_kws=dict(edgecolor='grey'),label='Kyphosis = absent')
sns.distplot(df[df['Kyphosis']=='present']['Age'],kde=False,color='y',bins=20,hist_kws=dict(edgecolor='grey'),label='Kyphosis = present')
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f03c9a55550>




![png](/assets/images/Python/Course 001/section-018/output_53_1.png)



```python
# multiple distplot (matplotlib)
plt.figure(figsize=(10,6))
df[df['Kyphosis']=='absent']['Age'].hist(alpha=0.5,color='b',bins=20,label='Kyphosis = absent',edgecolor="grey")
df[df['Kyphosis']=='present']['Age'].hist(alpha=0.5,color='y',bins=20,label='Kyphosis = present',edgecolor="grey")
plt.legend()
plt.xlabel('Age')
plt.grid(None)
```


![png](/assets/images/Python/Course 001/section-018/output_54_0.png)

